# Module google/&zwnj;nnlm-es-dim50/2
Token based text embedding trained on Spanish Google News
50B corpus.

<!-- dataset: Google News -->
<!-- asset-path: legacy -->
<!-- language: es -->
<!-- module-type: text-embedding -->
<!-- network-architecture: NNLM -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->


## TF2 SavedModel

This is a [SavedModel in TensorFlow 2
format](https://www.tensorflow.org/hub/tf2_saved_model).
Using it requires TensorFlow 2 (or 1.15) and TensorFlow Hub 0.5.0 or newer.

## Overview

Text embedding based on feed-forward Neural-Net Language Models[1] with
pre-built OOV. Maps from text to 50-dimensional embedding vectors.

#### Example use
The saved model can be loaded directly:

```
import tensorflow_hub as hub

embed = hub.load("https://tfhub.dev/google/nnlm-es-dim50/2")
embeddings = embed(["gato", "gato y perro"])
```

It can also be used within Keras:

```
hub_layer = hub.KerasLayer("https://tfhub.dev/google/nnlm-es-dim50/2",
                           input_shape=[], dtype=tf.string)

model = keras.Sequential()
model.add(hub_layer)
model.add(keras.layers.Dense(16, activation='relu'))
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.summary()
```

## Details
Based on NNLM with two hidden layers.

#### Input
The module takes **a batch of sentences in a 1-D tensor of strings** as input.

#### Preprocessing
The module preprocesses its input by **splitting on spaces**.

#### Out of vocabulary tokens
Small fraction of the least frequent tokens and embeddings (~2.5%) are
**replaced by hash buckets**. Each hash bucket is initialized using the remaining
embedding vectors that hash to the same bucket.

#### Sentence embeddings
Word embeddings are combined into sentence embedding using the `sqrtn` combiner
(see [tf.nn.embedding_lookup_sparse](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse)).

#### References
[1] Yoshua Bengio, RÃ©jean Ducharme, Pascal Vincent, Christian Jauvin.
[A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf).
Journal of Machine Learning Research, 3:1137-1155, 2003.
