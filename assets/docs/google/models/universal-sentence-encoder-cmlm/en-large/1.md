# Module google/universal-sentence-encoder-cmlm/en-large/1

Universal sentence encoder for English trained with conditional masked language
model.

<!-- asset-path: internal -->
<!-- module-type: text-embedding -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- network-architecture: Transformer -->
<!-- dataset: CommonCrawl -->
<!-- dataset: Wikipedia -->

## Overview

The universal sentence encoder family of models map text into high dimensional
vectors that capture sentence-level semantics. Our English-large (en-large)
model is trained using a conditional masked language model described in [1]. The
model is intended to be used for text classification, text clustering, semantic
textural similarity, etc. It can also be use used as modularized input for
multimodal tasks with text as a feature. The model can be fine-tuned for all of
these tasks. The large model employs a 24 layer BERT transformer architecture.

### Metrics

*   We evaluate this model on
    [SentEval](https://github.com/facebookresearch/SentEval) sentence
    representation benchmark.

    SentEval | MR | CR | SUBJ | MPQA | SST | TREC | MRPC | SICK-E | SICK-R |
    Avg :--------------------------------------------------------------------------------
    | ---: | ---: | ---: | ---: | ---: | ---: | ---: | -----: | -----: | --:
    [USE-CMLM-Base](https://tfhub.dev/google/universal-sentence-encoder-cmlm/large/1)
    | 83.6 | 89.9 | 96.2 | 89.3 | 88.5 | 91.0 | 69.7 | 82.3 | 83.4 | 86.0
    **USE-CMLM-Large** | 85.6 | 89.1 | 96.6 | 89.3 | 91.4 | 92.4 | 70.0 | 82.2 |
    84.5 | 86.8

More details of the evaluations can be found in the paper [1]. Large model
generally produces better performance, but inference and fine-tuning take longer
time than the base model.

To learn more about text embeddings, refer to the
[TensorFlow Embeddings](https://www.tensorflow.org/guide/embedding)
documentation. Our encoder differs from word level embedding models in that we
train on a number of natural language prediction tasks that require modeling the
meaning of word sequences rather than just individual words. Details are
available in the paper "Universal Sentence Representations Learning with
Conditional Masked Language Model" [1].

## Extended Uses and Limitations

The sentence embeddings can also be used for text classification, semantic
similarity, clustering and other natural language tasks. Performance depends on
the domain / data match of a particular downstream task to our pre-training
data. Additional models are available from
[Universal Sentence Encoder family](https://tfhub.dev/google/collections/universal-sentence-encoder/1).

The model extends the BERT transformer architecture and it can be used for any
tasks BERT can be applied. Performances gains over BERT depend on the particular
task. The [BERT model family](https://tfhub.dev/google/collections/bert/1)
contains additional BERT based models.

## Example Use

Our model is based on the BERT transformer architecture, which generates a
pooled_output of shape [batch_size, 768] with representations for the entire
input sequences. The representation generated from this model is recommended to
be used as is without fine-tuning. The model is also fine-tunable like other
BERT models. It can be called like a regular BERT model as follows:

```python
import tensorflow_hub as hub
import tensorflow as tf

english_sentences = tf.constant(["dog", "Puppies are nice.", "I enjoy taking long walks along the beach with my dog."])

preprocessor = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2")
encoder = hub.KerasLayer(
    "https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-large/1")

english_embeds = encoder(preprocessor(english_sentences))["default"]

print (english_embeds)

```

## References

[1] Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve. [Universal Sentence
Representations Learning with Conditional Masked Language
Model](https://openreview.net/forum?id=WDVD4lUCTzU). November 2020
