# Module jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1

Unsupervised Cross-lingual Representation Learning at Scale (XLM-RoBERTa).

<!-- asset-path: https://storage.googleapis.com/jeongukjae-tf-models/RoBERTa/xlm_roberta_cased_L-12_H-768_A-12.tar.gz -->
<!-- network-architecture: transformer -->
<!-- task: text-embedding -->
<!-- fine-tunable: true -->
<!-- license: mit -->
<!-- format: saved_model_2 -->
<!-- language: ar -->
<!-- language: as -->
<!-- language: bg -->
<!-- language: bn -->
<!-- language: ca -->
<!-- language: cs -->
<!-- language: da -->
<!-- language: de -->
<!-- language: el -->
<!-- language: en -->
<!-- language: es -->
<!-- language: et -->
<!-- language: fa -->
<!-- language: fi -->
<!-- language: fr -->
<!-- language: gu -->
<!-- language: he -->
<!-- language: hi -->
<!-- language: hr -->
<!-- language: hu -->
<!-- language: id -->
<!-- language: it -->
<!-- language: ja -->
<!-- language: ko -->
<!-- language: kn -->
<!-- language: lt -->
<!-- language: lv -->
<!-- language: ml -->
<!-- language: mr -->
<!-- language: ms -->
<!-- language: ne -->
<!-- language: nl -->
<!-- language: no -->
<!-- language: or -->
<!-- language: pa -->
<!-- language: pl -->
<!-- language: pt -->
<!-- language: ro -->
<!-- language: ru -->
<!-- language: sa -->
<!-- language: sd -->
<!-- language: sk -->
<!-- language: sl -->
<!-- language: sr -->
<!-- language: sv -->
<!-- language: ta -->
<!-- language: th -->
<!-- language: tr -->
<!-- language: uk -->
<!-- language: ur -->
<!-- language: vi -->
<!-- language: zh-cn -->
<!-- language: zh-tw -->

## Overview

This model is a tensorflow conversion of [`xlm-roberta-base`](https://huggingface.co/xlm-roberta-base) from the HuggingFace model hub. It is exported as TF SavedModel in [this repository(jeongukjae/huggingface-to-tfhub)](https://github.com/jeongukjae/huggingface-to-tfhub). For more descriptions or training details, you can check [the model card in HuggingFace model hub](https://huggingface.co/xlm-roberta-base).

If you want to see supported langauges, check [this link (pytorch/fairseq)](https://github.com/pytorch/fairseq/tree/main/examples/xlmr#introduction).

## Example Use

You can use this model with an interface that is almost identical to bert's in tfhub.

For example, you can define a text embedding model with below code.

```python
# define a text embedding model
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)
preprocessor = hub.KerasLayer("https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_preprocess/1")
encoder_inputs = preprocessor(text_input)

encoder = hub.KerasLayer("https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1", trainable=True)
encoder_outputs = encoder(encoder_inputs)
pooled_output = encoder_outputs["pooled_output"]      # [batch_size, 768].
sequence_output = encoder_outputs["sequence_output"]  # [batch_size, seq_length, 768].

model = tf.keras.Model(text_input, pooled_output)

# You can embed your sentences as follows
sentences = tf.constant(["(your text here)"])
print(model(sentences))
```

### Build model for multi text inputs

If you want a model for multi text inputs (i.e. fine-tuning with nli datasets), you can build as follows.

```python
preprocessor = hub.load("https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_preprocess/1")
tokenize = hub.KerasLayer(preprocessor.tokenize)
bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs)
encoder = hub.KerasLayer("https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1", trainable=True)

text_inputs = [
    tf.keras.layers.Input(shape=(), dtype=tf.string),
    tf.keras.layers.Input(shape=(), dtype=tf.string),
]
tokenized_inputs = [tokenize(segment) for segment in text_inputs]
encoder_inputs = bert_pack_inputs(tokenized_inputs)
encoder_outputs = encoder(encoder_inputs)

pooled_output = encoder_outputs["pooled_output"]      # [batch_size, 768].
sequence_output = encoder_outputs["sequence_output"]  # [batch_size, seq_length, 768].

model = tf.keras.Model(text_inputs, pooled_output)

# You can pass your sentences as follows
hypotheses = tf.constant(["(your hypothesis text here)"])
premises = tf.constant(["(your premise text here)"])
print(model([hypotheses, premises]))
```

## Output details

The outputs of this model are a dict, and each entries are as follows:

* `"pooled_output"`: pooled output of the entire sequence with shape `[batch size, hidden size(768 for this model)]`. You can use this output as the sentence representation.
* `"sequence_output"`: representations of every token in the input sequence with shape `[batch size, max sequence length, hidden size(768)]`.
* `"encoder_outputs"`: A list of 12 tensors of shapes are `[batch size, sequence length, hidden size(768)]` with the outputs of the i-th Transformer block.

## References

* [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)
* [XLM-RoBERTa examples in pytorch/fairseq](https://github.com/pytorch/fairseq/tree/main/examples/xlmr)
* [`xlm-roberta-base` Model card in HuggingFace model hub](https://huggingface.co/xlm-roberta-base)
