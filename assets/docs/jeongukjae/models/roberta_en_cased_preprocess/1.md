# Module jeongukjae/roberta_en_cased_preprocess/1

Text preprocessing model for `jeongukjae/roberta_en_cased_L-24_H-1024_A-16/1` and `jeongukjae/roberta_en_cased_L-12_H-768_A-12/1`.

<!-- asset-path: https://storage.googleapis.com/jeongukjae-tf-models/RoBERTa/roberta_en_cased_preprocess.tar.gz -->
<!-- task: text-preprocessing -->
<!-- fine-tunable: false -->
<!-- license: mit -->
<!-- format: saved_model_2 -->
<!-- language: en -->

## Overview

This model is a text preprocessing model for [`jeongukjae/roberta_en_cased_L-24_H-1024_A-16/1`](https://tfhub.dev/jeongukjae/roberta_en_cased_L-24_H-1024_A-16/1) and [`jeongukjae/roberta_en_cased_L-12_H-768_A-12/1`](https://tfhub.dev/jeongukjae/roberta_en_cased_L-12_H-768_A-12/1).

This model has no trainable parameters and can be used in an input pipeline outside the training loop.

**Note that this model can't tokenize multiple whitespaces appropriately** because `re2` (used as regex engine in TensorFlow) doesn't support the lookaround feature. For example, the original one tokenizes `"  token"` (two spaces) to `[" ", " token"]`, but this preprocess model tokenizes it to `["  ", "token"]`. **So I recommend normalizing whitespaces before passing sentences into preprocessing model**. For more details, you can check the "Empty strings" table in [this link](https://github.com/google/re2/wiki/Syntax).

## Prerequisites

This model uses [`SentencepieceTokenizer`](https://www.tensorflow.org/text/api_docs/python/text/SentencepieceTokenizer) in TensorFlow Text. You can register required ops as follows.

```python
# Install it with "pip install tensorflow-text"
import tensorflow_text as text
```

## Usage

This model supports preprocessing single or multi segment text inputs.

### Preprocess single text segment

```python
sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name="sentences")
preprocessor = hub.KerasLayer("https://tfhub.dev/jeongukjae/roberta_en_cased_preprocess/1")
encoder_inputs = preprocessor(sentences)
```

### Preprocess multiple text segments

```python
preprocessor = hub.load("https://tfhub.dev/jeongukjae/roberta_en_cased_preprocess/1")
tokenize = hub.KerasLayer(preprocessor.tokenize)
bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs)
# You can use different sequence length like below. (default is 128)
#
# bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs, arguments=dict(seq_length=64))

sentences = [
    tf.keras.layers.Input(shape=(), dtype=tf.string, name="segment_a"),
    tf.keras.layers.Input(shape=(), dtype=tf.string, name="segment_b"),
]
tokenized_sentences = [tokenize(segment) for segment in sentences]
encoder_inputs = bert_pack_inputs(tokenized_sentences)
```

### Output details

The result of preprocessing is a batch of fixed-length input sequences for the RoBERTa encoder.

An input sequence starts with one start-of-sequence token, followed by the tokenized segments, each terminated by one end-of-segment token. Remaining positions up to `seq_length`, if any, are filled up with padding tokens. If an input sequence would exceed `seq_length`, the tokenized segments in it are truncated to prefixes of approximately equal sizes to fit exactly.

The `encoder_inputs` are a dict of three int32 Tensors, all with shape `[batch_size, seq_length]`, whose elements represent the batch of input sequences as follows:

* `"input_word_ids"`: has the token ids of the input sequences.
* `"input_mask"`: has value 1 at the position of all input tokens present before padding and value 0 for the padding tokens.
* `"input_type_ids"`: was originally the index of the input segment that gave rise to the input token at the respective position. But for RoBERTa, this value always has the value 0 at all positions.

## Implementation details

RoBERTa uses GPT-2's BPE Tokenizer and it can be converted to SentencePiece's BPE model. You can see the codes to convert preprocessing model in [this link](https://github.com/jeongukjae/huggingface-to-tfhub/blob/0700fdbf415b622d55d280e2d3939d12c929f5ec/export_roberta.py#L737-L763).
