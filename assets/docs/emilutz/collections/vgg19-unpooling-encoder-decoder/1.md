# Collection emilutz/vgg19-unpooling-encoder-decoder/1

Collection of image encoders and decoders trained on the COCO 2017 dataset where encoders are VGG19 feature extractors from various intermediary layers that store argmax information from max-pooling operations and decoders mirror the architecture of the encoders and use unpooling layers for upsampling.

<!-- dataset: coco-2017 -->
<!-- task: image-feature-vector -->
<!-- network-architecture: vgg-style -->

## Overview

The following are a set of 5 encoder-decoder pairs in the form of [TF2 SavedModels](https://www.tensorflow.org/hub/tf2_saved_model) and trained on [COCO 2017](https://cocodataset.org/) dataset. These models can be used for latent space manipulation for domains like style transfer or image stylization as exemplified here: https://arxiv.org/pdf/1802.06474.pdf.

**Encoders** are nothing more than VGG19 subnetworks that kept their weights frozen during the training of the decoders. 

**Decoders** were set to mirror the layout of their corresponding encoders and were trained from scratch to reconstruct the original images from their encoded feature vectors. Reconstruction of fine image structures was facilitated by using *unpooling layers* instead of conventional methods like deconvolutions or bilinear/nearest interpolation. In order to ensure the reconstructions are of high quality from other perspectives like actual pixel values, brightness or contrast, the decoders were trained to minimize a combination of 4 different losses:
- L1 loss
- L2 loss
- MS-SSIM loss (computed as `1.0 - tf.image.ssim_multiscale(inputs, outputs, 255)`; more information about multi-scale structural similarity can be found here: https://www.cns.nyu.edu/pub/eero/wang03b.pdf)
- Perceptual loss (computed as `tf.keras.losses.mean_squared_error(encoder(inputs), encoder(decoder(encoder(inputs))))` so every encoder also acts as the perceptual loss feature extractor for his decoder; more information about perceptual loss can be found here: https://arxiv.org/pdf/1603.08155.pdf)

The weights used for the loss components were chosen in accordance to their magnitudes and are the following:
- `lambda_l1 = 10.0`
- `lambda_l2 = 1.0`
- `lambda_ms_ssim = 1000.0`
- `lambda_perceptual = 0.1`
  
The training process consisted of 30 to 100 epochs on 90% of the data, while 9% was used for validation and 1% left for testing. Adam optimizer was used with the default parameters and a linear learning rate decay between 0.001 and 0.0002, and the only data augmentation used was a random horizontal flip on each batch.

## Models

Encoder inputs are expected to be batches of images of arbitrary height and width but depending on the encoder-decoder pair used VGG19-Block**X**-Conv2 the sizes must be multiples of 2 ^ **(X - 1)**. The outputs of the encoder are a list of the extracted image features + the argmax values for all maxpooling layers encountered until that point in the VGG19. For the decoders the input and output shapes are reversed.

| Name | Input Shape | Output Shape | L1 loss* |
|------|-------------|--------------|----------|
| [VGG19-Block1-Conv2 Unpooling Encoder](https://tfhub.dev/emilutz/vgg19-block1-conv2-unpooling-encoder/1) | (B, H, W, 3) | (B, H, W, 64) |   -   |
| [VGG19-Block1-Conv2 Unpooling Decoder](https://tfhub.dev/emilutz/vgg19-block1-conv2-unpooling-decoder/1)  | (B, H, W, 64) | (B, H, W, 3) | 2.494 |
| [VGG19-Block2-Conv2 Unpooling Encoder](https://tfhub.dev/emilutz/vgg19-block2-conv2-unpooling-encoder/1)  | (B, H, W, 3) | [(B, H/2, W/2, 128), (B * H * W * 16, 4)] |   -   |
| [VGG19-Block2-Conv2 Unpooling Decoder](https://tfhub.dev/emilutz/vgg19-block2-conv2-unpooling-decoder/1)  | [(B, H/2, W/2, 128), (B * H * W * 16, 4)] | (B, H, W, 3) | 3.859 |
| [VGG19-Block3-Conv2 Unpooling Encoder](https://tfhub.dev/emilutz/vgg19-block3-conv2-unpooling-encoder/1)  | (B, H, W, 3) | [(B, H/4, W/4, 256), (B * H * W * 16, 4), (B * H * W * 8, 4)] |   -   |
| [VGG19-Block3-Conv2 Unpooling Decoder](https://tfhub.dev/emilutz/vgg19-block3-conv2-unpooling-decoder/1)  | [(B, H/4, W/4, 256), (B * H * W * 16, 4), (B * H * W * 8, 4)] | (B, H, W, 3) | 5.124 |
| [VGG19-Block4-Conv2 Unpooling Encoder](https://tfhub.dev/emilutz/vgg19-block4-conv2-unpooling-encoder/1)  | (B, H, W, 3) | [(B, H/8, W/8, 512), (B * H * W * 16, 4), (B * H * W * 8, 4), (B * H * W * 4, 4)] |   -   |
| [VGG19-Block4-Conv2 Unpooling Decoder](https://tfhub.dev/emilutz/vgg19-block4-conv2-unpooling-decoder/1)  | [(B, H/8, W/8, 512), (B * H * W * 16, 4), (B * H * W * 8, 4), (B * H * W * 4, 4)] | (B, H, W, 3) | 6.104 |
| [VGG19-Block5-Conv2 Unpooling Encoder](https://tfhub.dev/emilutz/vgg19-block5-conv2-unpooling-encoder/1)  | (B, H, W, 3) | [(B, H/16, W/16, 512), (B * H * W * 16, 4), (B * H * W * 8, 4), (B * H * W * 4, 4), (B * H * W * 2, 4)] |   -   |
| [VGG19-Block5-Conv2 Unpooling Decoder](https://tfhub.dev/emilutz/vgg19-block5-conv2-unpooling-decoder/1)  | [(B, H/16, W/16, 512), (B * H * W * 16, 4), (B * H * W * 8, 4), (B * H * W * 4, 4), (B * H * W * 2, 4)] | (B, H, W, 3) | 6.626 |

The **L1 loss** was estimated from the loss on the test set consisting of 1234 samples from the COCO 2017 dataset. The values represent the mean absolute pixel difference across all channels (in the range [0, 255]).
