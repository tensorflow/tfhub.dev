# Module google/object_detection/mobile_object_labeler_v1/1

A mobile object labeler.

<!-- asset-path: @visionkit/mobile_raid/classifier/mobile_object_labeler_opensource_V0/1 -->
<!-- module-type: image-classification -->
<!-- task: image-classification -->
<!-- fine-tunable: false -->
<!-- format: hub -->
<!-- language: en -->
<!-- network-architecture: mobilenet-v2 -->
<!-- interactive-model-name: vision -->

## Overview

### Module description

This model classifies localized objects in an image. The model is
mobile-friendly and can run on-device.

### Input

Inputs are expected to be 3-channel RGB color images of size 224 x 224 scaled to
`[0,1]`.

### Output

The model outputs a tensor `image_classifier` containing output logits of
dimension 631. See
[Labelmap](https://www.gstatic.com/aihub/tfhub/labelmaps/mobile_object_labeler_v1_labelmap.csv).

## Usage
### Use SavedModel in Python

The model can be loaded in a Python script as follows:

```python
images = ...  # A batch of images with shape [batch_size, height, width, 3].
module = hub.Module("https://tfhub.dev/google/object_detection/mobile_object_labeler_v1/1")
features = module(images)  # Features with shape [batch_size, num_outputs].
```

The input `images` are expected to have color values in the range [0,1],
following the
[common image input](https://www.tensorflow.org/hub/common_signatures/images#input)
conventions. The input image size is 224x224 pixels.

Fine-tuning is not currently supported.

## Model architecture and training

### Model architecture

This model uses a MobileNet V2 backbone with a 0.5 width multiplier chosen after
manual benchmarking, and two fully-connected layers of 1024 channels.

MobileNet V2 is a family of neural network architectures for efficient on-device
image classification and related tasks, originally published by

*   Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh
    Chen: ["Inverted Residuals and Linear Bottlenecks: Mobile Networks for
    Classification, Detection and
    Segmentation"](https://arxiv.org/abs/1801.04381), 2018.

MobileNets come in various sizes controlled by a multiplier for the depth
(number of features) in the convolutional layers. They can also be trained for
various sizes of input images to control inference speed.

### Model training

This model was trained using:

*   [Quantization-aware training](https://github.com/tensorflow/tensorflow/tree/r1.13/tensorflow/contrib/quantize)

### Additional information

*   It is recommended to use it on top of the
    [Mobile Object Localizer](https://tfhub.dev/google/object_detection/mobile_object_localizer_v1/1)
    as it requires a prominent object as input.
*   Results with confidence below 0.35 should be discarded.

## Suitable Use, Limitations, and Fair Use Terms

### Suitable usecases

This model is **suitable** for classifying the most prominent and localized
object in an image. Such objects are products (apparel, furniture, etc),
vehicles, food, animals, and other (see labelmap).

### Unsuitable usecases

This model is **unsuitable** for:

*   standalone use in mission-critical applications such as obstacle and human
    detection for autonomous driving.
*   applications of cultural or religious significance.

### Limitations

*   This model may not generalize to full images featuring multiple objects.
*   The model is an automatic image classifier, which may result in imperfect
    output.

### License

This model follows [*Apache 2.0*](https://www.apache.org/licenses/LICENSE-2.0).
If you intend to use it beyond permissible usage, please consult with the model
owners ahead of time.

### Citation

When used for publication or production, please cite this model as: "Google
Mobile Object labeler".
