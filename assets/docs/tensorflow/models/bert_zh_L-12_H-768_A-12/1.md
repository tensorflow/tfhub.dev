# Module tensorflow/bert_zh_L-12_H-768_A-12/1
Bidirectional Encoder Representations from Transformers (BERT).

<!-- dataset: wikipedia -->
<!-- asset-path: legacy -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: zh-cn -->
<!-- module-type: text-embedding -->
<!-- task: text-embedding -->
<!-- network-architecture: transformer -->


## TF2 SavedModel

This is a [SavedModel in TensorFlow 2
format](https://www.tensorflow.org/hub/tf2_saved_model).
Using it requires TensorFlow 2 (or 1.15) and TensorFlow Hub 0.5.0 or newer.

## Overview

BERT (Bidirectional Encoder Representations from Transformers)
provides dense vector representations for natural language
by using a deep, pre-trained neural network with the Transformer
architecture. It was originally published by

  * Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova:
    ["BERT: Pre-training of Deep Bidirectional Transformers for
    Language Understanding"](https://arxiv.org/abs/1810.04805), 2018.

This TF Hub model uses the implementation of BERT from the
TensorFlow Models repository on GitHub at
[tensorflow/models/official/nlp/bert](https://github.com/tensorflow/models/tree/master/official/nlp/bert).
It uses L=12 hidden layers (i.e., Transformer blocks),
a hidden size of H=768,
and A=12 attention heads.

This model has been pre-trained for Chinese
on the Wikipedia
using the code published on GitHub.
 For training, random input masking has been applied independently to word pieces
(as in the original BERT paper).

All parameters in the module are trainable, and fine-tuning all parameters is
the recommended practice.


## Usage

This model is called as follows on tokenized text input,
an input mask to hold out padding tokens,
and segment types when input mixes with different segments.

```python
max_seq_length = 128  # Your choice here.
input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                       name="input_word_ids")
input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                   name="input_mask")
segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                    name="segment_ids")
bert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/1",
                            trainable=True)
pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
```

There are two outputs:
a `pooled_output` of shape `[batch_size, 768]` with
representations for the entire input sequences and
a `sequence_output` of shape `[batch_size, max_seq_length, 768]`
with representations for each input token (in context).

The tokenization of input text can be performed in Python with the
`FullTokenizer` class from
[tensorflow/models/official/nlp/bert/tokenization.py](https://github.com/tensorflow/models/blob/master/official/nlp/bert/tokenization.py).
Its vocab_file is stored as a`tf.saved_model.Asset` and
the do_lower_case flag is stored as a `tf.Variable` object
on the SavedModel. They can be retrieved (using TensorFlow Hub 0.7.0 or newer)
as follows:

```python
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
```

For complete usage examples, see
[run_classifier.py](https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_classifier.py)
and [run_squad.py](https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_squad.py)
from [tensorflow/models/official/nlp/bert/](https://github.com/tensorflow/models/tree/master/official/nlp/bert)
on GitHub.
