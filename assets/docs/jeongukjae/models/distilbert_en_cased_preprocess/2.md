# Module jeongukjae/distilbert_en_cased_preprocess/2

Text preprocessing model for `jeongukjae/distilbert_en_cased_L-6_H-768_A-12/1`.

<!-- asset-path: https://storage.googleapis.com/jeongukjae-tf-models/distilbert-update-preprocessor/distilbert_en_cased_preprocess.tar.gz -->
<!-- task: text-preprocessing -->
<!-- fine-tunable: false -->
<!-- license: apache-2.0 -->
<!-- format: saved_model_2 -->
<!-- language: en -->

## Overview

This model is a text preprocessing model for [`jeongukjae/distilbert_en_cased_L-6_H-768_A-12/1`](https://tfhub.dev/jeongukjae/distilbert_en_cased_L-6_H-768_A-12/1). This model uses same vocab file with [tensorflow/bert_en_cased_preprocess/3](https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3) and has almost same feature, but does not return token type ids, since distilBERT does not use token type ids.

This model has no trainable parameters and can be used in an input pipeline outside the training loop.

## Prerequisites

This model uses [`BertTokenizer`](https://www.tensorflow.org/text/api_docs/python/text/BertTokenizer) in TensorFlow Text. You can register required ops as follows.

```python
# Install it with "pip install tensorflow-text"
import tensorflow_text as text
```

## Usage

This model supports preprocessing single or multi segment text inputs.

### Preprocess single text segment

```python
sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name="sentences")
preprocessor = hub.KerasLayer("https://tfhub.dev/jeongukjae/distilbert_en_cased_preprocess/2")
encoder_inputs = preprocessor(sentences)
```

### Preprocess multiple text segments

```python
preprocessor = hub.load("https://tfhub.dev/jeongukjae/distilbert_en_cased_preprocess/2")
tokenize = hub.KerasLayer(preprocessor.tokenize)
bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs)
# You can use different sequence length like below. (default is 128)
#
# bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs, arguments=dict(seq_length=64))

sentences = [
    tf.keras.layers.Input(shape=(), dtype=tf.string, name="segment_a"),
    tf.keras.layers.Input(shape=(), dtype=tf.string, name="segment_b"),
]
tokenized_sentences = [tokenize(segment) for segment in sentences]
encoder_inputs = bert_pack_inputs(tokenized_sentences)
```

### Output details

The result of preprocessing is a batch of fixed-length input sequences for the DistilBERT encoder.

An input sequence starts with one start-of-sequence token, followed by the tokenized segments, each terminated by one end-of-segment token. Remaining positions up to `seq_length`, if any, are filled up with padding tokens. If an input sequence would exceed `seq_length`, the tokenized segments in it are truncated to prefixes of approximately equal sizes to fit exactly.

The `encoder_inputs` are a dict of two int32 Tensors, all with shape `[batch_size, seq_length]`, whose elements represent the batch of input sequences as follows:

* `"input_word_ids"`: has the token ids of the input sequences.
* `"input_mask"`: has value 1 at the position of all input tokens present before padding and value 0 for the padding tokens.

## Changelog

### Version 1

* Initial release.

### Version 2

* Fix a bug with argument `seq_length`.
