# Module google/universal-sentence-encoder-large/2
Encoder of greater-than-word length text trained on a variety of data.

<!-- module-type: text-embedding -->
<!-- asset-path: legacy -->
<!-- network-architecture: Transformer -->
<!-- language: en -->
<!-- fine-tunable: true -->
<!-- format: hub -->


[![Open Colab notebok]](https://colab.research.google.com/github/tensorflow/hub/blob/50bbebaa248cff13e82ddf0268ed1b149ef478f2/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb)


## Overview

The Universal Sentence Encoder encodes text into high dimensional vectors that
can be used for text classification, semantic similarity, clustering and other
natural language tasks.

The model is trained and optimized for greater-than-word length text, such as
sentences, phrases or short paragraphs. It is trained on a variety of data
sources and a variety of tasks with the aim of dynamically accommodating a wide
variety of natural language understanding tasks. The input is variable length
English text and the output is a 512 dimensional vector. The
universal-sentence-encoder-large model is trained with a Transformer encoder.

To learn more about text embeddings, refer to the [TensorFlow Embeddings](https://www.tensorflow.org/guide/embedding)
documentation. Our encoder differs from word level embedding models in that we
train on a number of natural language prediction tasks that require modeling the
meaning of word sequences rather than just individual words. Details are
available in the paper "Universal Sentence Encoder" [1].

#### Example use

```python
embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder-large/2")
embeddings = embed([
    "The quick brown fox jumps over the lazy dog.",
    "I am a sentence for which I would like to get its embedding"])

print(session.run(embeddings))

# The following are example embedding output of 512 dimensions per sentence
# Embedding for: The quick brown fox jumps over the lazy dog.
# [0.0387121252716, -0.00189059402328, -0.00533464271575, ...]
# Embedding for: I am a sentence for which I would like to get its embedding.
# [0.0489358529449, -0.0615833997726, 0.0032471306622, ...]
```

This module is about 800MB. Depending on your network speed, it might take a while
to load the first time you instantiate it. After that, loading the model should
be faster as modules are cached by default
([learn more about caching](https://www.tensorflow.org/hub/tf1_hub_module)). Further,
once a module is loaded to memory, inference time should be relatively fast.

Please see [Universal Sentence
Encoder](https://tfhub.dev/google/universal-sentence-encoder/2) for details and
see [this
notebook](https://colab.research.google.com/github/tensorflow/hub/blob/50bbebaa248cff13e82ddf0268ed1b149ef478f2/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb)
for code examples.

#### Preprocessing
The module performs best effort text input preprocessing, therefore it is not
required to preprocess the data before applying the module.

## Changelog

#### Version 1
*  Initial release.

#### Version 2
*  Exposed internal variables as Trainable.

## References

[1] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco,
Rhomni St. John, Noah Constant, Mario Guajardo-CÃ©spedes, Steve Yuan, Chris Tar,
Yun-Hsuan Sung, Brian Strope, Ray Kurzweil. [Universal Sentence Encoder](https://arxiv.org/abs/1803.11175).
arXiv:1803.11175, 2018.
