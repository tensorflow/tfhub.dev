# Module emilutz/vgg19-block4-conv2-unpooling-encoder/1

Image encoder based on vgg19 that stores argmax values for maxpool layers.

<!-- asset-path: https://storage.googleapis.com/vgg19-with-unpooling/vgg19-block4-conv2/encoder.tar.gz -->
<!-- module-type: image-feature-vector -->
<!-- fine-tunable: true -->
<!-- dataset: COCO 2017 -->
<!-- format: saved_model_2 -->
<!-- network-architecture: VGG-style -->
<!-- license: MIT -->

## Overview

### Module description

This model accepts as input an image and outputs the features extracted from block4_conv2 level of VGG19. It served as a feature extractor for training an encoder-decoder structure based on VGG19 and is part of a larger collection of such pairs designed to offer the ability to manipulate images by changing the latent space. One example of such use case can be found in style transfer applications like https://arxiv.org/abs/1802.06474. The addition of unpooling layers (as oposed to using regular upsampling methods) ensures fine image details are reconstructed with higher precision and without introducing artifacts.

### Model architecture

This model contains only the component enclosed in the dashed line below.

<img src="https://storage.googleapis.com/vgg19-with-unpooling/vgg19-block4-conv2/encoder.png" height="256">

### Input

*   Inputs are expected to be 3-channel RGB images with values in range [0, 255] and type float32
*   Shape: `(B, H, W, 3)`
*   `H` and `W` must be multiples of 8 :warning:

### Output

*   Outputs are extracted image features + argmax indices of the pooling layer
*   Shape: `[(B, H/8, W/8, 512), (B * H * W * 16, 4), (B * H * W * 8, 4), (B * H * W * 4, 4)]`

## Usage

### Use SavedModel in Python

```python
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub

# Load encoder and decoder
encoder = hub.KerasLayer('https://tfhub.dev/emilutz/vgg19-block4-conv2-unpooling-encoder/1') # this model
decoder = hub.KerasLayer('https://tfhub.dev/emilutz/vgg19-block4-conv2-unpooling-decoder/1') # external model

# Read image from disk and add batch size
image_file = tf.io.read_file(image_path)
image_uint8 = tf.image.decode_image(image_file)
image_float32 = tf.cast(image_uint8, tf.float32)
image_batch = tf.expand_dims(image_float32, axis=0)

# Encode and decode the image
[image_features, b1_indices, b2_indices, b3_indices] = encoder(image_batch)
image_decoded = decoder([image_features, b1_indices, b2_indices, b3_indices])

reconstruction_loss = np.abs((image_decoded - image_batch).numpy()).mean()
```

## Training

While training its corresponding decoder (which is a mirror of this encoder) the weights of this model have been kept frozen. The pair was trained on the COCO 2017 dataset for 30 epochs using Adam optimizer with default parameters and a linear learning rate decay between 0.001 and 0.0002.

### Training dataset

COCO 2017 Unlabelled images (123K) https://cocodataset.org/#download.
- 90% used for training (111063 images)
- 9% used for validation (11106 images)
- 1% left for testing (1234 images)
