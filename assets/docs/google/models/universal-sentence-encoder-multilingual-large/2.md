# Module google/universal-sentence-encoder-multilingual-large/2

16 languages (Arabic, Chinese-simplified, Chinese-traditional, English, French,
German, Italian, Japanese, Korean, Dutch, Polish, Portuguese, Spanish, Thai,
Turkish, Russian) text encoder.

<!-- module-type: text-embedding -->
<!-- asset-path: legacy -->
<!-- network-architecture: Transformer -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->

### Model Details

*   Developed by researchers at Google, 2019, v2 [1].
*   Transformer.
*   Covers 16 languages, showing strong performance on cross-lingual retrieval.
*   The input to the module is variable length text in any of the aforementioned
    languages and the output is a 512 dimensional vector.
*   *Input text can have arbitrary length!* However, model time and space
    complexity is *O(n^2)* for input length *n*. We recommend inputs that are
    approximately one sentence in length.
*   A smaller model with a simpler encoder architecture is
    [available](https://tfhub.dev/google/universal-sentence-encoder-multilingual/2)
    that has time and space requirements that scale *O(n)* with input length.

To learn more about text embeddings, refer to the
[TensorFlow Embeddings](https://www.tensorflow.org/tutorials/text/word_embeddings)
documentation.

### Intended Use

*   The model is intented to be used for text classification, text clustering,
    semantic textural similarity retrieval, cross-lingual text retrieval, etc.

### Factors

*   The Universal Sentence Encoder Multilingual module is an extension of the
    [Universal Sentence Encoder Large](https://tfhub.dev/google/universal-sentence-encoder-large/4)
    that includes training on multiple tasks across languages.
*   The multi-task training setup is based on the paper "Learning Cross-lingual
    Sentence Representations via a Multi-task Dual Encoder" [2].
*   This specific module is optimized for mutli-word length text, such as
    sentences, phrases or short paragraphs.
*   Important: notice that the language of the text input does not need to be
    specified, as the model was trained such that text across languages with
    similar meanings will have close embeddings.

#### Universal Sentence Encoder family

There are several versions of universal sentence encoder models trained with
different goals including size/performance multilingual, and fine-grained
question answer retrieval.

*   [universal-sentence-encoder](https://tfhub.dev/google/universal-sentence-encoder/3)
*   [universal-sentence-encoder-large](https://tfhub.dev/google/universal-sentence-encoder-large/4)
*   [universal-sentence-encoder-qa](https://tfhub.dev/google/universal-sentence-encoder-qa/2)
*   [universal-sentence-encoder-lite](https://tfhub.dev/google/universal-sentence-encoder-lite/2)
*   [universal-sentence-encoder-multilingual](https://tfhub.dev/google/universal-sentence-encoder-multilingual/2)
*   [universal-sentence-encoder-multilingual-qa](https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/2)

### Metrics

*   We apply this model to the
    [STS benchmark](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) for
    semantic similarity. The eval can be seen in the
    [example notebook](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb)
    made available. Results are shown below:

    STSBenchmark                     | dev   | test
    :------------------------------- | ----: | ----:
    Pearson's corrletion coefficient | 0.837 | 0.825

*   For semantic similarity retrieval, we evaluate the model on
    [Quora and AskUbuntu retrieval task](https://arxiv.org/abs/1811.08008).

    Dataset               | Quora | AskUbuntu | Average
    :-------------------- | ----: | --------: | ------:
    Mean Averge Precision | 89.1  | 42.3      | 65.7

*   For the translation pair retrieval, we evaluate the model on the United
    Nation Parallal Corpus:

    Language Pair | en-es | en-fr | en-ru | en-zh
    :------------ | :---: | ----: | ----: | ----:
    Precision@1   | 86.1  | 83.3  | 88.9  | 78.8

#### Prerequisites

This module relies on the [Tensorflow Text](https://github.com/tensorflow/text)
for input preprocessing. On
[Google Colaboratory](https://colab.research.google.com/), the Tensorflow Text
library is available by:

```python
!pip3 install tensorflow_text>=2.0.0rc0
```

#### Example use

```python
import tensorflow_hub as hub
import numpy as np
import tensorflow_text

# Some texts of different lengths.
english_sentences = ["dog", "Puppies are nice.", "I enjoy taking long walks along the beach with my dog."]
italian_sentences = ["cane", "I cuccioli sono carini.", "Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane."]
japanese_sentences = ["犬", "子犬はいいです", "私は犬と一緒にビーチを散歩するのが好きです"]

embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/2")

# Compute embeddings.
en_result = embed(english_sentences)["output"]
it_result = embed(italian_sentences)["output"]
ja_result = embed(japanese_sentences)["output"]

# Compute similarity matrix. Higher score indicates greater similarity.
similarity_matrix_it = np.inner(en_result, it_result)
similarity_matrix_ja = np.inner(en_result, ja_result)
```

# Changelog

#### Version 1

*   Initial release.

#### Version 2

*   Retrained using TF2.

## References

[1] Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant,
Gustavo Hernandez Abrego , Steve Yuan, Chris Tar, Yun-hsuan Sung, Ray Kurzweil.
[Multilingual Universal Sentence Encoder for Semantic Retrieval](https://arxiv.org/abs/1907.04307).
July 2019

[2] Muthuraman Chidambaram, Yinfei Yang, Daniel Cer, Steve Yuan, Yun-Hsuan Sung,
Brian Strope, Ray Kurzweil. [Learning Cross-Lingual Sentence Representations via
a Multi-task Dual-Encoder Model](https://arxiv.org/abs/1810.12836).
Repl4NLP@ACL, July 2019.
