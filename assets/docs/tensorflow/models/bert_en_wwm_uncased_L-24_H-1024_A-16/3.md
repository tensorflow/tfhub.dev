# Module tensorflow/&zwnj;bert_en_wwm_uncased_L-24_H-1024_A-16/3
Bidirectional Encoder Representations from Transformers (BERT).

<!-- dataset: Wikipedia and BooksCorpus -->
<!-- asset-path: legacy -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->

## Overview

BERT (Bidirectional Encoder Representations from Transformers)
provides dense vector representations for natural language
by using a deep, pre-trained neural network with the Transformer
architecture. It was originally published by

  * Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova:
    ["BERT: Pre-training of Deep Bidirectional Transformers for
    Language Understanding"](https://arxiv.org/abs/1810.04805), 2018.

This TF Hub model uses the implementation of BERT from the
TensorFlow Models repository on GitHub at
[tensorflow/models/official/nlp/bert](https://github.com/tensorflow/models/tree/master/official/nlp/bert).
It uses L=24 hidden layers (i.e., Transformer blocks),
a hidden size of H=1024,
and A=16 attention heads.

The weights of this model are those released by the original BERT authors.
This model has been pre-trained for English
on the Wikipedia and BooksCorpus.
Text inputs have been normalized the "uncased" way, meaning that the text has
been lower-cased before tokenization into word pieces, and any accent markers
have been stripped. For training, random input masking as been applied to whole words (i.e.,
the pieces of each input word are randomly masked all at once or not).

All parameters in the module are trainable, and fine-tuning all parameters is
the recommended practice.


## Usage

This model expects three int32 Tensors as input: numeric token ids,
an input mask to hold out padding tokens,
and input types to mark different segments within one input (if any).
The separate **preprocessing model** at
[https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1)
transforms plain text inputs into this format.

```python
seq_length = 128  # Your choice here.
inputs = dict(
    input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32))
encoder = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/3",
    trainable=True)
outputs = encoder(inputs)
pooled_output = outputs["pooled_output"]      # [batch_size, 1024].
sequence_output = outputs["sequence_output"]  # [batch_size, seq_length, 1024].
```

The encoder's outputs are the `pooled_output` to represents each input sequence
as a whole, and the `sequence_output` to represent each input token in context.

For advanced uses, the intermediate activations of all L=24
Transformer blocks (hidden layers) are returned as a Python list:
`outputs["encoder_outputs"][i]` is a Tensor
of shape `[batch_size, seq_length, 1024]`
with the outputs of the i-th Transformer block, for `0 <= i < L`.
The last value of the list is equal to `sequence_output`.


## Changelog

### Version 3

  * Uses dicts (not lists) for inputs and outputs.
  * Comes with a companion model for preprocessing of plain text.
  * For legacy users, this version still provides the now-obsolete `.vocab_file`
    and `.do_lower_case` attributes on `bert_layer.resolved_object`.

### Version 2

  * Fixes missing dropout. Previously, dropout was not applied when re-training
    the SavedModel, possibly resulting in a loss of quality.
  * Improves shape propagation. Applying the model to inputs of statically
    known `max_seq_length` now produces `sequence_output` with known dimension 1.

### Version 1

  * Initial release.
