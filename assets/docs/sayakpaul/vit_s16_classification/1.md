# Module sayakpaul/vit_s16_classification/1

Vision Transformer (ViT) fine-tuned on ImageNet-1k.

<!-- asset-path: https://storage.googleapis.com/flowers-experimental/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.tar.gz  -->
<!-- task: image-classification -->
<!-- network-architecture: vit -->
<!-- format: saved_model_2 -->
<!-- fine-tunable: true -->
<!-- license: apache-2.0 -->
<!-- colab: https://colab.research.google.com/github/sayakpaul/ViT-jax2tf/blob/main/classification.ipynb -->


## Overview

This model is a Vision Transformer of type S-16 [1, 2] pre-trained on the ImageNet-21k dataset [3] and fine-tuned on the
ImageNet-1k dataset [3]. You can find the complete
collection of Vision Transformers on TF-Hub from [this page](https://tfhub.dev/sayakpaul/collections/vision_transformer/1).

## Using this model

```python
model = tf.keras.Sequential([
    hub.KerasLayer("https://tfhub.dev/sayakpaul/vit_s16_classification/1")
])
predictions = model.predict(images) 
```

Inputs to the model must:

1. be four dimensional Tensors of the shape `(batch_size, height, width, num_channels)`. Note that the model expects images with  `channels_last`  property. `num_channels` must be 3. 
2. be resized to 224x224 resolution.
3. have pixel values in the range `[-1, 1]`.

The model also has a [feature extractor variant](https://tfhub.dev/sayakpaul/vit_s16_fe/1) which should be used for the purposes of transfer learning. For more details, please refer to the accompanying Colab Notebook. _**Please make sure the inputs are in [-1, 1] range as opposed to [0, 1].**_

## Notes

The original model weights are provided as a `.npz` file [4]. There were ported to a TensorFlow SavedModel. The porting steps are available in [5].

## References

[1] [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.](https://arxiv.org/abs/2010.11929) 

[2] [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers by Steiner et al.](https://arxiv.org/abs/2106.10270)

[3] [ImageNet database](https://www.image-net.org/challenges/LSVRC/2012/index.php)  

[4] [Vision Transformer GitHub](https://github.com/google-research/vision_transformer)

[5] [Colab Notebook for assembling ViT models in TensorFlow](https://colab.research.google.com/github/sayakpaul/ViT-jax2tf/blob/main/conversion.ipynb)