# Module google/answer_equivalence/bem/1

BERT model trained on the _Answer Equivalence Dataset_.

<!-- asset-path: internal -->
<!-- task: text-question-answering -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- network-architecture: bert -->
<!-- dataset: other -->
<!-- colab: https://colab.research.google.com/github/google-research-datasets/answer-equivalence-dataset/blob/main/Answer_Equivalence_BEM_example.ipynb -->

## Overview

BERT model trained on the [Answer Equivalence Dataset](https://github.com/google-research-datasets/answer-equivalence-dataset).

### Example use

```python
import tensorflow_hub as hub

# Create BERT inputs of the usual form:
input_ids, segment_ids = tokenize_and_convert_to_ids(...)
inputs = {
  'input_ids': input_ids,
  'segment_ids': segment_ids
  }

# Load BEM model from TFHub.
bem = hub.load('https://tfhub.dev/google/answer_equivalence/bem/1')

# The outputs are raw logits.
raw_outputs = bem(inputs)

# They can be transformed into a classification 'probability' like so:
bem_score = softmax(np.squeeze(raw_outputs))[1]
```
