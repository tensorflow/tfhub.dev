# Module google/answer_equivalence/bem/1

BERT model trained on the Answer Equivalence Dataset.

<!-- asset-path: internal -->
<!-- task: text-question-answering -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- network-architecture: bert -->
<!-- dataset: other -->
<!-- colab: https://colab.research.google.com/github/google-research-datasets/answer-equivalence-dataset/blob/main/Answer_Equivalence_BEM_example.ipynb -->

## Overview

BERT model trained on the [Answer Equivalence Dataset](https://github.com/google-research-datasets/answer-equivalence-dataset).

This model classifies whether two answers to the same question are equivalent, even when they look different on the surface.

Consider this example:

```
question = 'how is the weather in california'
reference answer = 'infrequent rain'
candidate answer = 'rain'
bem(question, reference, candidate) ~ 0
```

This model can be used as a metric to evaluate automatic question answering systems: when the produced answer is different from the reference, it might still be equivalent to the reference and hence count as correct.

See our paper [Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation](https://arxiv.org/abs/2202.07654) for a detailed explanation of how the data was collected and how this metric compares to others such as exact match of F1.



### Example use

```python
import tensorflow_hub as hub

# Create BERT inputs of the usual form:
input_ids, segment_ids = tokenize_and_convert_to_ids(...)
inputs = {
  'input_ids': input_ids,
  'segment_ids': segment_ids
  }

# Load BEM model from TFHub.
bem = hub.load('https://tfhub.dev/google/answer_equivalence/bem/1')

# The outputs are raw logits.
raw_outputs = bem(inputs)

# They can be transformed into a classification 'probability' like so:
bem_score = softmax(np.squeeze(raw_outputs))[1]
```
