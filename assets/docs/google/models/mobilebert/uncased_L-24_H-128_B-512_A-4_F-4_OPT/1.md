# Module google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/1

MobileBert pre-trained encoder (for a fully TF2-compatible version, see
https://tfhub.dev/tensorflow/mobilebert_en_uncased_L-24_H-128_B-512_A-4_F-4_OPT)

<!-- module-type: text-classification -->
<!-- asset-path: legacy -->
<!-- fine-tunable: true -->
<!-- format: hub -->
<!-- language: en -->
<!-- network-architecture: MobileBert uncased_L-24_H-128_B-512_A-4_F-4_OPT -->
<!-- dataset: wikipedia-and-bookscorpus -->

## Overview

MobileBERT is a thin version of BERT, while equipped with bottleneck structures
and a carefully designed balance between self-attentions and feed-forward
networks.

*   Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou:
    ["MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"](https://arxiv.org/abs/2004.02984), 2020.

This TF Hub model uses the implementation of MobileBERT from the Google Research
repository on GitHub at
[google-research/google-research/tree/master/mobilebert](https://github.com/google-research/google-research/tree/master/mobilebert).
The
[model configuration](https://github.com/google-research/google-research/blob/master/mobilebert/config/uncased_L-24_H-128_B-512_A-4_F-4_OPT.json)
is a variant of Transformer with L=24 hidden layers (i.e., Transformer blocks),
a hidden size of H=128, B=512 as bottleneck size, A=4 attention heads, and F=4
inner feed-forward layers.

It is trained through distillation from a teacher BERT model.

## Usages

This model is called as follows on tokenized text input, an input mask to hold
out padding tokens, and segment types when input mixes with different segments.

```python
import tensorflow.compat.v1 as tf
import tensorflow_hub as hub

tags = set(["train"])
hub_url = 'https://tfhub.dev/google/mobilebert/uncased_L-24_H-128_B-512_A-4_F-4_OPT/1'
bert_module = hub.Module(hub_url, tags=tags, trainable=True)
bert_inputs = dict(
    input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)
bert_outputs = bert_module(bert_inputs, signature="tokens", as_dict=True)
final_hidden = bert_outputs["sequence_output"]
```

For complete usage examples, see
[run_squad.py](https://github.com/google-research/google-research/tree/master/mobilebert/run_squad.py)
from
[google-research/mobilebert](https://github.com/google-research/google-research/tree/master/mobilebert)
on GitHub.

This module is also used in
[TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/guide/model_maker)
in the
[question answer task](https://www.tensorflow.org/lite/tutorials/model_maker_question_answer)
and
[text classification task](https://www.tensorflow.org/lite/tutorials/model_maker_text_classification).
