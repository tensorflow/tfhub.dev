# Module google/universal-sentence-encoder-multilingual/1

16 languages (Arabic, Chinese-simplified, Chinese-traditional, English, French,
German, Italian, Japanese, Korean, Dutch, Polish, Portuguese, Spanish, Thai,
Turkish, Russian) text encoder.

<!-- module-type: text-embedding -->
<!-- asset-path: legacy -->
<!-- network-architecture: cnn -->
<!-- fine-tunable: true -->
<!-- format: hub -->


[![Open Colab notebook]](https://colab.research.google.com/github/tensorflow/hub/blob/3880b82596d2cf5401095b6ada51cb2d543c2050/examples/colab/cross_lingual_similarity_with_tf_hub_multilingual_universal_encoder.ipynb)

### Model Details

*   Developed by researchers at Google, 2019, v1 [1].
*   Convolutional Neural Net.
*   Covers 16 languages, showing strong performance on cross-lingual retrieval.
*   The input to the module is variable length text in any of the aforementioned
    languages and the output is a 512 dimensional vector.
*   A larger model with more complicated encoder architecture is
    [available](https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/1).

To learn more about text embeddings, refer to the
[TensorFlow Embeddings](https://www.tensorflow.org/tutorials/text/word_embeddings)
documentation.

### Intended Use

*   The model is intented to be used for text classification, text clustering,
    semantic textural similarity retrieval, cross-lingual text retrieval, etc.

### Factors

*   The Universal Sentence Encoder Multilingual module is an extension of the
    [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/2)
    that includes training on multiple tasks across languages.
*   The multi-task training setup is based on the paper "Learning Cross-lingual
    Sentence Representations via a Multi-task Dual Encoder" [2].
*   This specific module is optimized for multi-word length text, such as
    sentences, phrases or short paragraphs.
*   Important: notice that the language of the text input does not need to be
    specified, as the model was trained such that text across languages with
    similar meanings will have close embeddings.

#### Universal Sentence Encoder family

There are several versions of universal sentence encoder models trained with
different goals including size/performance multilingual, and fine-grained
question answer retrieval.

*   [universal-sentence-encoder](https://tfhub.dev/google/universal-sentence-encoder/2)
*   [universal-sentence-encoder-large](https://tfhub.dev/google/universal-sentence-encoder-large/3)
*   [universal-sentence-encoder-lite](https://tfhub.dev/google/universal-sentence-encoder-lite/2)
*   [universal-sentence-encoder-multilingual-large](https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/1)
*   [universal-sentence-encoder-multilingual-qa](https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/1)

### Metrics

*   We apply this model to the
    [STS benchmark](https://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) for
    semantic similarity. The eval can be seen in the
    [example notebook](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb)
    made available. Results are shown below:

    STSBenchmark                      | dev   | test
    :-------------------------------- | ----: | ----:
    Pearson's correlation coefficient | 0.829 | 0.809

*   For semantic similarity retrieval, we evaluate the model on
    [Quora and AskUbuntu retrieval task](https://arxiv.org/abs/1811.08008).

    Dataset               | Quora | AskUbuntu | Average
    :-------------------- | ----: | --------: | ------:
    Mean Averge Precision | 89.2  | 39.9      | 64.6

*   For the translation pair retrieval, we evaluate the model on the United
    Nation Parallal Corpus:

    Language Pair | en-es | en-fr | en-ru | en-zh
    :------------ | :---: | ----: | ----: | ----:
    Precision@1   | 85.8  | 82.7  | 87.4  | 79.5

#### Prerequisites

This module relies on the
[SentencePiece library](https://github.com/google/sentencepiece) for input
preprocessing (please make sure the TensorFlow version you use is compatible
with the SentencePiece library). On
[Google Colaboratory](https://colab.research.google.com/), the SentencePiece
library is available by:

```python
!pip3 install sentencepiece
!pip3 install tf-sentencepiece
```

**NOTE:** The combination of tensorflow 1.14.0 and tf-sentencepiece 0.1.82.1
crashes ([GitHub issue #345](https://github.com/tensorflow/hub/issues/345))
when using this module.

#### Example use

```python
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import tf_sentencepiece

# Some texts of different lengths.
english_sentences = ["dog", "Puppies are nice.", "I enjoy taking long walks along the beach with my dog."]
italian_sentences = ["cane", "I cuccioli sono carini.", "Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane."]
japanese_sentences = ["犬", "子犬はいいです", "私は犬と一緒にビーチを散歩するのが好きです"]

# Graph set up.
g = tf.Graph()
with g.as_default():
  text_input = tf.placeholder(dtype=tf.string, shape=[None])
  embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder-multilingual/1")
  embedded_text = embed(text_input)
  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
g.finalize()

# Initialize session.
session = tf.Session(graph=g)
session.run(init_op)

# Compute embeddings.
en_result = session.run(embedded_text, feed_dict={text_input: english_sentences})
it_result = session.run(embedded_text, feed_dict={text_input: italian_sentences})
ja_result = session.run(embedded_text, feed_dict={text_input: japanese_sentences})

# Compute similarity matrix. Higher score indicates greater similarity.
similarity_matrix_it = np.inner(en_result, it_result)
similarity_matrix_ja = np.inner(en_result, ja_result)
```

## References

[1] Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant,
Gustavo Hernandez Abrego , Steve Yuan, Chris Tar, Yun-hsuan Sung, Ray Kurzweil.
[Multilingual Universal Sentence Encoder for Semantic Retrieval](https://arxiv.org/abs/1907.04307).
July 2019

[2] Muthuraman Chidambaram, Yinfei Yang, Daniel Cer, Steve Yuan, Yun-Hsuan Sung,
Brian Strope, Ray Kurzweil. [Learning Cross-Lingual Sentence Representations via
a Multi-task Dual-Encoder Model](https://arxiv.org/abs/1810.12836). To appear,
Repl4NLP@ACL, July 2019.
