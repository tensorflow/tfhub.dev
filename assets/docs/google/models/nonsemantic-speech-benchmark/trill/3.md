# Module google/nonsemantic-speech-benchmark/trill/3
The TRILL model presented in Towards Learning a Universal Non-Semantic Representation of Speech.
It exceeds state-of-the-art performance on a number of transfer learning tasks
drawn from the non-semantic speech domain (speech emotion recognition, language
identification, etc). It is trained on publicly-available AudioSet.

<!-- asset-path: internal -->
<!-- task: audio-embedding -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- network-architecture: resnet -->
<!-- dataset: audioset -->

## Overview

The TRILL model presented in
[Towards Learning a Universal Non-Semantic Representation of Speech](http://arxiv.org/abs/2002.12764).
The
[github page](https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark)
contains the code to reproduce the paper's results, more code examples, and the
evaluation code to run new embeddings on the Non-Semantic Speech Benchmark
(NOSS).

If you use this model, please cite the following:

```
@inproceedings{shor20_interspeech,
  author={Joel Shor and Aren Jansen and Ronnie Maor and Oran Lang and Omry Tuval and FÃ©lix de Chaumont Quitry and Marco Tagliasacchi and Ira Shavitt and Dotan Emanuel and Yinnon Haviv},
  title={{Towards Learning a Universal Non-Semantic Representation of Speech}},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={140--144},
  doi={10.21437/Interspeech.2020-1242}
}
```

**NOTE**: This model has **two outputs**. In practice, the **layer19 output**
was **better on all downstream datasets** we tested on. The first layer, 
"embedding," is included in order to reproduce the results from the paper.

### TF 2.X

To run the model in TF 2:

```python
# Import TF 2.X and make sure we're running eager.
import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()
assert tf.executing_eagerly()

import tensorflow_hub as hub
import numpy as np

# Load the module and run inference.
module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/trill/3')
# `wav_as_float_or_int16` can be a numpy array or tf.Tensor of float type or
# int16. The sample rate must be 16kHz. Resample to this sample rate, if
# necessary.
wav_as_float_or_int16 = np.sin(np.linspace(-np.pi, np.pi, 128), dtype=np.float32)
emb_dict = module(samples=wav_as_float_or_int16, sample_rate=16000)
# For a description of the difference between the two endpoints, please see our
# paper (https://arxiv.org/abs/2002.12764), section "Neural Network Layer".
emb = emb_dict['embedding']
emb_layer19 = emb_dict['layer19']
# Embeddings are a [time, feature_dim] Tensors.
emb.shape.assert_is_compatible_with([None, 512])
emb_layer19.shape.assert_is_compatible_with([None, 12288])

print(emb)
print(emb_layer19)
```

### TF 1.X

Generating the embedding in TF 1.X is very similar, we just need to run the
graph in a `tf.Session`:

```python
# Import TF 1.X.
import tensorflow.compat.v1 as tf
assert not tf.executing_eagerly()

import tensorflow_hub as hub
import numpy as np

# Load the module and run inference.
module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/trill/3')
# `wav_as_float_or_int16` can be a numpy array or tf.Tensor of float type or
# int16. The sample rate must be 16kHz. Resample to this sample rate, if
# necessary.
wav_as_float_or_int16 = np.sin(np.linspace(-np.pi, np.pi, 128), dtype=np.float32)
emb_dict = module(samples=wav_as_float_or_int16, sample_rate=16000)
# For a description of the difference between the two endpoints, please see our
# paper (https://arxiv.org/abs/2002.12764), section "Neural Network Layer".
emb = emb_dict['embedding']
emb_layer19 = emb_dict['layer19']
# Embeddings are a [time, feature_dim] Tensors.
emb.shape.assert_is_compatible_with([None, 512])
emb_layer19.shape.assert_is_compatible_with([None, 12288])

with tf.train.MonitoredSession() as sess:
  emb_np, layer19_np = sess.run([emb, emb_layer19])

print(emb_np)
print(layer19_np)
```
