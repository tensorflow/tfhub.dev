# Module tensorflow/&zwnj;albert_en_large/2
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations

<!-- asset-path: legacy -->
<!-- dataset: Wikipedia and BooksCorpus -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->


## Overview

ALBERT is "A Lite" version of BERT with greatly reduced number of parameters. It
was originally published by

*   Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,
    Radu Soricut. [ALBERT: A Lite BERT for Self-supervised Learning of Language
    Representations](https://arxiv.org/abs/1909.11942). arXiv preprint
    arXiv:1909.11942, 2019.

This TF Hub model uses the implementation of ALBERT from the
TensorFlow Models repository on GitHub at
[tensorflow/models/official/nlp/modeling/networks/albert_encoder.py](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/networks/albert_encoder.py).
It uses L=24 hidden layers (i.e., Transformer blocks),
a hidden size of H=1024,
and A=16 attention heads.

The weights of this TF2 SavedModel have been converted from
[albert_large](https://tfhub.dev/google/albert_large/3)
in TF1 Hub module format.

All parameters in the module are trainable, and fine-tuning all parameters is
the recommended practice.


## Usage

This model expects three int32 Tensors as input: numeric token ids,
an input mask to hold out padding tokens,
and input types to mark different segments within one input (if any).
The separate **preprocessing model** at
[http://tfhub.dev/tensorflow/albert_en_preprocess/1](http://tfhub.dev/tensorflow/albert_en_preprocess/1)
transforms plain text inputs into this format.

```python
seq_length = 128  # Your choice here.
inputs = dict(
    input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32))
encoder = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/albert_en_large/2",
    trainable=True)
outputs = encoder(inputs)
pooled_output = outputs["pooled_output"]      # [batch_size, 1024].
sequence_output = outputs["sequence_output"]  # [batch_size, seq_length, 1024].
```

The encoder's outputs are the `pooled_output` to represents each input sequence
as a whole, and the `sequence_output` to represent each input token in context.

For advanced uses, the intermediate activations of all L=24
Transformer blocks (hidden layers) are returned as a Python list:
`outputs["encoder_outputs"][i]` is a Tensor
of shape `[batch_size, seq_length, 1024]`
with the outputs of the i-th Transformer block, for `0 <= i < L`.
The last value of the list is equal to `sequence_output`.


## Changelog

### Version 2

  * Uses dicts (not lists) for inputs and outputs.
  * Comes with a companion model for preprocessing of plain text.
  * For legacy users, this version still provides the now-obsolete
    `albert_layer.resolved_object.sp_model_file` asset.

### Version 1

  * Initial release.
