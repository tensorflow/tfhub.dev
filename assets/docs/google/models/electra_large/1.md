# Module google/&zwnj;electra_large/1
ELECTRA-1.75M: A BERT-like model pre-trained as a discriminator.

<!-- fine-tunable: true -->
<!-- asset-path: legacy -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->

## Overview

ELECTRA is a BERT-like model that is pre-trained as a discriminator of a
generative adversarial network (GAN). It was originally published by:

  * Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning:
  [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/forum?id=r1xMH1BtvB),
  ICLR 2020.


This version packages the original ELECTRA-1.75M checkpoint made available by
[Google Research](https://github.com/google-research/electra) as a
[TF2 SavedModel](https://www.tensorflow.org/hub/tf2_saved_model) using the
implementation of BERT from the
[TensorFlow Models repository](https://github.com/tensorflow/models/tree/master/official/nlp/bert).

In order to make the checkpoint compatible, the dense layer (pooler) on top of
the CLS token (which is non-existent in the original pretraining) was
initialized with the identity matrix.

## Usage

This model is called as follows on tokenized text input,
an input mask to hold out padding tokens,
and segment types when input mixes with different segments.

```python
max_seq_length = 128  # Your choice here.
input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                       name="input_word_ids")
input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                   name="input_mask")
segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                    name="segment_ids")
bert_layer = hub.KerasLayer("https://tfhub.dev/google/electra_large/1",
                            trainable=True)
pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
```

There are two outputs:
a `pooled_output` of shape `[batch_size, hidden_size]` with
representations for the entire input sequences and
a `sequence_output` of shape `[batch_size, max_seq_length, hidden_size]`
with representations for each input token (in context).

The tokenization of input text can be performed in Python with the
`FullTokenizer` class from
[tensorflow/models/official/nlp/bert/tokenization.py](https://github.com/tensorflow/models/blob/master/official/nlp/bert/tokenization.py).
Its vocab_file is stored as a`tf.saved_model.Asset` and
the do_lower_case flag is stored as a `tf.Variable` object
on the SavedModel. They can be retrieved (using TensorFlow Hub 0.7.0 or newer)
as follows:

```python
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
```

For complete usage examples, see
[run_classifier.py](https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_classifier.py)
and [run_squad.py](https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_squad.py)
from [tensorflow/models/official/nlp/bert/](https://github.com/tensorflow/models/tree/master/official/nlp/bert)
on GitHub.

## Changelog

### Version 1

  * Initial release.
