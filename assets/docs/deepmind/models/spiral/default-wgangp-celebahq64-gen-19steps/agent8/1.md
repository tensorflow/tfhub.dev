# Module deepmind/spiral/default-wgangp-celebahq64-gen-19steps/agent8/1

SPIRAL agent trained on the CelebA-HQ dataset using WGAN-GP objective. This
agent has index 8 in a population of 10 agents.

<!-- dataset: celeba-hq -->
<!-- asset-path: legacy -->
<!-- module-type: image-rnn-agent -->
<!-- network-architecture: other -->
<!-- fine-tunable: false -->
<!-- format: hub -->


## Overview

This module provides the policy network of the SPIRAL agent trained to sample
**19-strokes** drawings of human faces. The discriminator network is **not**
included in this release.

### Architecture

The architecture of the policy network is detailed in

*   Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami, Oriol
    Vinyals:
    ["Synthesizing programs for images using reinforced adversarial learning"](http://proceedings.mlr.press/v80/ganin18a.html),
    ICML 2018

This TF-Hub module uses the [Sonnet implementation]() of the network provided as
a part of the [`spiral`](https://github.com/deepmind/spiral) package for
`python`.

### Training

The weights for this module were obtained by training on the
[CelebA-HQ](https://github.com/tkarras/progressive_growing_of_gans) dataset.
CelebA-HQ consists of `30,000` images at `1024 x 1024` resolution taken from the
[CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset. It was
originally described in

*   Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen: ["Progressive Growing
    of GANs for Improved Quality, Stability, and
    Variation"](https://research.nvidia.com/publication/2017-10_Progressive-Growing-of),
    ICLR 2018.

In order to be compatible with the architecture, images were downsampled
bilinearly to the resolution of `64 x 64`.

The distributed setup used to train the model consisted of `10` policy learners
(each with *NVIDIA V100*), `1` discriminator learner (also with *NVIDIA V100*)
and `2560` actors (`256` actors per policy learner; *CPU only*). Note that
unlike in the original
[SPIRAL paper](http://proceedings.mlr.press/v80/ganin18a.html), only one
discriminator was used for the entire population of agents. As a result, it was
no longer necessary to maintain a replay of generated samples. This was replaced
by a regular queue. [PBT](https://arxiv.org/abs/1711.09846) was only applied to
the policy learners.

Each policy learner ran *Adam* optimizer with `epsilon = 1e-8`, `beta1 = 0.5`,
`beta1 = 0.999`, and a batch size of `64`. The initial learning rate was sampled
*log-uniformly* from the `[1e-5, 3e-4]` range. Similarly, the entropy cost was
sampled *log-uniformly* from `[2e-3, 1e-1]`.

The discriminator learner ran *Adam* optimizer with the same parameters except
the learning rate was set to a fixed value of `1e-4`. The slope and `gamma` in
the WGAN-GP objective were both chosen to be `1.0`.

## Usage

This module provides two methods: `initial_state` and `step`. The first returns
the initial state of the policy network, the second takes an observation from
the environment and produces an action. Both methods in their raw form are
difficult to deal with and therefore are not meant to be used directly. Instead,
it is suggested to load the module using the
[`spiral`](https://github.com/deepmind/spiral) (please refer to its
documentation for further details):

```python
import matplotlib.pyplot as plt

import spiral.agents.default as default_agent
import spiral.agents.utils as agent_utils
import spiral.environments.libmypaint as libmypaint


# The path to a TF-Hub module.
MODULE_PATH = "https://tfhub.dev/deepmind/spiral/default-wgangp-celebahq64-gen-19steps/agent8/1"
# The folder containing `libmypaint` brushes.
BRUSHES_PATH = "the/path/to/libmypaint-brushes"

# Here, we create an environment.
env = libmypaint.LibMyPaint(episode_length=20,
                            canvas_width=64,
                            grid_width=32,
                            brush_type="classic/dry_brush",
                            brush_sizes=[1, 2, 4, 8, 12, 24],
                            use_color=True,
                            use_pressure=True,
                            use_alpha=False,
                            background="white",
                            brushes_basedir=BRUSHES_PATH)


# Now we load the agent from a snapshot.
initial_state, step = agent_utils.get_module_wrappers(MODULE_PATH)

# Everything is ready for sampling.
state = initial_state()
noise_sample = np.random.normal(size=(10,)).astype(np.float32)

time_step = env.reset()
for t in range(19):
    time_step.observation["noise_sample"] = noise_sample
    action, state = step(time_step.step_type, time_step.observation, state)
    time_step = env.step(action)

# Show the sample.
plt.close("all")
plt.imshow(time_step.observation["canvas"], interpolation="nearest")
```

The code above will produce a single sample from the model.

## Changelog

#### Version 1

*   Initial release.
