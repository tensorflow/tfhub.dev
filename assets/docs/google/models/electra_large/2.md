# Module google/electra_large/2
ELECTRA-1.75M: A BERT-like model pre-trained as a discriminator.

<!-- asset-path: legacy -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- task: text-embedding -->
<!-- network-architecture: transformer -->
<!-- colab: https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/bert_glue.ipynb -->

## Overview

ELECTRA is a BERT-like model that is pre-trained as a discriminator
in a set-up resembling a
generative adversarial network (GAN). It was originally published by:

  * Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning:
    [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/forum?id=r1xMH1BtvB),
    ICLR 2020.


This version packages the original ELECTRA-1.75M checkpoint made available by
[Google Research](https://github.com/google-research/electra) as a
[TF2 SavedModel](https://www.tensorflow.org/hub/tf2_saved_model) using the
implementation of BERT from the
[TensorFlow Models repository](https://github.com/tensorflow/models/tree/master/official/nlp/bert).

In order to make the checkpoint compatible, the dense layer (pooler) on top of
the CLS token (which is non-existent in the original pretraining) was
initialized with the identity matrix.

## Usage

This SavedModel implements the encoder API for [text embeddings with transformer
encoders](https://www.tensorflow.org/hub/common_saved_model_apis/text#transformer-encoders).
It uses an associated **preprocessing model** at
[https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3) that
transforms plain text inputs into the format that can be fed into this model.

```
!pip3 install --quiet tensorflow-text

import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text  # Imports TF ops for preprocessing.

# Define some sentences to feed into the model
sentences = [
  "Here We Go Then, You And I is a 1999 album by Norwegian pop artist Morten Abel. It was Abel's second CD as a solo artist.",
  "The album went straight to number one on the Norwegian album chart, and sold to double platinum.",
  "Ceylon spinach is a common name for several plants and may refer to: Basella alba Talinum fruticosum",
  "A solar eclipse occurs when the Moon passes between Earth and the Sun, thereby totally or partly obscuring the image of the Sun for a viewer on Earth.",
  "A partial solar eclipse occurs in the polar regions of the Earth when the center of the Moon's shadow misses the Earth.",
]

# Load the BERT encoder and preprocessing models
preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')
bert = hub.load('https://tfhub.dev/google/electra_large/2')

# Convert the sentences to bert inputs
bert_inputs = preprocess(sentences)

# Feed the inputs to the model to get the pooled and sequence outputs
bert_outputs = bert(bert_inputs, training=False)
pooled_output = bert_outputs['pooled_output']
sequence_output = bert_outputs['sequence_output']

print('\nSentences:')
print(sentences)
print('\nPooled output:')
print(pooled_output)
print('\nSequence output:')
print(sequence_output)
```


## Changelog

### Version 2

  * Uses dicts (not lists) for inputs and outputs.
  * Works with a companion model for preprocessing of plain text.
  * For legacy users, this version still provides the now-obsolete `.vocab_file`
    and `.do_lower_case` attributes on `hub.KerasLayer.resolved_object`.

### Version 1

  * Initial release.
