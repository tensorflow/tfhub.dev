# Module jeongukjae/distilkobert_sentence_encoder/1

Korean sentence encoder model using `jeongukjae/distilkobert_cased_L-3_H-768_A-12/1`.

<!-- asset-path: https://storage.googleapis.com/jeongukjae-tf-models/distilkobert/distilkobert_sentence_encoder.tar.gz -->
<!-- network-architecture: transformer -->
<!-- task: text-embedding -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: ko -->

## Overview

This model is a fine-tuned sentence encoder model based on [`jeongukjae/distilkobert_cased_L-3_H-768_A-12/1`](https://tfhub.dev/jeongukjae/distilkobert_cased_L-3_H-768_A-12/1) using KorSTS and KLUE STS datasets. I applied knowledge distillation to train this model. You can simply extract Korean sentence embeddings efficiently with this model.

## Model Performance

### KorSTS development set

| Model                             | # Params | encoding strategy | Spearman correlation \* 100 |
| --------------------------------- | -------: | ----------------- | --------------------------: |
| **distilkobert_sentence_encoder** |      28M | bi-encoding       |                       86.53 |
| Korean SRoBERTa (base)†           |     111M | bi-encoding       |                       83.54 |
| Korean SRoBERTa (large)†          |     338M | bi-encoding       |                       84.21 |
| SXLM-R (base)†                    |     270M | bi-encoding       |                       81.95 |
| SXLM-R (large)†                   |     550M | bi-encoding       |                       84.13 |
| Korean RoBERTa (base)†            |     111M | cross-encoding    |                       84.97 |
| Korean RoBERTa (large)†           |     338M | cross-encoding    |                       87.82 |
| XLM-R (base)†                     |     270M | cross-encoding    |                       83.02 |
| XLM-R (large)†                    |     550M | cross-encoding    |                       88.37 |

- †: results from [Ham et al., 2020](https://arxiv.org/abs/2004.03289).

### KorSTS test set

| Model                             | # Params | encoding strategy | Spearman correlation \* 100 |
| --------------------------------- | -------: | ----------------- | --------------------------: |
| **distilkobert_sentence_encoder** |      28M | bi-encoding       |                       83.12 |
| Korean SRoBERTa (base)†           |     111M | bi-encoding       |                       80.29 |
| Korean SRoBERTa (large)†          |     338M | bi-encoding       |                       80.49 |
| SXLM-R (base)†                    |     270M | bi-encoding       |                       79.13 |
| SXLM-R (large)†                   |     550M | bi-encoding       |                       81.84 |
| Korean RoBERTa (base)†            |     111M | cross-encoding    |                       83.00 |
| Korean RoBERTa (large)†           |     338M | cross-encoding    |                       85.27 |
| XLM-R (base)†                     |     270M | cross-encoding    |                       77.78 |
| XLM-R (large)†                    |     550M | cross-encoding    |                       84.68 |

- †: results from [Ham et al., 2020](https://arxiv.org/abs/2004.03289).

### KLUE STS development set

| Model                             | # Params | encoding strategy | Pearson correlation \* 100 |
| --------------------------------- | -------: | ----------------- | -------------------------: |
| **distilkobert_sentence_encoder** |      28M | bi-encoding       |                      86.87 |
| KLUE-BERT (base)\*                |     110M | cross-encoding    |                      91.01 |
| KLUE-RoBERTa (base)\*             |     110M | cross-encoding    |                      92.91 |

- \*: results from [Park et al., 2021](https://arxiv.org/abs/2105.09680)

## Example Use

```python
# Load required models
encoder = hub.KerasLayer("https://tfhub.dev/jeongukjae/distilkobert_sentence_encoder/1")
preprocessor = hub.KerasLayer("https://tfhub.dev/jeongukjae/distilkobert_cased_preprocess/1")

# Define sentence encoder model
inputs = tf.keras.Input([], dtype=tf.string)
encoder_inputs = preprocessor(inputs)
sentence_embedding = encoder(encoder_inputs)
normalized_sentence_embedding = tf.nn.l2_normalize(sentence_embedding, axis=-1)
model = tf.keras.Model(inputs, normalized_sentence_embedding)

# Encode sentences using distilkobert_sentence_encoder
sentences1 = tf.constant([
    "다만, 도로와 인접해서 거리의 소음이 들려요.",
    "형이 다시 캐나다 들어가야 하니 가족모임 일정은 바꾸지 마세요.",
    "방안에 필요한 시설이 모두 있어서 매우 편리합니다.",
    "관광자원화 검토를 모범적으로 적용한 지자체에는 홍보·컨설팅, 관광상품 개발 지원 등을 제공할 계획이다.",
])
sentences2 = tf.constant([
    "하지만, 길과 가깝기 때문에 거리의 소음을 들을 수 있습니다.",
    "가족 모임 일정은 바꾸지 말도록 하십시오.",
    "특히, 숙소 근처에 안전한 실내 주차장이 있어서 편리합니다.",
    "아울러 지자체, 지역관광협회 등과 함께 수시로 관광지 현장을 점검할 계획이다.",
])
embeddings1 = model(sentences1)
embeddings2 = model(sentences2)

# Calculate cosine similarity
print(tf.tensordot(embeddings1, embeddings2, axes=[[1], [1]]))
# Expected outputs:
#
# tf.Tensor(
# [[ 0.8907616   0.07906969 -0.09612353  0.00167902]
#  [ 0.0184274   0.6840409  -0.1102942   0.02653065]
#  [-0.00795126 -0.10688838  0.5041443  -0.01270578]
#  [ 0.04684553 -0.0619101   0.00684686  0.68705124]], shape=(4, 4), dtype=float32)
```

## Output details

The output of this model is a tensor of shape `[batch size, hidden size(768 for this model)]`

## References

- [KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding](https://arxiv.org/abs/2004.03289)
- [KLUE: Korean Language Understanding Evaluation](https://arxiv.org/abs/2105.09680)
