# Module rishit-dagli/nnclr/1

NNCLR (Nearest-Neighbor Contrastive Learning of Visual Representations), a SSL approach pre-trained on STL-10.

<!-- task: image-classification -->
<!-- network-architecture: nnclr -->
<!-- dataset: stl10 -->
<!-- fine-tunable: false -->
<!-- license: apache-2.0 -->
<!-- format: saved_model_2 -->
<!-- asset-path: https://storage.googleapis.com/rishit-dagli/nnclr_model/saved_model.tar.gz -->
<!-- colab: https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/nnclr.ipynb -->

### TF2 SavedModel
This is a [SavedModel in TensorFlow 2 format](https://www.tensorflow.org/hub/tf2_saved_model). Using it requires TensorFlow 2 (or 1.15) and TensorFlow Hub 0.5.0 or newer.

### Overview

NNCLR is a self-supervised learning approach as proposed in the paper "With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations" [1], by Google Research and DeepMind pre-trained on STL-10.

NNCLR learns self-supervised representations that go beyond single-instance positives, which allows for learning better features that are invariant to different viewpoints, deformations, and even intra-class variations. Clustering based methods offer a great approach to go beyond single instance positives, but assuming the entire cluster to be positives could hurt performance due to early over-generalization. Instead, NNCLR uses nearest neighbors in the learned representation space as positives. In addition, NNCLR increases the performance of existing contrastive learning methods like SimCLR and reduces the reliance of self-supervised methods on data augmentation strategies.

### Usage

This model can be used with the `hub.load` as follows.

```py
model = hub.load("https://tfhub.dev/rishit-dagli/nnclr/1")
```

The input images are expected to have color values in the range `[0,1]`, following the [common image input](https://www.tensorflow.org/hub/common_signatures/images#input) conventions. The expected size of the input images is height x width = 96 x 96 pixels by default. The outputs are 10 class probabilities corresponding to STL-10 classes. 

The pre-trained model could be used on a variety of downstream tasks preferrably by learning a linear classifier on the frozen features of the trained backbone model and evaluate the classifier on unseen images often fine-tuned on a target dataset with 5% or 10% labels present. You can use this backbone for any downstream task such as image classification (like we do in the COlab Notebook) or segmentation or detection

### References

[1] Dwibedi, Debidatta, et al. “With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations.” ArXiv:2104.14548 [Cs], Apr. 2021. arXiv.org, http://arxiv.org/abs/2104.14548