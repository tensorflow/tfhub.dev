# Module google/universal-sentence-encoder-xling/en-de/1

English and German language-agnostic text encoder.

<!-- module-type: text-embedding -->
<!-- asset-path: legacy -->
<!-- network-architecture: Transformer -->
<!-- fine-tunable: true -->
<!-- format: hub -->


[![Open Colab notebook]](https://colab.research.google.com/github/tensorflow/hub/blob/3880b82596d2cf5401095b6ada51cb2d543c2050/examples/colab/cross_lingual_similarity_with_tf_hub_multilingual_universal_encoder.ipynb)

## Overview

The Universal Sentence Encoder Cross-lingual (XLING) module is an extension of
the
[Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/2)
that includes training on multiple tasks across languages. The multi-task
training setup is based on the paper "Learning Cross-lingual Sentence
Representations via a Multi-task Dual Encoder" [1].

This specific module is trained on **English and German (en-de)** tasks, and
optimized for greater-than-word length text, such as sentences, phrases or short
paragraphs. It is trained on a variety of data sources and tasks, with the goal
of learning text representations that are useful out-of-the-box for a number of
applications. The input to the module is variable length English or German text
and the output is a 512 dimensional vector. We note that one _does not need to
specify the language_ that the input is in, as the model was trained such that
English and German text with similar meanings will have similar (high dot
product score) embeddings. We also note that this model can be used for
monolingual English (and potentially monolingual German) tasks with comparable
or even better performance than the purely English Universal Sentence Encoder.

To learn more about text embeddings, refer to the
[TensorFlow Embeddings](https://www.tensorflow.org/guide/embedding)
documentation.

#### Prerequisites

This module relies on the
[SentencePiece library](https://github.com/google/sentencepiece) for input
preprocessing (please make sure the TensorFlow version you use is compatible
with the SentencePiece library). On
[Google Colaboratory](https://colab.research.google.com/), the SentencePiece
library is available by:

```python
!pip3 install sentencepiece
!pip3 install tf-sentencepiece
```

#### Example use

```python
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import tf_sentencepiece

# Some texts of different lengths.
english_sentences = ["dog", "Puppies are nice.", "I enjoy taking long walks along the beach with my dog."]
german_sentences = ["Hund", "Welpen sind nett.", "Ich genieße lange Spaziergänge am Strand entlang mit meinem Hund."]

# Set up graph.
g = tf.Graph()
with g.as_default():
  text_input = tf.placeholder(dtype=tf.string, shape=[None])
  en_de_embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder-xling/en-de/1")
  embedded_text = en_de_embed(text_input)
  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
g.finalize()

# Initialize session.
session = tf.Session(graph=g)
session.run(init_op)

# Compute embeddings.
en_result = session.run(embedded_text, feed_dict={text_input: [english_sentences[0]]})
de_result = session.run(embedded_text, feed_dict={text_input: [german_sentences[0]]})

# Compute similarity. Higher score indicates greater similarity.
similarity_score = np.dot(np.squeeze(en_result), np.squeeze(de_result))
```

## References

[1] M. Chidambaram, Y. Yang, D. Cer, S. Yuan, Y.-H. Sung, B. Strope, and R.
Kurzweil. Learning Cross-Lingual Sentence Representations via a Multi-task
Dual-Encoder Model. ArXiv e-prints, October 2018.
