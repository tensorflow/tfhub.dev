# Collection google/bert/1

Bidirectional Encoder Representations from Transformers (BERT).

<!-- dataset: wikipedia-and-bookscorpus -->
<!-- module-type: text-embedding -->
<!-- network-architecture: transformer -->
<!-- language: en -->

## Overview

BERT (Bidirectional Encoder Representations from Transformers)
provides dense vector representations for natural language
by using a deep, pre-trained neural network with the Transformer
architecture. It was originally published by

  * Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova:
    ["BERT: Pre-training of Deep Bidirectional Transformers for
    Language Understanding"](https://arxiv.org/abs/1810.04805), 2018.

This page collects models with the original BERT architecture and
training procedure. For the numerous extensions of BERT and other,
related architectures, please consult the broader collection of
[Transformer Encoders for
Text](https://tfhub.dev/google/collections/transformer_encoders_text/1).


## BERT models

The following models in the
[SavedModel format of TensorFlow 2](https://www.tensorflow.org/hub/tf2_saved_model)
use the implementation of BERT from the
TensorFlow Models repository on GitHub at
[tensorflow/models/official/nlp/bert](https://github.com/tensorflow/models/tree/master/official/nlp/bert)
with the trained weights released by the original BERT authors.

These SavedModels implement the encoder API for [text embeddings with
transformer encoders](https://www.tensorflow.org/hub/common_saved_model_apis/text#transformer-encoders).
Plain text input can be fed to them models via the separate preprocessing
model referenced in the second column.

| BERT encoder model | preprocessing for use with it |
|--------------------|-------------------------------|
| [tensorflow/bert_en_uncased_L-12_H-768_A-12](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12) | [tensorflow/bert_en_uncased_preprocess](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess) |
| [tensorflow/bert_en_uncased_L-24_H-1024_A-16](https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16) | [tensorflow/bert_en_uncased_preprocess](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess) |
| [tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16](https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16) | [tensorflow/bert_en_uncased_preprocess](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess) |
| [tensorflow/bert_en_cased_L-12_H-768_A-12](https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12) | [tensorflow/bert_en_cased_preprocess](https://tfhub.dev/tensorflow/bert_en_cased_preprocess) |
| [tensorflow/bert_en_cased_L-24_H-1024_A-16](https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16) | [tensorflow/bert_en_cased_preprocess](https://tfhub.dev/tensorflow/bert_en_cased_preprocess) |
| [tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16](https://tfhub.dev/tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16) | [tensorflow/bert_en_cased_preprocess](https://tfhub.dev/tensorflow/bert_en_cased_preprocess) |
| [tensorflow/bert_zh_L-12_H-768_A-12](https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12) | [tensorflow/bert_zh_preprocess](https://tfhub.dev/tensorflow/bert_zh_preprocess) |
| [tensorflow/bert_multi_cased_L-12_H-768_A-12](https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12) | [tensorflow/bert_multi_cased_preprocess](https://tfhub.dev/tensorflow/bert_multi_cased_preprocess) |


## Small BERT models

The Small BERT models are instances of the original BERT architecture
with a smaller number L of layers (i.e., residual blocks) combined with
a smaller hidden size H and a matching smaller number A of attention heads,
as published by

  * Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova:
    ["Well-Read Students Learn Better: On the Importance of Pre-training
    Compact Models"](https://arxiv.org/abs/1908.08962), 2019.

The following models in the
[SavedModel format of TensorFlow 2](https://www.tensorflow.org/hub/tf2_saved_model)
use the implementation of BERT from the
TensorFlow Models repository on GitHub at
[tensorflow/models/official/nlp/bert](https://github.com/tensorflow/models/tree/master/official/nlp/bert)
with the trained weights released by the authors of Small BERT.

|          |H=128|H=256|H=512|H=768|
|----------|:---:|:---:|:---:|:---:|
| **L=2**  | [2/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2)  | [2/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4)  | [2/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8)   | [2/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12)   |
| **L=4**  | [4/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2)  | [4/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4)  | [4/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8)   | [4/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12)   |
| **L=6**  | [6/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2)  | [6/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4)  | [6/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8)   | [6/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12)   |
| **L=8**  | [8/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2)  | [8/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4)  | [8/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8)   | [8/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12)   |
| **L=10** | [10/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2)| [10/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4)| [10/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8) | [10/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12) |
| **L=12** | [12/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2)| [12/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4)| [12/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8) | [12/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12) |

These SavedModels implement the encoder API for [text embeddings with
transformer encoders](https://www.tensorflow.org/hub/common_saved_model_apis/text#transformer-encoders).
Plain text input can be fed to them via

  * [tensorflow/bert_en_uncased_preprocess](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess)


## DEPRECATED: BERT models for TF1

TF Hub also offers the same BERT and Small BERT models in the older,
now deprecated
[Hub module format for TF1](https://www.tensorflow.org/hub/tf1_hub_module),
exported from the original BERT implementation at
[github.com/google-research/bert](https://github.com/google-research/bert).

| BERT       |
|------------|
| [google/bert_uncased_L-12_H-768_A-12](https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12) |
| [google/bert_uncased_L-24_H-1024_A-16](https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16) |
| [google/bert_cased_L-12_H-768_A-12](https://tfhub.dev/google/bert_cased_L-12_H-768_A-12) |
| [google/bert_cased_L-24_H-1024_A-16](https://tfhub.dev/google/bert_cased_L-24_H-1024_A-16) |
| [google/bert_chinese_L-12_H-768_A-12](https://tfhub.dev/google/bert_chinese_L-12_H-768_A-12) |
| [google/bert_multi_cased_L-12_H-768_A-12](https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12) |


|          |H=128|H=256|H=512|H=768|
|----------|:---:|:---:|:---:|:---:|
| **L=2**  | [2/128](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-128_A-2)  | [2/256](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-256_A-4)  | [2/512](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-512_A-8)   | [2/768](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-768_A-12)   |
| **L=4**  | [4/128](https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-128_A-2)  | [4/256](https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-256_A-4)  | [4/512](https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-512_A-8)   | [4/768](https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-768_A-12)   |
| **L=6**  | [6/128](https://tfhub.dev/google/small_bert/bert_uncased_L-6_H-128_A-2)  | [6/256](https://tfhub.dev/google/small_bert/bert_uncased_L-6_H-256_A-4)  | [6/512](https://tfhub.dev/google/small_bert/bert_uncased_L-6_H-512_A-8)   | [6/768](https://tfhub.dev/google/small_bert/bert_uncased_L-6_H-768_A-12)   |
| **L=8**  | [8/128](https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-128_A-2)  | [8/256](https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-256_A-4)  | [8/512](https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-512_A-8)   | [8/768](https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-768_A-12)   |
| **L=10** | [10/128](https://tfhub.dev/google/small_bert/bert_uncased_L-10_H-128_A-2)| [10/256](https://tfhub.dev/google/small_bert/bert_uncased_L-10_H-256_A-4)| [10/512](https://tfhub.dev/google/small_bert/bert_uncased_L-10_H-512_A-8) | [10/768](https://tfhub.dev/google/small_bert/bert_uncased_L-10_H-768_A-12) |
| **L=12** | [12/128](https://tfhub.dev/google/small_bert/bert_uncased_L-12_H-128_A-2)| [12/256](https://tfhub.dev/google/small_bert/bert_uncased_L-12_H-256_A-4)| [12/512](https://tfhub.dev/google/small_bert/bert_uncased_L-12_H-512_A-8) | [12/768](https://tfhub.dev/google/small_bert/bert_uncased_L-12_H-768_A-12) |
