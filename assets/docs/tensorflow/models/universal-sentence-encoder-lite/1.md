# Placeholder tensorflow/universal-sentence-encoder-lite/1
Encoder of greater-than-word length text trained on a variety of data.

<!-- module-type: text-embedding -->
<!-- task: text-embedding -->
<!-- network-architecture: dan -->
<!-- language: en -->

## Overview

This model is a copy of [https://tfhub.dev/google/universal-sentence-encoder-lite/1](https://tfhub.dev/google/universal-sentence-encoder-lite/1).

The Universal Sentence Encoder Lite module is a lightweight version of
[Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/1).
This lite version is good for use cases when your computation resource is
limited. For example, on-device inference. It's small and still gives good
performance on various natural language understanding tasks.

The model is trained and optimized for greater-than-word length text, such as
sentences, phrases or short paragraphs. It is trained on a variety of data
sources and a variety of tasks with the aim of dynamically accommodating a wide
variety of natural language understanding tasks. The input is variable length
English text and the output is a 512 dimensional vector. Our encoder differs
from word level embedding models in that we train on a number of natural
language prediction tasks that require modeling the meaning of word sequences
rather than just individual words. Details are available in the paper "Universal
Sentence Encoder" [1].

## References

[1] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco,
Rhomni St. John, Noah Constant, Mario Guajardo-CÃ©spedes, Steve Yuan, Chris Tar,
Yun-Hsuan Sung, Brian Strope, Ray Kurzweil. [Universal Sentence Encoder](https://arxiv.org/abs/1803.11175).
arXiv:1803.11175, 2018.
