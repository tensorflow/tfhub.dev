# Module google/&zwnj;tf2-preview/&zwnj;mobilenet_v2/&zwnj;feature_vector/2
[TF2] Feature vectors of images with MobileNet V2 trained on ImageNet (ILSVRC-2012-CLS).

<!-- dataset: ImageNet (ILSVRC-2012-CLS) -->
<!-- module-type: image-feature-vector -->
<!-- network-architecture: MobileNet V2 -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->


[![Open Colab notebok]](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb)

## TensorFlow 2.0 Preview

This module uses the SavedModel 2.0 format and was created to help
preview TensorFlow 2.0 functionality. Using it requires the (currently
experimental) library versions TensorFlow 2.0 and TensorFlow Hub 0.3.0.

## Overview

MobileNet V2 is a family of neural network architectures for efficient
on-device image classification and related tasks, originally published by

  * Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
    Liang-Chieh Chen: ["Inverted Residuals and Linear Bottlenecks:
    Mobile Networks for Classification, Detection and
    Segmentation"](https://arxiv.org/abs/1801.04381), 2018.

Mobilenets come in various sizes controlled by a multiplier for the
depth (number of features) in the convolutional layers. They can also be
trained for various sizes of input images to control inference speed.

This TF-Hub module uses the TF-Slim implementation of
`mobilenet_v2`
with a depth multiplier of 1.0 and an input size of
224x224 pixels.


The module contains a trained instance of the network, packaged to get
[feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector).
If you want the full model including the classification it was originally
trained for, use module
[`google/tf2-preview/mobilenet_v2/classification/2`](https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2)
instead.


## Training

The checkpoint exported into this module was `mobilenet_v2_1.0_224/mobilenet_v2_1.0_224.ckpt` downloaded
from
[MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md).
Its weights were originally obtained by training on the ILSVRC-2012-CLS
dataset for image classification ("Imagenet").

## Usage

This module can be used with the `hub.KerasLayer` as follows:

```python
m = tf.keras.Sequential([
    hub.KerasLayer("https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2", output_shape=[1280],
                   trainable=False),  # Can be True, see below.
    tf.keras.layers.Dense(num_classes, activation='softmax')
])
m.build([None, 224, 224, 3])  # Batch input shape.
```

The output of the module is a batch of feature vectors. For each input image,
the feature vector has size `num_features` = 1280. The feature
vectors can then be used further, e.g., for classification as above.

For this module, the size of the input image is fixed to
`height` x `width` = 224 x 224 pixels.
The input `images` are expected to have 3 RGB color values in the range [0,1],
following the
[common image input](https://www.tensorflow.org/hub/common_signatures/images#input)
conventions (analogously to TF 1.x).


## Fine-tuning

Consumers of this module can [fine-tune](https://www.tensorflow.org/hub/tf2_saved_model#fine-tuning) it
by passing `trainable=True` to `hub.KerasLayer`.
(Note that this automatically updates the moving averages of
batch normalization.)


