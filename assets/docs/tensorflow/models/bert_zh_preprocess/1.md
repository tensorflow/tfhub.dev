# Module tensorflow/&zwnj;bert_zh_preprocess/1
Text preprocessing for BERT.

<!-- dataset: Wikipedia -->
<!-- asset-path: legacy -->
<!-- fine-tunable: false -->
<!-- format: saved_model_2 -->
<!-- language: zh -->
<!-- module-type: text-preprocessing -->

## Overview

This SavedModel is a companion of BERT models to preprocess plain text inputs
into the input format expected by BERT.
**Check the encoder model documentation** to
find the correct preprocessing model for each particular encoder.

BERT and its preprocessing were originally published by

  * Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova:
    ["BERT: Pre-training of Deep Bidirectional Transformers for
    Language Understanding"](https://arxiv.org/abs/1810.04805), 2018.

This model uses a vocabulary for Chinese extracted from
the Wikipedia (same as in the models by the original BERT authors).


This model has no trainable parameters and can be used in an input pipeline
outside the training loop.


## Prerequisites

This model uses TensorFlow operations defined by the
[TensorFlow Text](https://github.com/tensorflow/text) library.
On [Google Colaboratory](https://colab.research.google.com/),
it can be installed with

```python
!pip install tensorflow_text
import tensorflow_text as text  # Registers the ops.
```


## Usage

This model expects a batch of text segments (plain text encoded as UTF-8)
and produces a batch of inputs to the BERT encoder, one for each segment.

The input text gets normalized and tokenized with the wordpiece vocabulary
of the model. As expected by BERT, special tokens are added at the start and
the end. Tokenized inputs are truncated if the length including special tokens
would exceed 128.

```python
text_inputs = tf.keras.layers.Input(shape=(), dtype=tf.string)
preprocessing = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/bert_zh_preprocess/1")
encoder_inputs = preprocessing(text_inputs)
```

The `encoder_inputs` are a dict of tensors ready for input into the BERT
encoder model: the keys are `"input_word_ids"`, `"input_mask"`, and
`"input_type_ids"` (in any order), and the values are `Tensor`s of type
`tf.int32` and shape `[batch_size, 128]`.


## Changelog

### Version 1

  * Initial release.
