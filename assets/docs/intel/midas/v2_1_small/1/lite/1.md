# Lite intel/midas/v2_1_small/1/lite/1
A deployment format of Lite intel/midas/v2_1_small/1

<!-- asset-path: legacy -->
<!-- parent-model: intel/midas/v2_1_small/1 -->

## License
MIT License

#### Inference speed

Mobile real-time convolutional neural network for monocular depth estimation from a single RGB image:

* iOS 13.7 - iPhone11 (A13 Bionic) - 30 FPS on NPU
* Andoird 10 - OnePlus8 (Snapdragon 865) - 22 FPS on GPU

#### Requirements

* iOS: [iOS application](https://github.com/intel-isl/MiDaS/tree/master/mobile/ios)
* Android: [Android application](https://github.com/intel-isl/MiDaS/tree/master/mobile/android)
* CPU or nVidia GPU:

```
pip install tensorflow opencv-python matplotlib
```


#### Model Details
This module is based on a Pytorch version of MiDaS, which performs efficiently and with very high accuracy to compute depth from a single image.

* input: (uint8) RGB image with shape (3, 256, 256)
* output: (float32) inverse depth maps (1, 256, 256)

#### Example Use

```python
import cv2
import tensorflow as tf
import urllib.request
import matplotlib.pyplot as plt

url, filename = ("https://github.com/intel-isl/MiDaS/releases/download/v2/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)

url, filename = ("https://github.com/intel-isl/MiDaS/releases/download/v2_1/model_opt.tflite", "model_opt.tflite")
urllib.request.urlretrieve(url, filename)


# input
img = cv2.imread('dog.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0

img_resized = tf.image.resize(img, [256,256], method='bicubic', preserve_aspect_ratio=False)
#img_resized = tf.transpose(img_resized, [2, 0, 1])
img_input = img_resized.numpy()
mean=[0.485, 0.456, 0.406]
std=[0.229, 0.224, 0.225]
img_input = (img_input - mean) / std
reshape_img = img_input.reshape(1,256,256,3)
tensor = tf.convert_to_tensor(reshape_img, dtype=tf.float32)

# load model
interpreter = tf.lite.Interpreter(model_path="model_opt.tflite")
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_shape = input_details[0]['shape']

# inference
interpreter.set_tensor(input_details[0]['index'], tensor)
interpreter.invoke()
output = interpreter.get_tensor(output_details[0]['index'])
output = output.reshape(256, 256)
             
# output file
prediction = cv2.resize(output, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)
print(" Write image to: output.png")
depth_min = prediction.min()
depth_max = prediction.max()
img_out = (255 * (prediction - depth_min) / (depth_max - depth_min)).astype("uint8")

cv2.imwrite("output.png", img_out)
plt.imshow(img_out)
# plt.show()
```


**Source:** https://github.com/intel-isl/MiDaS
