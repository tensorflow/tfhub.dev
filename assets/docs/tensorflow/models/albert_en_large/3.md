# Module tensorflow/albert_en_large/3
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations

<!-- asset-path: internal -->
<!-- dataset: wikipedia-and-bookscorpus -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- module-type: text-embedding -->
<!-- task: text-embedding -->
<!-- network-architecture: transformer -->

[![Open Colab notebook]](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/bert_glue.ipynb)

## Overview

ALBERT is "A Lite" version of BERT with greatly reduced number of parameters. It
was originally published by

*   Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,
    Radu Soricut. [ALBERT: A Lite BERT for Self-supervised Learning of Language
    Representations](https://arxiv.org/abs/1909.11942). arXiv preprint
    arXiv:1909.11942, 2019.

This TF Hub model uses the implementation of ALBERT from the
TensorFlow Models repository on GitHub at
[tensorflow/models/official/nlp/modeling/networks/albert_encoder.py](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/networks/albert_encoder.py).
It uses L=24 hidden layers (i.e., Transformer blocks),
a hidden size of H=1024,
and A=16 attention heads.
For other model sizes, see the
[ALBERT](https://tfhub.dev/google/collections/albert/1) collection.

The weights of this TF2 SavedModel have been converted from
[albert_large](https://tfhub.dev/google/albert_large/3)
in TF1 Hub module format.

All parameters in the module are trainable, and fine-tuning all parameters is
the recommended practice.


## Usage

This SavedModel implements the encoder API for [text embeddings with transformer
encoders](https://www.tensorflow.org/hub/common_saved_model_apis/text#transformer-encoders).
It expects a dict with three int32 Tensors as input:
`input_word_ids`, `input_mask`, and `input_type_ids`.

The separate **preprocessor** SavedModel at
[http://tfhub.dev/tensorflow/albert_en_preprocess/3](http://tfhub.dev/tensorflow/albert_en_preprocess/3)
transforms plain text inputs into this format, which its documentation
describes in greater detail.

### Basic usage

The simplest way to use this model in the
[Keras functional API](https://www.tensorflow.org/guide/keras/functional)
is

```python
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)
preprocessor = hub.KerasLayer(
    "http://tfhub.dev/tensorflow/albert_en_preprocess/3")
encoder_inputs = preprocessor(text_input)
encoder = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/albert_en_large/3",
    trainable=True)
outputs = encoder(encoder_inputs)
pooled_output = outputs["pooled_output"]      # [batch_size, 1024].
sequence_output = outputs["sequence_output"]  # [batch_size, seq_length, 1024].
```

The encoder's outputs are the `pooled_output` to represents each input sequence
as a whole, and the `sequence_output` to represent each input token in context.
Either of those can be used as input to further model building.

To print pooled_outputs for inspection, the following code can be used:

```python
embedding_model = tf.keras.Model(text_input, pooled_output)
sentences = tf.constant(["(your text here)"])
print(embedding_model(sentences))
```

### Advanced topics

The [preprocessor documentation](http://tfhub.dev/tensorflow/albert_en_preprocess/3)
explains how to input segment pairs and how to control `seq_length`.

The intermediate activations of all L=24
Transformer blocks (hidden layers) are returned as a Python list:
`outputs["encoder_outputs"][i]` is a Tensor
of shape `[batch_size, seq_length, 1024]`
with the outputs of the i-th Transformer block, for `0 <= i < L`.
The last value of the list is equal to `sequence_output`.

The preprocessor can be run from inside a callable passed to
`tf.data.Dataset.map()` while this encoder stays a part of a larger
model that gets trained on that dataset.
The Keras input objects for running on preprocessed inputs are

```python
encoder_inputs = dict(
    input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
)
```

### Masked Language Model

This SavedModel provides a trainable `.mlm` subobject with predictions for the
Masked Language Model task it was originally trained with. This allows advanced
users to continue MLM training for fine-tuning to a downstream task. It extends
the encoder interface above with a zero-padded tensor of positions in the input
sequence for which the `input_word_ids` have been randomly masked or altered.
(See the [preprocessor model page](http://tfhub.dev/tensorflow/albert_en_preprocess/3) for how to get the
id of the mask token and more.)

```python
mlm_inputs = dict(
    input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    masked_lm_positions=tf.keras.layers.Input(shape=(num_predict,), dtype=tf.int32),
)

encoder = hub.load("https://tfhub.dev/tensorflow/albert_en_large/3")
mlm = hub.KerasLayer(encoder.mlm, trainable=True)
mlm_outputs = mlm(mlm_inputs)
mlm_logits = mlm_outputs["mlm_logits"]  # [batch_size, num_predict, vocab_size]
# ...plus pooled_output, sequence_output and encoder_outputs as above.
```


## Changelog

### Version 3
  * Support persistent gradient tapes.

### Version 2

  * Uses dicts (not lists) for inputs and outputs.
  * Comes with a companion model for preprocessing of plain text.
  * For legacy users, this version still provides the now-obsolete
    `albert_layer.resolved_object.sp_model_file` asset.

### Version 1

  * Initial release.
