# Module tensorflow/albert_en_base/1

ALBERT: A Lite BERT for Self-supervised Learning of Language Representations

<!-- dataset: Wikipedia and BooksCorpus -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->


## TF2 SavedModel

This is a [SavedModel in TensorFlow 2
format](https://www.tensorflow.org/hub/tf2_saved_model).
Using it requires TensorFlow 2 (or 1.15) and TensorFlow Hub 0.5.0 or newer.

## Overview

ALBERT is "A Lite" version of BERT with greatly reduced number of parameters. It
was originally published by

*   Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,
    Radu Soricut. [ALBERT: A Lite BERT for Self-supervised Learning of Language
    Representations](https://arxiv.org/abs/1909.11942). arXiv preprint
    arXiv:1909.11942, 2019.

This TF2 SavedModel is converted from
[albert_base](https://tfhub.dev/google/albert_base/3) in TF1 version, using the
implementation of ALBERT from the TensorFlow Models repository on GitHub at
[models/official/nlp/modeling/networks/albert_transformer_encoder.py](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/networks/albert_transformer_encoder.py).

It uses L=12 hidden layers (i.e., Transformer blocks), a hidden size of H=768,
and A=12 attention heads.

All parameters in the module are trainable, and fine-tuning all parameters is
the recommended practice.


## Usage

This model is called as follows on tokenized text input,
an input mask to hold out padding tokens,
and segment types when input mixes with different segments.

```python
max_seq_length = 128  # Your choice here.
input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                       name="input_word_ids")
input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                   name="input_mask")
segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                    name="segment_ids")
albert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/albert_en_base/1",
                              trainable=True)
pooled_output, sequence_output = albert_layer([input_word_ids, input_mask, segment_ids])
```

There are two outputs:
a `pooled_output` of shape `[batch_size, 768]` with
representations for the entire input sequences and
a `sequence_output` of shape `[batch_size, max_seq_length, 768]`
with representations for each input token (in context).

The tokenization of input text can be performed in Python with the
`FullSentencePieceTokenizer` class from
[tensorflow/models/official/nlp/bert/tokenization.py](https://github.com/tensorflow/models/blob/master/official/nlp/bert/tokenization.py).
Its sp_model_file is stored as a `tf.saved_model.Asset`. It can be retrieved
(using TensorFlow Hub 0.7.0 or newer) as follows:

```python
sp_model_file = albert_layer.resolved_object.sp_model_file.asset_path.numpy()
tokenizer = tokenization.FullSentencePieceTokenizer(sp_model_file)
```

For complete usage examples, see
[run_classifier.py](https://github.com/tensorflow/models/blob/master/official/nlp/albert/run_classifier.py)
and
[run_squad.py](https://github.com/tensorflow/models/blob/master/official/nlp/albert/run_squad.py)
from
[tensorflow/models/official/nlp/albert/](https://github.com/tensorflow/models/tree/master/official/nlp/albert)
on GitHub.
