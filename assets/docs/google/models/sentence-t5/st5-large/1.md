# Module google/sentence-t5/st5-large/1

Sentence encoders for English built on top of T5 models.

<!-- asset-path: internal -->
<!-- task: text-embedding -->
<!-- fine-tunable: false -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- network-architecture: transformer -->

### Model Details

The sentence-T5 family of models encode text into high-dimensional vectors that
can be used for text classification, semantic similarity, clustering and other
natural language processing tasks.

Our model is built on top of [T5](https://arxiv.org/pdf/1910.10683.pdf) (i.e.
the Text-To-Text Transfer Transformer). It is trained on a variety of data
sources and initialized from pre-trained T5 models with different model sizes as
described in [1]. The input is variable-length English text and the output is a
768-dimensional vector. The sentence-T5 large model employs a 24-layer
transformer architecture as the T5 large model does.

### Metrics

*   We evaluate this model on the
    [SentEval](https://github.com/facebookresearch/SentEval) sentence
    representation benchmark.

    Transfer tasks                                              | MR   | CR   | SUBJ | MPQA | SST  | TREC | MRPC | Avg
    :---------------------------------------------------------- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | --:
    [ST5-Base](https://tfhub.dev/google/sentence-t5/st5-base/1) | 85.8 | 92.1 | 94.6 | 90.9 | 91.8 | 96.4 | 75.2 | 89.5
    **ST5-Large**                                               | 88.9 | 93.5 | 95.4 | 91.5 | 94.2 | 96.2 | 77.1 | 91.0
    [ST5-3B](https://tfhub.dev/google/sentence-t5/st5-3b/1)     | 89.9 | 94.1 | 95.9 | 91.6 | 94.8 | 96.2 | 77.9 | 91.5
    ST5-11B                                                     | 90.8 | 94.4 | 96.3 | 91.7 | 94.8 | 95.4 | 77.9 | 91.6

    <br/>

    STS tasks                                                   | STS12 | STS13 | STS14 | STS15 | STS16 | STSb | SICK-R | Avg
    :---------------------------------------------------------- | ----: | ----: | ----: | ----: | ----: | ---: | -----: | --:
    [ST5-Base](https://tfhub.dev/google/sentence-t5/st5-base/1) | 78.1. | 85.8  | 82.2  | 87.5  | 84.0  | 86.0 | 79.8   | 83.3
    **ST5-Large**                                               | 79.1  | 87.3  | 83.2  | 88.3  | 84.4  | 86.7 | 79.8   | 84.1
    [ST5-3B](https://tfhub.dev/google/sentence-t5/st5-3b/1)     | 79.0  | 88.8  | 84.3  | 88.9  | 85.3  | 86.3 | 79.5   | 84.6
    ST5-11B                                                     | 80.1  | 88.8  | 84.7  | 88.9  | 85.2  | 86.8 | 80.4   | 85.0

More details about the evaluations can be found in the paper [1].

### Example Use

```python
import tensorflow_hub as hub
import tensorflow as tf

english_sentences = tf.constant(["dog", "Puppies are nice.", "I enjoy taking long walks along the beach with my dog."])

hub_url = "https://tfhub.dev/google/sentence-t5/st5-large/1"
encoder = hub.KerasLayer(hub_url)

english_embeds = encoder(english_sentences)

print (english_embeds)
```

### References

[1] Jianmo, Ni, Gustavo Hernández Ábrego, Noah Constant, Ji Ma, Keith B. Hall,
Daniel Cer, Yinfei Yang.
[Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models.](https://arxiv.org/abs/2108.08877)
August 2021.
