# Collection sayakpaul/bit-resnet/1

Collection of new BiT-ResNet models trained on ImageNet-1k

<!-- dataset: imagenet-ilsvrc-2012-cls -->
<!-- task: image-classification -->

## Overview

This collection contains BiT-ResNet [1] classifiers and feature extractors trained on ImageNet-1k [2]. They can be used for out-of-the box inference as well as fine-tuning. These models are known to show excellent performance on several downstream tasks in computer vision as shown in [1]. Notably, the `152x2` (ResNet152x2) models were trained as a part of [3]. They act as teachers for
distilling the student `50x1` (ResNet50x1) models. At the time of this publication, these ResNet50 models set state-of-the-art on ImageNet-1k at 224x224 and 160x160 input resolutions.

You can find the other BiT-ResNet models [here](https://tfhub.dev/google/collections/bit).

## Notes and acknowledgements

* The model weights are taken from the official code repository of BiT [4]. The initial weights are in NumPy and they were
ported to TensorFlow SavedModel format. 
* The porting steps are available in [5].
* Thanks to [Willi Gierke](https://ch.linkedin.com/in/willi-gierke) for helping with porting the weights. 


## Table of contents

Models included in this collection have two variants: (1) off-the-shelf inference, (2) fine-tuning on downstream tasks.

|  Model <br>Name 	| Input<br>Resolution 	|                                 Classifier                                 	|                            Feature<br>Extractor                            	|
|:---------------:	|:-------------------:	|:--------------------------------------------------------------------------:	|:--------------------------------------------------------------------------:	|
| BiT-ResNet152x2 	|         384         	|  [Link](https://tfhub.dev/sayakpaul/bit_resnet152x2_384_classification/1)  	|   [Link](https://tfhub.dev/sayakpaul/bit_r152x2_384_feature_extraction/1)  	|
| BiT-ResNet152x2 	|         224         	|  [Link](https://tfhub.dev/sayakpaul/bit_resnet152x2_224_classification/1)  	|   [Link](https://tfhub.dev/sayakpaul/bit_r152x2_224_feature_extraction/1)  	|
|  BiT-ResNet50x1 	|         224         	| [Link](https://tfhub.dev/sayakpaul/distill_bit_r50x1_224_classification/1) 	| [Link](https://tfhub.dev/sayakpaul/distill_bit_r50x1_224_classification/1) 	|
|  BiT-ResNet50x1 	|         160         	| [Link](https://tfhub.dev/sayakpaul/distill_bit_r50x1_160_classification/1) 	| [Link](https://tfhub.dev/sayakpaul/distill_bit_r50x1_160_classification/1) 	|



## References 

[1] [Big Transfer (BiT): General Visual Representation Learning by Kolesnikov et al.](https://arxiv.org/abs/1912.11370)
[2] [ImageNet-1k](https://www.image-net.org/challenges/LSVRC/2012/index.php)  
[3] [Knowledge distillation: A good teacher is patient and consistent by Beyer et al.](https://arxiv.org/abs/2106.05237)   
[4] [BiT GitHub](https://github.com/google-research/big_transfer)
[5] [Colab Notebook for assembling BiT models in TensorFlow](https://colab.research.google.com/github/sayakpaul/BiT-jax2tf/blob/main/convert_jax_weights_tf.ipynb)
