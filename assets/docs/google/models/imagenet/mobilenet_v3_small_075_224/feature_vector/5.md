# Module google/imagenet/mobilenet_v3_small_075_224/feature_vector/5

Feature vectors of images with MobileNet V3 small(depth multiplier 0.75) trained on ImageNet (ILSVRC-2012-CLS).

<!-- asset-path: legacy -->
<!-- dataset: imagenet-ilsvrc-2012-cls -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- task: image-feature-vector -->
<!-- network-architecture: mobilenet-v3 -->
<!-- colab: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/hub/tutorials/tf2_image_retraining.ipynb -->

## TF2 SavedModel

This is a [SavedModel in TensorFlow 2
format](https://www.tensorflow.org/hub/tf2_saved_model).
Using it requires TensorFlow 2 (or 1.15) and TensorFlow Hub 0.5.0 or newer.

## Overview

MobileNet V3 is a family of neural network architectures for efficient
on-device image classification and related tasks, originally published by

  * Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing
    Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le,
    Hartwig Adam: ["Searching for MobileNetV3"](https://arxiv.org/abs/1905.02244),
    2019.

Similar to other Mobilenets, MobileNet V3 uses a multiplier for the depth
(number of features) in the convolutional layers to tune the accuracy vs.
latency tradeoff. In addition, MobileNet V3 comes in two different sizes, small and large, to
adapt the network to low or high resource use cases. Although V3 networks can be
built with custom input resolutions, just like other Mobilenets, all pre-trained
checkpoints were published with the same 224x224 input resolution.

For a quick comparison between these variants, please refer to the following
table:

Size  | Depth multiplier | Top1 accuracy (%) | Pixel 1 (ms) | Pixel 2 (ms) | Pixel 3 (ms)
----- | ---------------- | ----------------- | ------------ | ------------ | ------------
Large | 1.0              | 75.2              | 51.2         | 61           | 44
Large | 0.75             | 73.3              | 39.8         | 48           | 32
Small | 1.0              | 67.5              | 15.8         | 19.4         | 14.4
Small | 0.75             | 65.4              | 12.8         | 15.9         | 11.6

This TF Hub model uses the TF-Slim implementation of
[`mobilenet_v3`](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v3.py)
as a small network with a depth multiplier of 0.75.

This implementation of Mobilenet V3 rounds feature depths to multiples of 8
(an optimization *not* described in the paper).
Depth multipliers less than 1.0 are not applied to the last convolutional layer
(from which the module takes the image feature vector).

The model contains a trained instance of the network, packaged to get
[feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector).
If you want the full model including the classification it was originally
trained for, use
[`google/imagenet/mobilenet_v3_small_075_224/classification/5`](https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/classification/5)
instead.


## Training

The checkpoint exported into this model was `v3-small_224_0.75_float/ema/model-497500` downloaded
from
[MobileNet V3 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md).
Its weights were originally obtained by training on the ILSVRC-2012-CLS
dataset for image classification ("Imagenet").

## Usage

This model can be used with the `hub.KerasLayer` as follows.
It *cannot* be used with the `hub.Module` API for TensorFlow 1.

```python
m = tf.keras.Sequential([
    hub.KerasLayer("https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5",
                   trainable=False),  # Can be True, see below.
    tf.keras.layers.Dense(num_classes, activation='softmax')
])
m.build([None, 224, 224, 3])  # Batch input shape.
```

The output is a batch of feature vectors. For each input image,
the feature vector has size `num_features` = 1024. The feature
vectors can then be used further, e.g., for classification as above.


The input `images` are expected to have color values in the range [0,1],
following the
[common image input](https://www.tensorflow.org/hub/common_signatures/images#input)
conventions.
For this model, the size of the input images is fixed to
`height` x `width` = 224 x 224 pixels.


## Fine-tuning

Consumers of this model can
[fine-tune](https://www.tensorflow.org/hub/tf2_saved_model#fine-tuning) it
by passing `trainable=True` to `hub.KerasLayer`.

The momentum (a.k.a. decay coefficient) of batch norm's exponential moving
averages defaults to 0.99 for this model, in order to accelerate training
on small datasets (or with huge batch sizes).
Advanced users can set another value (say, 0.997) by loading this model like

```python
hub.KerasLayer("https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5",
               trainable=True, arguments=dict(batch_norm_momentum=0.997))
```


## Changelog

#### Version 5

  * Initial release.
