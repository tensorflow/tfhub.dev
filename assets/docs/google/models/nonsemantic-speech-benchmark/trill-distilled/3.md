# Module google/nonsemantic-speech-benchmark/trill-distilled/3
A distilled version of TRILL presented in "Towards Learning a Universal Non-Semantic Representation of Speech".
It exceeds state-of-the-art performance on a number of transfer learning tasks
drawn from the non-semantic speech domain (speech emotion recognition, language
identification, etc). It is trained on publicly-available AudioSet.

<!-- asset-path: internal -->
<!-- task: audio-embedding -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- network-architecture: mobilenet -->
<!-- dataset: audioset -->

## Overview

A distilled version of TRILL presented in
[Towards Learning a Universal Non-Semantic Representation of Speech](http://arxiv.org/abs/2002.12764).
The
[github page](https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark)
contains the code to reproduce the paper's results, more code examples, and the
evaluation code to run new embeddings on the Non-Semantic Speech Benchmark
(NOSS).

If you use this model, please cite the following:

```
@inproceedings{shor20_interspeech,
  author={Joel Shor and Aren Jansen and Ronnie Maor and Oran Lang and Omry Tuval and FÃ©lix de Chaumont Quitry and Marco Tagliasacchi and Ira Shavitt and Dotan Emanuel and Yinnon Haviv},
  title={{Towards Learning a Universal Non-Semantic Representation of Speech}},
  year=2020,
  booktitle={Proc. Interspeech 2020},
  pages={140--144},
  doi={10.21437/Interspeech.2020-1242}
}
```

### TF 2.X

To run the model in TF 2:

```python
# Import TF 2.X and make sure we're running eager.
import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()
assert tf.executing_eagerly()

import tensorflow_hub as hub
import numpy as np

# Load the module and run inference.
module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/3')
# `wav_as_float_or_int16` can be a numpy array or tf.Tensor of float type or
# int16. The sample rate must be 16kHz. Resample to this sample rate, if
# necessary.
wav_as_float_or_int16 = np.sin(np.linspace(-np.pi, np.pi, 128), dtype=np.float32)
emb = module(samples=wav_as_float_or_int16, sample_rate=16000)['embedding']
# `emb` is a [time, feature_dim] Tensor.
emb.shape.assert_is_compatible_with([None, 2048])

print(emb)
```

```python
trill_layer = hub.KerasLayer(
    handle='https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/3',
    trainable=True,
    arguments={'sample_rate': tf.constant(16000, tf.int32)},
    output_key='embedding',
    output_shape=[None, 2048]
)
assert trill_layer.trainable_variables
```

### TF 1.X

Generating the embedding in TF 1.X is very similar, we just need to run the
graph in a `tf.Session`:

```python
# Import TF 1.X.
import tensorflow.compat.v1 as tf
assert not tf.executing_eagerly()

import tensorflow_hub as hub
import numpy as np

# Load the module and run inference.
module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/3')
# `wav_as_float_or_int16` can be a numpy array or tf.Tensor of float type or
# int16. The sample rate must be 16kHz. Resample to this sample rate, if
# necessary.
wav_as_float_or_int16 = np.sin(np.linspace(-np.pi, np.pi, 128), dtype=np.float32)
emb = module(samples=wav_as_float_or_int16, sample_rate=16000)['embedding']
# `emb` is a [time, feature_dim] Tensor.
emb.shape.assert_is_compatible_with([None, 2048])
with tf.train.MonitoredSession() as sess:
  emb_np = sess.run(emb)

print(emb_np)
```
