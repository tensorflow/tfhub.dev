# Module google/bit/s-r101x3/ilsvrc2012_classification/1

ImageNet-1k (ILSVRC-2012) classification with the BiT-S R101x3 model.

<!-- dataset: imagenet-ilsvrc-2012-cls -->
<!-- asset-path: legacy -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- module-type: image-classification -->
<!-- network-architecture: resnet101x3-v2 -->


## Overview

Big Transfer (BiT) is a recipe for pre-training image classification models on
large supervised datasets and efficiently fine-tuning them on any given target
task. The recipe achieves excellent performance on a wide variety of tasks,
even when using very few labeled examples from the target dataset.

* A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly and
  N. Houlsby:
  [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370).

In the paper, three families of models are presented: "BiT-S", pre-trained on
ImageNet-1k (also known as ILSRCV-2012-CLS); "BiT-M", pre-trained on
ImageNet-21k (also known as the "Full ImageNet, Fall 2011 release");
and "BiT-L", pre-trained on JFT-300M, a proprietary dataset.

Each family is composed of a ResNet-50 (R50x1), a ResNet-50 three times wider
(R50x3), a ResNet-101 (R101x1), a ResNet-101 three times wider (R101x3), and our
flagship architecture, a ResNet-152 four times wider (R152x4). Contrary to the
original ResNet architecture, we used Group Normalization instead of Batch
Normalization, and Weight Standardization of the convolution kernels.

This module implements the R101x3 architecture, trained to perform
image classification on ImageNet ILSRCV-2012-CLS, a dataset containing around
1.3 milion images labeled with 1,000 classes.
It was later fine-tuned on the ImageNet ILSVRC-2012 dataset, as described in
the BiT paper. Its outputs are the logits (before the softmax activation) for
the 1,000 classes.

If you have used this architecture, please refer to it as
**BiT-S R101x3** and cite the aforementioned work.

## Usage

```python
module = hub.KerasLayer("https://tfhub.dev/google/bit/s-r101x3/ilsvrc2012_classification/1")
images = ...  # A batch of images with shape [batch_size, height, width, 3].
logits = module(images)  # Logits with shape [batch_size, 1000].
probabilities = tf.nn.softmax(logits)
```

We provide the mapping from indices to
[WordNet IDs](https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_ids.txt)
as well as
[to words](https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt).

## Fine-tuning

All our models can be directly fine-tuned.
If you want to fine-tune this architecture on your own dataset, please use the
[module that extracts feature vectors](https://tfhub.dev/google/bit/s-r101x3/1)
instead.
You can find the recipe that we use for fine-tuning in the BiT paper, which we
call the BiT HyperRule.

## BiT collection

If you want to use this architecture to perform feature extraction, or to
fine-tune it on a different dataset, we provided a
[specific module for doing so](https://tfhub.dev/google/bit/s-r101x3/1).

Finally, you can find the collection of all BiT models here: [https://tfhub.dev/google/collections/bit/1](https://tfhub.dev/google/collections/bit/1)
