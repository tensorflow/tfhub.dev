# Module jeongukjae/smaller_LaBSE_15lang/1

Patched `google/LaBSE/2` model using "Load What You Need: Smaller Multilingual Transformers" method to reduce word embedding parameters.

<!-- asset-path: https://storage.googleapis.com/jeongukjae-tf-models/smaller-LaBSE/smaller_LaBSE_15lang.tar.gz -->
<!-- network-architecture: transformer -->
<!-- task: text-embedding -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: ar -->
<!-- language: zh-cn -->
<!-- language: en -->
<!-- language: fr -->
<!-- language: de -->
<!-- language: it -->
<!-- language: ja -->
<!-- language: ko -->
<!-- language: nl -->
<!-- language: pl -->
<!-- language: pt -->
<!-- language: es -->
<!-- language: th -->
<!-- language: tr -->
<!-- language: ru -->

## Overview

The smaller LaBSE(language-agnostic BERT sentence embedding) is a patched version of [google/LaBSE/2](https://tfhub.dev/google/LaBSE/2) to handle fewer languages by applying ["Load What You Need: Smaller Versions of Multilingual BERT"](https://arxiv.org/abs/2010.05609). To summarize the contents of the paper, only selected tokens that appear frequently in the corpus(Wikipedia dump for this model) are left in the word embedding table.

Since this model is not subsequently trained, and the only thing changed is relocating vectors in the word embedding table, this model will encode texts in the exact same way as [google/LaBSE/2](https://tfhub.dev/google/LaBSE/2) if texts are preprocessed in the same way(it means, if all tokens you want to encode is selected, the representations won't change).

You can check the code at [this repository (jeongukjae/smaller-labse)](https://github.com/jeongukjae/smaller-labse).

## Model Size

| Model                             | #param(transformer) | #param(word embedding) | #param(model) | vocab size |
| --------------------------------- | ------------------: | ---------------------: | ------------: | ---------: |
| google/LaBSE/2                    |               85.1M |                 384.9M |        470.9M |    501,153 |
| jeongukjae/smaller_LaBSE_15lang/1 |               85.1M |                 133.1M |        219.2M |    173,347 |

## Example Use

```python
import tensorflow as tf
import tensorflow_text  # noqa
import tensorflow_hub as hub

# Loading models from tfhub.dev
encoder = hub.KerasLayer("https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1")
preprocessor = hub.KerasLayer("https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1")

# Constructing model to encode texts into high-dimensional vectors
sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name="sentences")
encoder_inputs = preprocessor(sentences)
sentence_representation = encoder(encoder_inputs)["pooled_output"]
normalized_sentence_representation = tf.nn.l2_normalize(sentence_representation, axis=-1)  # for cosine similarity
model = tf.keras.Model(sentences, normalized_sentence_representation)
model.summary()

# Encoding multilingual sentences.
english_sentences = tf.constant(["dog", "Puppies are nice.", "I enjoy taking long walks along the beach with my dog."])
italian_sentences = tf.constant(["cane", "I cuccioli sono carini.", "Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane."])
japanese_sentences = tf.constant(["犬", "子犬はいいです", "私は犬と一緒にビーチを散歩するのが好きです"])

english_embeds = model(english_sentences)
italian_embeds = model(italian_sentences)
japanese_embeds = model(japanese_sentences)

# English-Italian similarity
print(tf.tensordot(english_embeds, italian_embeds, axes=[[1], [1]]))

# English-Japanese similarity
print(tf.tensordot(english_embeds, japanese_embeds, axes=[[1], [1]]))

# Italian-Japanese similarity
print(tf.tensordot(italian_embeds, japanese_embeds, axes=[[1], [1]]))
```

## Output details

The outputs of this model are a dict, and each entries are as follows:

* `"pooled_output"`: pooled output of the entire sequence with shape `[batch size, hidden size(768 for this model)]`. You can use this output as the sentence representation.
* `"sequence_output"`: representations of every token in the input sequence with shape `[batch size, max sequence length, hidden size(768)]`.
* `"encoder_outputs"`: A list of 12 tensors of shapes are `[batch size, sequence length, hidden size(768)]` with the outputs of the i-th Transformer block.

## References

* [Language-agnostic BERT Sentence Embedding](https://arxiv.org/abs/2007.01852)
* [Load What You Need: Smaller Versions of Multilingual BERT](https://arxiv.org/abs/2010.05609).
