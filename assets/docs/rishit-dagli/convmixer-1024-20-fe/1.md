# Module rishit-dagli/convmixer-1024-20-fe/1

ConvMixer is a simple model that uses only standard convolutions to achieve the mixing steps. Despite its simplicity ConvMixer outperforms ViT and MLP-Mixer. This is a feature extractor.

<!-- task: image-classification -->
<!-- network-architecture: convmixer -->
<!-- dataset: imagenet -->
<!-- fine-tunable: true -->
<!-- license: mit -->
<!-- format: saved_model_2 -->
<!-- asset-path: https://storage.googleapis.com/convmixer-hubmodels.appspot.com/convmixer_1024_20_fe.tar.gz -->

### TF2 SavedModel
This is a [SavedModel in TensorFlow 2 format](https://www.tensorflow.org/hub/tf2_saved_model). Using it requires TensorFlow 2 (or 1.15) and TensorFlow Hub 0.5.0 or newer.

### Overview

The ConvMixer model is a simple model proposed in the paper "Patches Are All You Need?" for feature extraction pre-trained on ImageNet-1K.

The paper shows evidence that the performance of ViTs is at least partly due to using patches as the input representation. ConvMixer relies directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network. The authors further boast that is the first model that achieves the elusive dual goals of 80%+ ImageNet top-1 accuracy while a raw implementation also fitting into a tweet.

### Example use

The saved model can be loaded directly:

```py
import tensorflow_hub as hub

hub_layer = hub.KerasLayer(
    "https://tfhub.dev/rishit-dagli/convmixer-1024-20-fe/1",
    signature="serving_default",
    output_key="output",
    trainable=True,
)

model = tf.keras.Sequential(
    [
        hub_layer,
        tf.keras.layers.Reshape((1024,)),
    ]
)
```

The input images are expected to have color values in the range `[0,1]`, following the [common image input](https://www.tensorflow.org/hub/common_signatures/images#input) conventions. The expected size of the input images is height x width = 224 x 224 pixels by default in the defult channels last format. This outputs an array of size `[-1, 1024]` representing the pooled features.

### Notes

1. Due to the signatures of the SavedModel, you should always follow the `hub.KerasLayer` layer with a `tf.keras.layers.Reshape((1024,))` layer like shown in the above usage example. This allows you to place a `Dense` or any other layers for fine-tuning you might want to add.
2. The outputs are already the _pooled_ features, you would not want to add a pooling layer after this.

### References

[1] Anonymous. Patches Are All You Need? 2021. openreview.net, https://openreview.net/forum?id=TVHS5Y4dNvM.
