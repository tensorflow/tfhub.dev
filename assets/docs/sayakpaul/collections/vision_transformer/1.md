# Collection sayakpaul/vit/1

Collection of new Vision Transformer models fine-tuned on ImageNet-1k.

<!-- dataset: imagenet-ilsvrc-2012-cls -->
<!-- task: image-classification -->

## Overview

This collection contains seven different Vision Transformer [1] models that were fine-tuned 
on the ImageNet-1k dataset [2]. For more details on the training protocols, please follow [3].
The authors of [3] open-sourced about 50k different variants of Vision Transformer models in JAX. 
This collection contains seven of the best ImageNet-1k models from that pool. 

The models contained in this collection were converted from the original model classes and
weights [4] using the `jax2tf` tool [5]. For more details on the conversion process, please
follow [this notebook](https://colab.research.google.com/github/sayakpaul/ViT-jax2tf/blob/main/conversion.ipynb).
**Using this notebook, one should be able to take a model (that is not already a part of
this collection) from [4] and convert that to a TensorFlow SavedModel.**

The criteria that were used to select the models included in this collection can be found
in [this notebook](https://colab.research.google.com/github/sayakpaul/ViT-jax2tf/blob/main/model-selector.ipynb). 

## About the models

Models included in this collection have two variants: (1) off-the-shelf inference for image
classification, (2) fine-tuning on downstream tasks. These models are accompanied by
Colab Notebooks for demonstration purposes. 

A huge thanks to the authors of [3] for their open-sourcing efforts and for making the models
as reproducible as possible.

### Image classifiers

* [ViT-S16](https://tfhub.dev/sayakpaul/vit_s16_classification/1)
* [ViT-B16](https://tfhub.dev/sayakpaul/vit_b16_classification/1)
* [ViT-B32](https://tfhub.dev/sayakpaul/vit_b32_classification/1)
* [ViT-L16](https://tfhub.dev/sayakpaul/vit_l16_classification/1)
* [ViT-R26-S32 (light augmentation)](https://tfhub.dev/sayakpaul/vit_r26_s32_lightaug_classification/1)
* [ViT-R26-S32 (medium augmentation)](https://tfhub.dev/sayakpaul/vit_r26_s32_medaug_classification/1)
* [ViT-R50-L32](https://tfhub.dev/sayakpaul/vit_r50_l32_classification/1)

### Feature extractors

* [ViT-S16]
* [ViT-B16]
* [ViT-B32]
* [ViT-L16]
* [ViT-R26-S32 (light augmentation)]
* [ViT-R26-S32 (medium augmentation)]
* [ViT-R50-L32]

## References

[1] [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)

[2] [ImageNet-1k](https://www.image-net.org/challenges/LSVRC/2012/index.php)

[3] [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers by Steiner et al.](https://arxiv.org/abs/2106.10270)

[4] [Vision Transformer GitHub](https://github.com/google-research/vision_transformer)

[5] [jax2tf tool](https://github.com/google/jax/tree/main/jax/experimental/jax2tf/)