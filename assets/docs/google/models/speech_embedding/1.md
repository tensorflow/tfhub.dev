# Module google/speech_embedding/1
Embedding vectors from spoken audio.

<!-- fine-tunable: true -->
<!-- asset-path: legacy -->
<!-- format: hub -->
<!-- module-type: audio-embedding -->
<!-- network-architecture: KeyWordSpottingNet -->

## Overview

This module produces embeddings from audio using a sample rate of 16kHz.

### Input Features

Required audio input:
 * Tensor batch_size x num_samples
 * sample rate: 16kHz
 * each sample should be a 32bit float between -1 and 1.

The module computes its own 32 dimensional log-mel features from the provided
audio samples using the following parameters:
 * stft window size: 25ms
 * stft window step: 10ms
 * mel band limits: 60Hz - 3800Hz
 * mel frequency bins: 32

### Architecture

The speech embedding module uses a stack of 5 convolutional blocks.
Each convolutional block consists of 5 layers:
* a 1x3 convolution,
* a 3x1 convolution,
* a maxpool layer,
* a 1x3 convolution,
* and a 3x1 convolution.

More details will be available from our paper:
James Lin, Kevin Kilgour, Dominik Roblek, Matthew Sharifi
"[Training Keyword Spotters with Limited and Synthesized Speech Data](http://arxiv.org/abs/2002.01322)"


The architecture results in the first embedding requiring
12400 samples (775ms or 76 feature vectors). Each subsequent embedding vector
requires a further 1280 samples (80ms or 8 feature vectors).

The following snippet calculates the expected embedding length.
```python
embedding_length = 1 + (sample_length - 12400) // 1280
```

## Usage

It can be used like this to create embeddings.

```python
module = hub.Module("https://tfhub.dev/google/speech_embedding/1")
samples = ...  # A batch of audio clips with shape [batch_size, sample_length].
embeddings = module(samples)  # Embeddings with shape [batch_size, embedding_length, 1, 96].
```

See the architecture section to understand the relationship between
sample_length and embedding_length. All samples are expected to be 32-bit floats
between -1 and -1.


## Fine-tuning

The module can also be fine-tuned on your task by setting
`trainable=True` and importing the graph version with tag set `{"train"}`
which sets batch normalization to training mode.

```python
module = hub.Module("https://tfhub.dev/google/speech_embedding/1"),
                    trainable=True, tags={"train"})
```

## Example Colabs

Check our these example colabs to see the module in action:
 * [speech_commands.ipynb](https://github.com/google-research/google-research/tree/master/speech_embedding/speech_commands.ipynb)
 Demonstrates how to train a keyword spotting model on the speech commands dataset.
 * [record_train.ipynb](https://github.com/google-research/google-research/tree/master/speech_embedding/record_train.ipynb)
 Shows how a keyword spotting model can be trained on your own voice (requires a microphone).

## Changelog

#### Version 1

  * Initial release.
