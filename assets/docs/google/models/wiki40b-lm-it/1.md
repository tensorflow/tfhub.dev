# Module google/wiki40b-lm-it/1
Embeddings and negative log likelihood from a language model
trained on Italian from Wiki40B dataset with sentence-piece
vocabulary size 32k.

<!-- task: text-language-model -->
<!-- asset-path: legacy -->
<!-- fine-tunable: false -->
<!-- format: hub -->
<!-- language: it -->
<!-- network-architecture: transformer-xl -->
<!-- dataset: wiki40b -->
<!-- colab: https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/wiki40b_lm.ipynb -->

## Overview

The Wiki40B-lm module includes language models trained on the newly published
cleaned-up Wiki40B dataset available on TFDS. The training setup is based on the
paper “Wiki-40B: Multilingual Language Model Dataset” [1].

This specific module is trained on Italian available in Wiki40B dataset.
It is trained using a 12 layers Transformer XL model [2]. The input to the
module is variable length text in Italian, and the output is a 768
dimensional vector of the input word embeddings, activations of all layers, and
the negative log likelihood of the text.

## Details

A medium sized Transformer-XL of 12 layers, with hidden size 768, and 12
attention heads with 64 dimensions, leading to a total number of 141.4M
parameters. A 32k vocabulary spm model learned from sampling 10k examples from
Italian Wiki40B.

### Input

The module takes **a single sentence** as input.

### Prerequisites

This module relies on the
[Tensorflow Text library](https://github.com/tensorflow/text) for input
preprocessing (please make sure the TensorFlow version you use is compatible
with Tensorflow Text library). On
[RaggedTensorToTensor](https://www.tensorflow.org/api_docs/python/tf/raw_ops/RaggedTensorToTensor),
which is only available after TF version 2.1.0.
The memory for the colab needs to be more than 20GB to run the generation demo.

```python
!pip install --quiet tensorflow_text
```

### Example use

```python
import numpy as np
import tensorflow.compat.v1 as tf
import tensorflow_hub as hub
import tensorflow_text as tf_text
tf.disable_eager_execution()

n_layer = 12
d_model = 768
max_gen_len = 128

def generate(module, inputs, mems):
  """Generate text."""
  inputs = tf.dtypes.cast(inputs, tf.int64)
  generation_input_dict = dict(input_tokens=inputs)
  mems_dict = {}
  for i in range(n_layer):
    mems_dict["mem_{}".format(i)] = mems[i]
  generation_input_dict.update(mems_dict)

  generation_outputs = module(generation_input_dict, signature="prediction",
                              as_dict=True)
  probs = generation_outputs["probs"]

  new_mems = []
  for i in range(n_layer):
    new_mems.append(generation_outputs["new_mem_{}".format(i)])

  return probs, new_mems

g = tf.Graph()
with g.as_default():
  module = hub.Module("https://tfhub.dev/google/wiki40b-lm-it/1")
  text = ["\n_START_ARTICLE_\n28th Street (linea IRT Lexington Avenue)\n_START_SECTION_\nStoria\n_START_PARAGRAPH_\nLa stazione, i cui lavori di costruzione ebbero inizio nel 1900, venne aperta il 27 ottobre 1904, come"]

  # Word embeddings.
  embeddings = module(dict(text=text), signature="word_embeddings",
                      as_dict=True)
  embeddings = embeddings["word_embeddings"]

  # Activations at each layer.
  activations = module(dict(text=text),signature="activations", as_dict=True)
  activations = activations["activations"]

  # Negative log likelihood of the text, and perplexity.
  neg_log_likelihood = module(dict(text=text), signature="neg_log_likelihood",
                              as_dict=True)
  neg_log_likelihood = neg_log_likelihood["neg_log_likelihood"]
  ppl = tf.exp(tf.reduce_mean(neg_log_likelihood, axis=1))

  # Tokenization and detokenization with the sentencepiece model.
  token_ids = module(dict(text=text), signature="tokenization", as_dict=True)
  token_ids = token_ids["token_ids"]

  detoken_text = module(dict(token_ids=token_ids), signature="detokenization",
                        as_dict=True)
  detoken_text = detoken_text["text"]

  # Generation
  mems_np = [np.zeros([1, 0, d_model], dtype=np.float32) for _ in range(n_layer)]
  inputs_np = token_ids
  sampled_ids = []
  for step in range(max_gen_len):
    probs, mems_np = generate(module, inputs_np, mems_np)
    sampled_id = tf.random.categorical(tf.math.log(probs[0]), num_samples=1, dtype=tf.int32)
    sampled_id = tf.squeeze(sampled_id)

    sampled_ids.append(sampled_id)
    inputs_np = tf.reshape(sampled_id, [1, 1])

  sampled_ids = tf.expand_dims(sampled_ids, axis=0)
  generated_text = module(dict(token_ids=sampled_ids),
                          signature="detokenization", as_dict=True)
  generated_text = generated_text["text"]

  init_op = tf.group([tf.global_variables_initializer(),
                      tf.tables_initializer()])

# Initialize session.
with tf.Session(graph=g) as session:
  session.run(init_op)
  embeddings, neg_log_likelihood, ppl, activations, token_ids, detoken_text, generated_text = session.run([
    embeddings, neg_log_likelihood, ppl, activations, token_ids, detoken_text, generated_text])
```

## Reference

[1] Mandy Guo, Zihang Dao, Denny Vrandecic, Rami Al-Rfou.
[Wiki-40B: Multilingual Language Model Dataset](https://research.google/pubs/pub49029/).
To appear, LREC, May 2020.

[2] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan
Salakhutdinov.
[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860).
ACL, July 2019.
