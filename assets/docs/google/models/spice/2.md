# Module google/spice/2

A mobile-compatible pitch extraction model to recognize the dominant pitch in
sung audio. Trained (in a self-supervised way) on the MIR-1k dataset.

<!-- module-type: audio-pitch-extraction -->
<!-- asset-path: legacy -->
<!-- fine-tunable: false -->
<!-- format: saved_model -->
<!-- interactive-model-name: spice -->

[![Open Colab notebook]](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/spice.ipynb)

## Overview

### Module description

A mobile-compatible pitch extraction model to recognize the dominant pitch in
sung audio. Trained (in a self-supervised way) on the
[MIR-1k](https://sites.google.com/site/unvoicedsoundseparation/mir-1k) dataset.
Compared to v1 of the same model, v2 was trained on mixed tracks (including both
singer and background music), which makes it more robust to background music and
noise.

Note that the v2 model is also significantly larger (2.3M parameters compared to
180k for v1).

### Input

This model takes raw audio waveform as input.

*   Input is expected to be a list of floats corresponding to audio samples of
    mono audio at 16khz sampling rate (make sure to convert to mono and
    re-sample to 16khz before calling the model). The model normalizes the range
    of the input (it computes a
    [CQT](https://en.wikipedia.org/wiki/Constant-Q_transform) and takes the
    normalized result as input).

### Output

The model outputs two lists of values.

*   **`uncertainties`**: A list of values in the interval [0, 1], each of which
    corresponds to the uncertainty of the model in getting the pitch prediction
    correct (you can use 1 - uncertainty to obtain the model's confidence in
    having identified pitch correctly.)

*   **`pitches`**: A list of values in the interval [0, 1], each of which
    corresponds to the pitch of the input audio. The output value needs to be
    calibrated (with a little labeled data) to obtain the pitch in semitones:
    `semitones = kOffset + kSlope * pitch`. For this particular model, you can
    use the calibration: `semitones = 25.58 + 63.07 * pitch`.

Each output value in the two lists is centered around a multiples of 32ms (512
samples) in the input. For example, if one passes 128ms of audio, the model
returns 5 pitch values, corresponding to times [0ms, 32ms, 64ms, 96ms, 128ms] in
the input audio.

Internally the model computes a
[Constant-Q transform](https://en.wikipedia.org/wiki/Constant-Q_transform),
which requires padding of the input. This padding affects in particular the
first output (corresponding to `t=0ms`) and the last output (corresponding to
`t=128ms`). These two values will end up being less accurate than the remainder
of the output (depending on the application, these can be ignored).

## Usage

### Use SavedModel in Python

The model can be loaded in Python with TensorFlow 2 preinstalled as follows:

```python
import tensorflow_hub as hub
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

model = hub.load("https://tfhub.dev/google/spice/2")

# A single wave, 128 samples (8ms at 16kHz) long.
wave = np.array(np.sin(np.linspace(-np.pi, np.pi, 128)), dtype=np.float32)

# 16 such waves (2048 samples).
waves = np.tile(wave, 16)
plt.plot(waves)

# Run model. One would use real singing as input, here we use the above
# waveform for testing.
input = tf.constant(waves)
output = model.signatures["serving_default"](input)
pitches = output["pitch"]
some_pitch = pitches[2]

def output2hz(pitch_output):
  # Calibration constants
  PT_OFFSET = 25.58
  PT_SLOPE = 63.07
  FMIN = 10.0;
  BINS_PER_OCTAVE = 12.0;
  cqt_bin = pitch_output * PT_SLOPE + PT_OFFSET;
  return FMIN * 2.0 ** (1.0 * cqt_bin / BINS_PER_OCTAVE)

# Should be ~ 125 hz
print(output2hz(some_pitch))
```

## Performance

For details on the evaluation metrics, see the
[SPICE_paper](https://arxiv.org/abs/1910.11664): The model provided here
corresponds to the SPICE model mentioned in Table III (2.38M parameters).

The model obtains the following **raw pitch accuracy** (measured on MIR-1k) on
various mixed audio.

*   91.4% on clean audio (containing just the singing voice).
*   91.2% when mixed with backing track (with 20 dB signal-to-noise ratio)
*   90.0% when mixed with backing track (with 10 dB signal-to-noise ratio)
*   81.6% when mixed with backing track (with 0 dB signal-to-noise ratio)

in which

*   **Raw Pitch Accuracy (RPA)** is defined as the percentage of voiced frames
    for which the pitch error is less than 0.5 semitones.

## Training

### Training dataset

The model was trained on the
[MIR-1k](https://sites.google.com/site/unvoicedsoundseparation/mir-1k) dataset,
in a self-supervised way. That is, the labels were not used during training.
Later, a small subset of the labels was used to calibrate the output of the
model.

### Model architecture

The (simplified) SPICE model architecture is shown below. Two pitch-shifted
versions of the same CQT frame are fed to two encoders with shared weights. The
loss is designed to make the difference between the outputs of the encoders
proportional to the relative pitch difference. In addition (not shown), a
reconstruction loss is added to regularize the model. The model also learns to
produce the confidence/uncertainty of the pitch estimation.

<div width:500 text-align:"center">
<img src="https://1.bp.blogspot.com/-cHsSiIoMqQ0/XcyfHjk07dI/AAAAAAAAE88/aQVUDIYeIZEJc3yYa9nzbxGr6kF5GisZACEwYBhgL/s1600/image2.png" alt="Architecture" width="500">
</div>

For details see

*   Beat Gfeller, Christian Frank, Dominik Roblek, Matt Sharifi, Marco
    Tagliasacchi, Mihajlo Velimirović:
    [“SPICE: Self-supervised Pitch Estimation”](https://arxiv.org/abs/1910.11664), 2019.
*   Or the short version in the
    [Google AI blogpost](http://ai.googleblog.com/2019/11/spice-self-supervised-pitch-estimation.html).

## Suitable Use, Limitations, and Fair Use Terms

### Suitable use cases

This model is suitable for:

*   Extracting the dominant pitch from mixed audio recordings (including voice
    and backing instruments).

### Limitations

*   Depending on the amount of polyphonic audio (multiple singers, or a singer
    with a backing track) the performance may degrade.
*   This model may not work well with background noise (it may still return a
    pitch).

### License

This model follows
*[CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)* license. If
you intend to use it beyond permissible usage, please consult with the model
owners ahead of time.
