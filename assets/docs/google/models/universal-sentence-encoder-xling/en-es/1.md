# Module google/universal-sentence-encoder-xling/en-es/1

English and Spanish language-agnostic text encoder.

<!-- module-type: text-embedding -->
<!-- asset-path: legacy -->
<!-- network-architecture: transformer -->
<!-- fine-tunable: true -->
<!-- format: hub -->


[![Open Colab notebook]](https://colab.research.google.com/github/tensorflow/hub/blob/3880b82596d2cf5401095b6ada51cb2d543c2050/examples/colab/cross_lingual_similarity_with_tf_hub_multilingual_universal_encoder.ipynb)

## Overview

The Universal Sentence Encoder Cross-lingual (XLING) module is an extension of
the
[Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/2)
that includes training on multiple tasks across languages. The multi-task
training setup is based on the paper "Learning Cross-lingual Sentence
Representations via a Multi-task Dual Encoder" [1].

This specific module is trained on **English and Spanish (en-es)** tasks, and
optimized for greater-than-word length text, such as sentences, phrases or short
paragraphs. It is trained on a variety of data sources and tasks, with the goal
of learning text representations that are useful out-of-the-box for a number of
applications. The input to the module is variable length English or Spanish text
and the output is a 512 dimensional vector. We note that one _does not need to
specify the language_ that the input is in, as the model was trained such that
English and Spanish text with similar meanings will have similar (high dot
product score) embeddings. We also note that this model can be used for
monolingual English (and potentially monolingual Spanish) tasks with comparable
or even better performance than the purely English Universal Sentence Encoder.

To learn more about text embeddings, refer to the
[TensorFlow Embeddings](https://www.tensorflow.org/tutorials/text/word_embeddings)
documentation.

#### Prerequisites

This module relies on the
[SentencePiece library](https://github.com/google/sentencepiece) for input
preprocessing. On [Google Colaboratory](https://colab.research.google.com/), the
SentencePiece library is available by:

```python
!pip3 install sentencepiece
!pip3 install tf-sentencepiece
```

#### Example use

```python
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import tf_sentencepiece

# Some texts of different lengths.
english_sentences = ["dog", "Puppies are nice.", "I enjoy taking long walks along the beach with my dog."]
spanish_sentences = ["perro", "Los cachorros son agradables.", "Disfruto de dar largos paseos por la playa con mi perro."]

# Set up graph.
g = tf.Graph()
with g.as_default():
  text_input = tf.placeholder(dtype=tf.string, shape=[None])
  en_es_embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder-xling/en-es/1")
  embedded_text = en_es_embed(text_input)
  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
g.finalize()

# Initialize session.
session = tf.Session(graph=g)
session.run(init_op)

# Compute embeddings.
en_result = session.run(embedded_text, feed_dict={text_input: [english_sentences[0]]})
es_result = session.run(embedded_text, feed_dict={text_input: [spanish_sentences[0]]})

# Compute similarity. Higher score indicates greater similarity.
similarity_score = np.dot(np.squeeze(en_result), np.squeeze(es_result))
```

## References

[1] M. Chidambaram, Y. Yang, D. Cer, S. Yuan, Y.-H. Sung, B. Strope, and R.
Kurzweil. Learning Cross-Lingual Sentence Representations via a Multi-task
Dual-Encoder Model. ArXiv e-prints, October 2018.
