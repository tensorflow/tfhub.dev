# Module google/nonsemantic-speech-benchmark/frill/1
FRILL is a non-semantic speech embedding model presented in "FRILL: A Non-Semantic Speech Embedding for Mobile Devices". It is fast enough to run in real-time on a mobile device and exhibits minimal performance degradation on a benchmark of non-semantic speech tasks. It is 32x faster on a Pixel 1 smartphone and 40% the size of TRILL, with an average decrease in accuracy of only 2%. To our knowledge, FRILL is the highest-quality non-semantic embedding designed for use on mobile devices.

<!-- asset-path: internal -->
<!-- task: audio-embedding -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- network-architecture: mobilenet-v3 -->
<!-- dataset: audioset -->

## Overview

**Links**

1. Benchmark evaluation code: [here](https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark/eval_embedding/sklearn)
1. [FRILL: A Non-Semantic Speech Embedding for Mobile Devices](https://arxiv.org/abs/2011.04609)
1. Blog post (TODO)

This model was trained to 380K steps, so it's slightly more accurate than in the
paper.

The `frill` SavedModel and TFLite models include the frontend. They can take
input of any length. Because special TensorFlow ops are required, the TFLite
model is significantly larger with the frontend. For the smallest version of
`FRILL`, see `frill-nofrontend`. The model outputs of the two models are the
same.

The accuracies on [NOSS](https://arxiv.org/abs/2002.12764) are:

| Dataset | FRILL SavedModel<br>w (wout) frontend | FRILL TFLite<br>w (wout) frontend |  FRILL<br>([paper](https://arxiv.org/abs/2011.04609)) |  TRILL<br>[[1](https://arxiv.org/abs/2002.12764)]      |
| ------- |     -----------                          | ----------                             | -----------                                        |    -----------                                      |
| Voxceleb1         | 45.5 | 45.5 | 44.5 | 46.8  |
| Voxforge          | 78.8 | 78.8 | 76.9 | 84.5  |
| Speech Commands   | 81.2 | 81.0 | 79.7 | 81.7  |
| CREMA-D           | 71.3 | 71.3 | 70.9 | 65.9  |
| SAVEE             | 63.3 | 63.3 | 67.5 | 70.0  |
| Masked Speech     | 68.0 | 68.0 | 65.7 | 65.8  |
| ESC-50 HS         | 87.9 | 87.9 | 86.4 | 86.4  |
| Size (MB)         | (44)   | (38.5) | 38.5 | 98.1  |
| Latency (ms)      | ---  | (8.5) |  8.5 | 275.3 |
<!--                  SM   TFLite  Paper  TRILL    -->

A fast, mobile-capable version of TRILL presented in [FRILL: A Non-Semantic Speech Embedding for Mobile Devices](https://arxiv.org/abs/2011.04609).

### TF 2.X

To run the model in TF 2:

```python
# Import TF 2.X and make sure we're running eager.
import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()
assert tf.executing_eagerly()

import tensorflow_hub as hub
import numpy as np

# Load the module and run inference.
module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/frill/1')
# `wav_as_float_or_int16` can be a numpy array or tf.Tensor of float type or
# int16. The sample rate must be 16kHz. Resample to this sample rate, if
# necessary.
wav_as_float_or_int16 = np.sin(np.linspace(-np.pi, np.pi, 128), dtype=np.float32)
batched_input = np.expand_dims(wav_as_float_or_int16, axis=0)
emb = module(batched_input)['embedding']
# `emb` is a [time, feature_dim] Tensor.
emb.shape.assert_is_compatible_with([None, 2048])

print(emb)
```

### TF 1.X

Generating the embedding in TF 1.X is very similar, we just need to run the
graph in a `tf.Session`:

```python
# Import TF 1.X.
import tensorflow.compat.v1 as tf
assert not tf.executing_eagerly()

import tensorflow_hub as hub
import numpy as np

# Load the module and run inference.
module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/frill/1')
# `wav_as_float_or_int16` can be a numpy array or tf.Tensor of float type or
# int16. The sample rate must be 16kHz. Resample to this sample rate, if
# necessary.
wav_as_float_or_int16 = np.sin(np.linspace(-np.pi, np.pi, 128), dtype=np.float32)
batched_input = np.expand_dims(wav_as_float_or_int16, axis=0)
emb = module(batched_input)['embedding']
# `emb` is a [time, feature_dim] Tensor.
emb.shape.assert_is_compatible_with([None, 2048])
with tf.train.MonitoredSession() as sess:
  emb_np = sess.run(emb)

print(emb_np)
```

## Model Card

### Model Summary

| Category | Description |
| ------   | ----------  |
| Model Architecture | FRILL is a [small MobileNetV3](https://arxiv.org/abs/1905.02244) with alpha=2.0 and [global average pooling](https://arxiv.org/abs/2011.04609). We release 1 SavedModel version, and 2 TFLite versions. |
| Input(s) | We release 3 versions of the model. <ul><li>SavedModel: Takes a tensor of audio samples</li><li>TFLite with frontend: Takes a tensor of audio samples</li><li>TFLite without frontend: Takes a tensor of frontend features</li></ul> |
| Output(s) | The outputs of all three models are a time-series of 2048 feature vectors. There are no existing techniques capable of identifying a person from just these embeddings. |

### Usage

| Category | Description |
| ------   | ----------  |
| Benefits | It is 32x faster on a Pixel 1 smartphone and 40% the size of TRILL, with an average decrease in accuracy of only 2% on the [Non-Semantic Speech Benchmark](https://arxiv.org/abs/2002.12764). <br><br>Some use-cases for TRILL / FRILL are language identification, [video thumbnail selection](https://www.prophy.science/article/120031192), keyword spotting, and the other tasks proposed in the [Interspeech Paralinguistics Challenge](http://www.compare.openaudio.eu/tasks/). |
| Licenses | [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0.html) |

### Model Creators

| Category | Description |
| ------   | ----------  |
| Person of Contact | Joel Shor, Google (joelshor@google.com) |
| Citation | @misc{peplinski2021frill, title={FRILL: A Non-Semantic Speech Embedding for Mobile Devices}, author={Jacob Peplinski and Joel Shor and Sachin Joglekar and Jake Garrison and Shwetak Patel}, year={2021}, eprint={2011.04609}, archivePrefix={arXiv}, primaryClass={cs.SD}} |

### Behaviors and Limitations

| Category | Description |
| ------   | ----------  |
| System Type | These models can be used as standalone (audio to embeddings), or the audio can first be processed into frontend features. Audio must be at least 960 ms long. |
| Known Caveats | Any form of surveillance, biometric processing, or identity recognition is explicitly out of scope and not enabled by this technology. |

### ML Framework(s)

| Category | Description |
| ------   | ----------  |
| Model Framework | TensorFlow 2.0, TFLite |
| Tech Stack | This model is intended to generate a time-series of 2048 dimensional features to be used in downstream tasks. |
| Training Data | Speech subset of the open source [AudioSet](https://research.google.com/audioset/) |
| Testing Data | Non-Semantic Speech Benchmark ([blog post](https://ai.googleblog.com/2020/06/improving-speech-representations-and.html), [paper](http://www.interspeech2020.org/uploadfile/pdf/Mon-1-4-1.pdf), [code](https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark/eval_embedding)) |

### Model Characteristics

| Category | Description |
| ------   | ----------  |
| Model Initialization | The model is trained from random initialization. |
| Model Status | Speech subset of the open source [AudioSet](https://research.google.com/audioset/) in May 2021. |

### Data Overview

| Category | Description |
| ------   | ----------  |
| Training Dataset Breakdown | Size of dataset: [Speech subset](https://research.google.com/audioset/ontology/speech_1.html) of AudioSet as of May 2021, which is ~900K clips |
| Data Format Conditions | We only train on the speech-subset of AudioSet. The input is expected to be sampled at 16kHz. |
| Maintenance & Versions | The model will not be updated, but AudioSet is a dynamic dataset. |
| Instrumentation | Please see the [AudioSet website](https://research.google.com/audioset/) for more details. |
| Evaluation Data | We use: <ul><li>Non-Semantic Speech Benchmark (NOSS). Some examples of datasets / tasks in NOSS are language identification (Voxforge) and word classification (Speech Commands).</li><li>speakers wearing masks \[[1](http://www.interspeech2020.org/index.php?m=content&c=index&a=show&catid=313&id=729)\]</li><li>the human-sound subset of the Environmental Sound Classification dataset \[[1](https://github.com/karolpiczak/ESC-50)\].</li></ul>

### Model Usage & Limitations

| Category | Description |
| ------   | ----------  |
| Intended Usage | The model is intended to replace classical features for non-semantic downstream tasks, such as those used in the "Interspeech Computational Paralinguistics ChallengE (ComParE)" series, which takes place annually at the INTERSPEECH conference. See [this](http://www.compare.openaudio.eu/tasks/) for a list of ComParE tasks. FRILL is designed to be used in low-resource settings, like low-compute phones. |

### Model Evaluation

| Category | Description |
| ------   | ----------  |
| Performance Measure #1 | We use the Non-Semantic Speech Benchmark (NOSS) to evaluate. In addition, we evaluate on a dataset of speakers wearing masks [1](http://www.interspeech2020.org/index.php?m=content&c=index&a=show&catid=313&id=729) and the human-sound subset of the Environmental Sound Classification dataset [1](https://github.com/karolpiczak/ESC-50). For more information on NOSS datasets and tasks, please see [this paper](http://www.interspeech2020.org/uploadfile/pdf/Mon-1-4-1.pdf). Code to reproduce the eval pipeline is [here](https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark/eval_embedding/sklearn). |
| Performance Measure #2 | Our latency numbers were generated using the Pixel 1 smartphone. While this is a common phone that is resource constrained compared to newer models, we do not know the extent to which our results generalize to even more resource constrained environments. Future work can explore benchmarking these smaller models in more constrained environments. The smallest, fastest models might be suitable for use on smart watches or smart home devices. Finally, having a non-semantic speech embedding in a mobile setting unlocks many privacy-sensitive applications. Future work will include benchmarking on more tasks in this category. Shor [4] demonstrated that non-semantic embedding can be fine-tuned for improved performance, and future work includes testing latency and performance for on-device training |

### Evaluation Results

Code to reproduce the eval pipeline is [here](https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark/eval_embedding/sklearn).

We train a set of simple classifiers using embeddings as input features to solve each classification task in the NOSS benchmark. As detailed [here](https://arxiv.org/pdf/2011.04609.pdf), for each dataset in NOSS, we train a logistic regression, random forest, and linear discriminant analysis classifier using the SciKit-Learn library. Embeddings for each utterance are averaged in time to produce a single feature vector. For tasks that contain multiple observations per speaker (SpeechCommands, CREMA-D, SAVEE) we also train a set of classifiers using L2 speaker normalization. We report the best test accuracy across combinations of downstream classifiers and normalization techniques.

## APPENDIX: Key Principles

*Based on Googleâ€™s AI Principles that are most relevant to this model.

| Principle | Relevance |
| ------- |     -----------    |
| Potential Research & Social Benefits         | Because this model leverages pre-training from a large dataset and is on-device, it can unlock privacy-preserving applications of speech technology. Since all the data can remain on a user's phone, a user can finetune FRILL and get a customized model without ever sending their data over a network |
| Bias | This representation was trained on representative data from the Audioset dataset. |
| Privacy | Because this model leverages pre-training from a large dataset and is on-device, it can unlock privacy-preserving applications of speech technology. Since all the data can remain on a user's phone, a user can finetune FRILL and get a customized model without ever sending their data over a network |
| Reproducibility | Open-sourcing this model and eval code will allow speech researchers to fairly compare their techniques to that of FRILL |
