# Module rishit-dagli/convmixer-1536-20/1

ConvMixer is a simple model, that uses only standard convolutions to achieve the mixing steps. Despite it's simplicity ConvMixer outperforms ViT and MLP-Mixer [1].

<!-- task: image-classification -->
<!-- network-architecture: convmixer -->
<!-- dataset: imagenet -->
<!-- fine-tunable: false -->
<!-- license: mit -->
<!-- format: saved_model_2 -->
<!-- asset-path: https://storage.googleapis.com/convmixer-hubmodels.appspot.com/convmixer_1536_20.tar.gz -->
<!-- colab: https://colab.research.google.com/github/Rishit-dagli/ConvMixer-torch2tf/blob/main/classification.ipynb -->

### TF2 SavedModel
This is a [SavedModel in TensorFlow 2 format](https://www.tensorflow.org/hub/tf2_saved_model). Using it requires TensorFlow 2 (or 1.15) and TensorFlow Hub 0.5.0 or newer.

### Overview

The ConvMixer model is a simple model proposed in the paper "Patches Are All You Need?" for image classification pre-trained on ImageNet-1K. ConvMixer-1536/20 with 52M parameters can achieve 81.4% top-1 accuracy on ImageNet-1K.

The paper shows evidence that the performance of ViTs is at least partly due to using patches as the input representation. ConvMixer directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network. The authors further boast that is the first model that achieves the elusive dual goals of 80%+ ImageNet top-1 accuracy while also fitting into a tweet.

### Example use

The saved model can be loaded directly:

```py
import tensorflow_hub as hub

model = hub.load("https://tfhub.dev/rishit-dagli/convmixer-1536-20/1")
```

The input images are expected to have color values in the range `[0,1]`, following the [common image input](https://www.tensorflow.org/hub/common_signatures/images#input) conventions. The expected size of the input images is height x width = 224 x 224 pixels by default in the defult channels last format. This outputs an array of size `[-1, 1000]` corresponding to the 1000 ImageNet-1K classes.

It can also be used within a KerasLayer:

```py
hub_layer = hub.KerasLayer("https://tfhub.dev/rishit-dagli/convmixer-1536-20/1")
```

### A complete example

Make sure to download the class names first:

```sh
!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt -O ilsvrc2012_wordnet_lemmas.txt
```

Here is a complete example to infer on an image:

```py
import tensorflow_hub as hub
import tensorflow as tf
import numpy as np

model = hub.load("https://tfhub.dev/rishit-dagli/convmixer-1536-20/1")


def preprocess_image(image):
    image = np.array(image)
    image_resized = tf.image.resize(image, (224, 224))
    image_resized = tf.cast(image_resized, tf.float32)
    image_resized = image_resized / 255
    # ImageNet stats
    image_resized = tf.keras.layers.Normalization(
        mean=(0.485, 0.456, 0.406), variance=(0.052441, 0.050176, 0.050625)
    )(image_resized)
    return tf.expand_dims(image_resized, 0).numpy()


def load_image_from_url(url):
    response = requests.get(url)
    image = Image.open(BytesIO(response.content))
    image = preprocess_image(image)
    return image


with open("ilsvrc2012_wordnet_lemmas.txt", "r") as f:
    lines = f.readlines()
imagenet_int_to_str = [line.rstrip() for line in lines]


def infer_on_image(img_url, expected_label, model):
    image = load_image_from_url(img_url)
    predictions = model.signatures["serving_default"](tf.constant(image))
    logits = predictions["output"][0]
    predicted_label = imagenet_int_to_str[int(np.argmax(logits))]
    assert (
        predicted_label == expected_label
    ), f"Expected {expected_label} but was {predicted_label}"


infer_on_image(
    img_url="https://storage.googleapis.com/rishit-dagli.appspot.com/sample-images/A5m4ZG1.jpg", expected_label="scorpion", model=model
)
```

### References

[1] Anonymous. Patches Are All You Need? 2021. openreview.net, https://openreview.net/forum?id=TVHS5Y4dNvM.