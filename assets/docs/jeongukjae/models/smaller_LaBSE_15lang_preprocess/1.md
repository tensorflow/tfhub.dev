# Module jeongukjae/smaller_LaBSE_15lang_preprocess/1

Text preprocessing model for [`jeongukjae/smaller_LaBSE_15lang/1`](https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1).

<!-- asset-path: https://storage.googleapis.com/jeongukjae-tf-models/smaller-LaBSE/smaller_LaBSE_15lang_preprocess.tar.gz -->
<!-- task: text-preprocessing -->
<!-- dataset: wikipedia -->
<!-- dataset: commoncrawl -->
<!-- fine-tunable: false -->
<!-- format: saved_model_2 -->
<!-- language: ar -->
<!-- language: zh-cn -->
<!-- language: en -->
<!-- language: fr -->
<!-- language: de -->
<!-- language: it -->
<!-- language: ja -->
<!-- language: ko -->
<!-- language: nl -->
<!-- language: pl -->
<!-- language: pt -->
<!-- language: es -->
<!-- language: th -->
<!-- language: tr -->
<!-- language: ru -->

## Overview

This model is patched version of [`google/universal-sentence-encoder-cmlm/multilingual-preprocess/2`](https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2) to use with [`jeongukjae/smaller_LaBSE_15lang/1`](https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1). This model is constructed by selecting tokens that appear frequently in Wikipedia dump data.

## Prerequisites

This model uses [`BertTokenizer`](https://www.tensorflow.org/text/api_docs/python/text/BertTokenizer) in TensorFlow Text. You can register required ops as follows.

```python
# Install it with "pip install tensorflow-text"
import tensorflow_text as text
```

## Usage

This model is exported using [`official/nlp/tools/export_tfhub_lib.py::export_preprocessing`](https://github.com/tensorflow/models/blob/v2.6.0/official/nlp/tools/export_tfhub_lib.py#L392) in [tf-models-official==2.6.0](https://github.com/tensorflow/models/blob/v2.6.0/), so usage is very similar to the text preprocessing models for BERT in tfhub (for example, [tensorflow/bert_en_cased_preprocess/3](https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3)).

### Preprocess single text segment

```python
sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name="sentences")
preprocessor = hub.KerasLayer("https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1")
model_inputs = preprocessor(sentences)
```

### Preprocess multiple text segments

```python
preprocessor = hub.load("https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1")
tokenize = hub.KerasLayer(preprocessor.tokenize)
bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs)
# You can use different sequence length like below. (default is 128)
#
# bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs, arguments=dict(seq_length=seq_length))

sentences = [
    tf.keras.layers.Input(shape=(), dtype=tf.string, name="segment_a"),
    tf.keras.layers.Input(shape=(), dtype=tf.string, name="segment_a"),
]
tokenized_sentences = [tokenize(segment) for segment in sentences]
model_inputs = bert_pack_inputs(tokenized_sentences)
```

### Output details

The result of preprocessing is a batch of fixed-length input sequences for the Transformer encoder.

An input sequence starts with one start-of-sequence token, followed by the tokenized segments, each terminated by one end-of-segment token. Remaining positions up to `seq_length`, if any, are filled up with padding tokens. If an input sequence would exceed `seq_length`, the tokenized segments in it are truncated to prefixes of approximately equal sizes to fit exactly.

The `encoder_inputs` are a dict of three int32 Tensors, all with shape `[batch_size, seq_length]`, whose elements represent the batch of input sequences as follows:

* `"input_word_ids"`: has the token ids of the input sequences.
* `"input_mask"`: has value 1 at the position of all input tokens present before padding and value 0 for the padding tokens.
* `"input_type_ids"`: has the index of the input segment that gave rise to the input token at the respective position. The first input segment (index 0) includes the start-of-sequence token and its end-of-segment token. The second segment (index 1, if present) includes its end-of-segment token. Padding tokens get index 0 again.
