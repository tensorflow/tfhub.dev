# Collection google/LEALLA/1

LEALLA: Lightweight language-agnostic sentence embedding model supporting 109 languages.

<!-- task: text-embedding -->
<!-- language: multilingual -->
<!-- network-architecture: bert -->
<!-- dataset: commoncrawl -->
<!-- dataset: wikipedia -->
<!-- dataset: translation -->

## Overview

LEALLA \[1\] encodes text into low-dimensional vectors.
It is faster for inference because LEALLA contains fewer model parameters compared with [LaBSE](https://tfhub.dev/google/LaBSE) \[2\].
LEALLA also accelerates applications to downstream tasks because it generates low-dimensional sentence embeddings.

Same as LaBSE, LEALLA is trained and optimized to produce similar representations exclusively for bilingual sentence pairs that are translations of each other.
So it can be used for mining translations of sentences in a larger corpus.
LEALLA is further enhanced by knowledge distillation from LaBSE.

## Models

We trained three LEALLA models: LEALLA-small, LEALLA-base, and LEALLA-large with different model sizes as shown below:

|Model|Number of languages|Dimension of sentence embedding|Number of parameters|Exported SavedModel size|
|:-------------------------------------------------------------|:---:|:---:|:----:|:----:|
| [LEALLA-small](https://tfhub.dev/google/LEALLA/LEALLA-small) | 109 | 128 | 69M  | 263M |
| [LEALLA-base](https://tfhub.dev/google/LEALLA/LEALLA-base)   | 109 | 192 | 107M | 408M |
| [LEALLA-large](https://tfhub.dev/google/LEALLA/LEALLA-large) | 109 | 256 | 147M | 562M |

## References

\[1\] Zhuoyuan Mao, Tetsuji Nakagawa. LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with Knowledge Distillation. EACL 2023.

\[2\] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Narveen Ari, Wei Wang. [Language-agnostic BERT Sentence Embedding](https://arxiv.org/abs/2007.01852). July 2020
