# Module sayakpaul/deit_base_patch16_224_fe/1

Fine-tunable DeiT model pre-trained on the ImageNet-1k dataset.

<!-- asset-path: https://storage.googleapis.com/deit-tf/tars/deit_base_patch16_224_fe.tar.gz  -->
<!-- task: image-classification -->
<!-- network-architecture: deit -->
<!-- format: saved_model_2 -->
<!-- fine-tunable: true -->
<!-- license: mit -->
<!-- colab: https://colab.research.google.com/github/sayakpaul/deit-tf/blob/main/notebooks/finetune.ipynb -->

## Overview

This model is a DeiT [1] model pre-trained on the ImageNet-1k dataset. You can find the complete
collection of DeiT models on TF-Hub on [this page](https://tfhub.dev/sayakpaul/collections/deit/1).

You can use this model for featue extraction and fine-tuning. Please refer to
the Colab Notebook linked on this page for more details.

## Notes

* The original model weights are provided from [2]. There were ported to Keras models
(`tf.keras.Model`) and then serialized as TensorFlow SavedModels. The porting
steps are available in [3].
* The model can be unrolled into a standard Keras model and you can inspect its topology.
To do so, first download the model from TF-Hub and then load it using `tf.keras.models.load_model`
providing the path to the downloaded model folder.

## References

[1] [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)
[2] [DeiT GitHub](https://github.com/facebookresearch/deit)
[3] [DeiT-TF GitHub](https://github.com/sayakpaul/deit-tf)

## Acknowledgements

* [Aritra Roy Gosthipaty](https://github.com/ariG23498)
* [ML-GDE program](https://developers.google.com/programs/experts/)

