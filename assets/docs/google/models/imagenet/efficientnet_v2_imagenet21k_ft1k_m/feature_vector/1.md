# Module google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/1

Feature vectors of images with EfficientNet V2 with input size 480x480, trained on imagenet-21k (Full ImageNet, Fall 2011 release). Fine-tuned on ImageNet1K.

<!-- asset-path: internal -->
<!-- dataset: imagenet-21k -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- network-architecture: efficientnet-v2 -->
<!-- task: image-feature-vector -->
<!-- colab: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/hub/tutorials/tf2_image_retraining.ipynb -->

## TF2 SavedModel

This is a [SavedModel in TensorFlow 2
format](https://www.tensorflow.org/hub/tf2_saved_model).
Using it requires TensorFlow 2 (or 1.15) and TensorFlow Hub 0.5.0 or newer.

## Overview

[EfficientNet V2](https://arxiv.org/abs/2104.00298)
are a family of image classification models,
which achieve better parameter efficiency and faster training speed than prior
arts.

  * Mingxing Tan and Quoc V. Le: ["EfficientNetV2: Smaller Models and Faster Training"](https://arxiv.org/abs/2104.00298v2),
    ICML 2021.

Built upon [EfficientNet V1](https://arxiv.org/abs/1905.11946),
our EfficientNetV2 models use neural architecture
search (NAS) to jointly optimize model size and training speed, and are scaled
up in a way for faster training and inference speed.

<img src="https://www.gstatic.com/aihub/tfhub/efficientnetv2_images/train_params.png" width="50%" />

Here are the comparison on parameters and flops:

<img src="https://www.gstatic.com/aihub/tfhub/efficientnetv2_images/param_flops.png" width="80%" />


We have provided a list of results and checkpoints as follows:

|      ImageNet1K   |     Top1 Acc.  |    Params   |  FLOPs   | Inference Latency |
|    ----------     |      ------    |    ------   | ------  | ------   |
|    EffNetV2-S     |    83.9%   |    21.5M    |  8.4B    | [V100/A100](https://www.gstatic.com/aihub/tfhub/efficientnetv2_images/effnetv2-s-gpu.png) |
|    EffNetV2-M     |    85.2%   |    54.1M    | 24.7B    | [V100/A100](https://www.gstatic.com/aihub/tfhub/efficientnetv2_images/effnetv2-m-gpu.png) |
|    EffNetV2-L     |    85.7%   |   119.5M    | 56.3B    | [V100/A100](https://www.gstatic.com/aihub/tfhub/efficientnetv2_images/effnetv2-l-gpu.png) |

Here is a list of models pretrained on ImageNet21K and fine-tuned on ImageNet1K:

|  ImageNet21K  |  Finetuned ImageNet1K |
|  ----------   |          ------       |
|  EffNetV2-S   |  top1=84.9% |
|  EffNetV2-M   |  top1=86.2% |
|  EffNetV2-L   |  top1=86.9% |

For comparison with EfficientNetV1, we have also provided a few smaller V2 models using the same scaling and preprocessing as V1:

|      ImageNet1K    | Top1 Acc.  |    Params  |  FLOPs   |
|    ----------      |  ------    |    ------  | ------   |
|    EffNetV2-B0     |    78.7%   |    7.1M    | 0.72B    |
|    EffNetV2-B1     |    79.8%   |    8.1M    | 1.2B     |
|    EffNetV2-B2     |    80.5%   |   10.1M    | 1.7B     |
|    EffNetV2-B3     |    82.1%   |   14.4M    | 3.0B     |

[Here](https://github.com/google/automl/tree/master/efficientnetv2#readme)
is the repository for all the source code.


This model contains a trained instance of the network, packaged to get
[feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images#feature-vector).
If you want the full model including the classification it was originally
trained for, use
[`google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/classification/1`](https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/classification/1)
instead.


## Training

The checkpoint exported into this model was `EffNetV2-M for ImageNet21k fine-tuned on ImageNet1K` downloaded
from
[EfficientNetV2 pre-trained models]
(https://github.com/google/automl/blob/master/efficientnetv2).
Its weights were originally obtained by training on the Full ImageNet, Fall 2011 release
dataset for image classification ("imagenet-21k").

## Usage

This model can be used with `hub.KerasLayer` as follows.
It *cannot* be used with the `hub.Module` API for TensorFlow 1.

```python
m = tf.keras.Sequential([
    hub.KerasLayer("https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/1",
                   trainable=False),  # Can be True, see below.
    tf.keras.layers.Dense(num_classes, activation='softmax')
])
m.build([None, 480, 480, 3])  # Batch input shape.
```

The output is a batch of feature vectors. For each input image,
the feature vector has size `num_features` = 1280. The feature
vectors can then be used further, e.g., for classification as above.


The input `images` are expected to have color values in the range [0,1],
following the
[common image input](https://www.tensorflow.org/hub/common_signatures/images#input)
conventions.
For this model, the size of the input images is fixed to
`height` x `width` = 480 x 480 pixels.


## Fine-tuning

Consumers of this model can
[fine-tune](https://www.tensorflow.org/hub/tf2_saved_model#fine-tuning) it
by passing `trainable=True` to `hub.KerasLayer`.

```python
hub.KerasLayer("https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/1", trainable=True)
```


## Changelog

#### Version 1

  * Initial release.
