# Module google/albert_xxlarge/3

ALBERT is "A Lite" version of BERT with greatly reduced number of parameters.

<!-- dataset: Wikipedia -->
<!-- dataset: BooksCorpus -->
<!-- dataset: Stories -->
<!-- dataset: CommonCrawl -->
<!-- dataset: Giga5 -->
<!-- dataset: Clue Web -->
<!-- asset-path: legacy -->
<!-- network-architecture: Transformer -->
<!-- language: en -->
<!-- fine-tunable: true -->
<!-- format: hub -->
<!-- module-type: text-embedding -->

## Overview

ALBERT is "A Lite" version of BERT, a popular unsupervised language
representation learning algorithm. ALBERT uses parameter-reduction techniques
that allow for large-scale configurations, overcome previous memory limitations,
and achieve better behavior with respect to model degradation. The details are
described in the paper "ALBERT: A Lite BERT for Self-supervised Learning of
Language Representations." [1]

This module uses the following `albert_config.json`:

```
{
  "attention_probs_dropout_prob": 0,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0,
  "embedding_size": 128,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 512,
  "num_attention_heads": 64,
  "num_hidden_layers": 12,
  "num_hidden_groups": 1,
  "net_structure_type": 0,
  "layers_to_keep": [],
  "gap_size": 0,
  "num_memory_blocks": 0,
  "inner_group_num": 1,
  "down_scale_factor": 1,
  "type_vocab_size": 2,
  "vocab_size": 30000
}
```

This module assumes pre-processed inputs from the
[ALBERT repository](https://github.com/google-research/google-research/tree/master/albert)

This module outputs a representation for every token in the input sequence and
a pooled representation of the entire input.

#### Trainable parameters

All parameters in the module are trainable, and fine-tuning all parameters is
the recommended practice.

#### Example use

Please see
[https://github.com/google-research/ALBERT/blob/master/run_classifier.py](https://github.com/google-research/ALBERT/blob/master/run_classifier.py)
for how the input preprocessing should be done to retrieve the input ids, masks,
and segment ids.

```
albert_module = hub.Module(
    "https://tfhub.dev/google/albert_xxlarge/2",
    trainable=True)
albert_inputs = dict(
    input_ids=input_ids,
    input_mask=input_mask,
    segment_ids=segment_ids)
albert_outputs = albert_module(albert_inputs, signature="tokens", as_dict=True)
pooled_output = albert_outputs["pooled_output"]
sequence_output = albert_outputs["sequence_output"]
```

The pooled_output is a `[batch_size, hidden_size]` Tensor. The sequence_output
is a `[batch_size, sequence_length, hidden_size]` Tensor.

### Inputs

We currently only support the `tokens` signature, which assumes pre-processed
inputs. `input_ids`, `input_mask`, and `segment_ids` are `int32` Tensors of
shape `[batch_size, max_sequence_length]`

### Outputs

The output dictionary contains:

*   `pooled_output`: pooled output of the entire sequence with shape
    `[batch_size, hidden_size]`.
*   `sequence_output`: representations of every token in the input sequence with
    shape `[batch_size, max_sequence_length, hidden_size]`.

## Changelog

#### Version 3

*   Fixed saved_model.pb to work with TF 1.X. See
    [this GitHub issue](https://github.com/google-research/ALBERT/issues/31)
    for details.

#### Version 2

*   Updated release with best configuration for all models, including no
    dropout, all training data, and training for a longer time

#### Version 1

*   Initial release with the same weights used in the paper [1].

#### References

[1] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,
Radu Soricut. [ALBERT: A Lite BERT for Self-supervised Learning of Language
Representations](https://arxiv.org/abs/1909.11942). arXiv preprint
arXiv:1909.11942, 2019.
