# Module jeongukjae/klue_roberta_cased_preprocess/1

Text preprocessing model for KLUE-RoBERTa.

<!-- asset-path: https://storage.googleapis.com/jeongukjae-tf-models/klue-roberta/klue_roberta_cased_preprocess.tar.gz -->
<!-- task: text-preprocessing -->
<!-- fine-tunable: false -->
<!-- format: saved_model_2 -->
<!-- language: ko -->
<!-- license: cc-by-sa-4.0 -->

## Overview

This model is a text preprocessing model for KLUE-RoBERTa.

This model has no trainable parameters and can be used in an input pipeline outside the training loop.

## Prerequisites

This model uses [`BertTokenizer`](https://www.tensorflow.org/text/api_docs/python/text/BertTokenizer) in TensorFlow Text. You can register required ops as follows.

```python
# Install it with "pip install tensorflow-text"
import tensorflow_text as text
```

## Usage

This model supports preprocessing single or multi segment text inputs.

### Preprocess single text segment

```python
sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name="sentences")
preprocessor = hub.KerasLayer("https://tfhub.dev/jeongukjae/klue_roberta_cased_preprocess/1")
encoder_inputs = preprocessor(sentences)
```

### Preprocess multiple text segments

```python
preprocessor = hub.load("https://tfhub.dev/jeongukjae/klue_roberta_cased_preprocess/1")
tokenize = hub.KerasLayer(preprocessor.tokenize)
bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs)
# You can use different sequence length like below. (default is 128)
#
# bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs, arguments=dict(seq_length=64))

sentences = [
    tf.keras.layers.Input(shape=(), dtype=tf.string, name="segment_a"),
    tf.keras.layers.Input(shape=(), dtype=tf.string, name="segment_b"),
]
tokenized_sentences = [tokenize(segment) for segment in sentences]
encoder_inputs = bert_pack_inputs(tokenized_sentences)
```

### Output details

The result of preprocessing is a batch of fixed-length input sequences for the BERT encoder.

An input sequence starts with one start-of-sequence token, followed by the tokenized segments, each terminated by one end-of-segment token. Remaining positions up to `seq_length`, if any, are filled up with padding tokens. If an input sequence would exceed `seq_length`, the tokenized segments in it are truncated to prefixes of approximately equal sizes to fit exactly.

The `encoder_inputs` are a dict of three int32 Tensors, all with shape `[batch_size, seq_length]`, whose elements represent the batch of input sequences as follows:

* `"input_word_ids"`: has the token ids of the input sequences.
* `"input_mask"`: has value 1 at the position of all input tokens present before padding and value 0 for the padding tokens.
* `"input_type_ids"`: was the index of the input segment that gave rise to the input token at the respective position. But for RoBERTa, this value always has the value 0 at all positions.
