# Collection google/bit/1

Collection of BiT models for feature extraction, and image classification on
Imagenet-1k (ILSVRC-2012-CLS) and Imagenet-21k.

<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- dataset: ImageNet (ILSVRC-2012-CLS) -->
<!-- dataset: ImageNet-21k -->
<!-- module-type: image-classification -->
<!-- module-type: image-feature-vector -->
<!-- network-architecture: ResNet50-v2 -->
<!-- language: en -->

## Overview

Big Transfer (BiT) is a recipe for pre-training image classification models on
large supervised datasets and efficiently fine-tuning them on any given target
task. The recipe achieves excellent performance on a wide variety of tasks, even
when using very few labeled examples from the target dataset.

*   A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly and N.
    Houlsby:
    [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370).

In the paper, three families of models are presented: "BiT-S", pre-trained on
ImageNet-1k (also known as ILSRCV-2012-CLS); "BiT-M", pre-trained on
ImageNet-21k (also known as the "Full ImageNet, Fall 2011 release"); and
"BiT-L", pre-trained on JFT-300M, a proprietary dataset. This collection
contains the BiT-S and BiT-M families.

Each family is composed of a ResNet-50 (R50x1), a ResNet-50 three times wider
(R50x3), a ResNet-101 (R101x1), a ResNet-101 three times wider (R101x3), and our
flagship architecture, a ResNet-152 four times wider (R152x4). Contrary to the
original ResNet architecture, we used Group Normalization instead of Batch
Normalization, and Weight Standardization of the convolution kernels.

### Models

We have released all the models trained on both of the public datasets as TF2
SavedModels. For each architecture, we provide different SavedModels intended to
use for a) feature extraction or fine-tuning on new tasks, b) image
classification on the popular ImageNet (ILSVRC-2012-CLS) dataset, and c)
multi-label image classification on the bigger ImageNet-21k dataset.

#### BiT-S (pre-trained on ImageNet-1k)

Feature extraction:

*   [R50x1](https://tfhub.dev/google/bit/s-r50x1/1)
*   [R50x3](https://tfhub.dev/google/bit/s-r50x3/1)
*   [R101x1](https://tfhub.dev/google/bit/s-r101x1/1)
*   [R101x3](https://tfhub.dev/google/bit/s-r101x3/1)
*   [R152x4](https://tfhub.dev/google/bit/s-r152x4/1)

ImageNet-1k classification:

*   [R50x1](https://tfhub.dev/google/bit/s-r50x1/ilsvrc2012_classification/1)
*   [R50x3](https://tfhub.dev/google/bit/s-r50x3/ilsvrc2012_classification/1)
*   [R101x1](https://tfhub.dev/google/bit/s-r101x1/ilsvrc2012_classification/1)
*   [R101x3](https://tfhub.dev/google/bit/s-r101x3/ilsvrc2012_classification/1)
*   [R152x4](https://tfhub.dev/google/bit/s-r152x4/ilsvrc2012_classification/1)

#### BiT-M (pre-trained on ImageNet-21k)

Feature extraction:

*   [R50x1](https://tfhub.dev/google/bit/m-r50x1/1)
*   [R50x3](https://tfhub.dev/google/bit/m-r50x3/1)
*   [R101x1](https://tfhub.dev/google/bit/m-r101x1/1)
*   [R101x3](https://tfhub.dev/google/bit/m-r101x3/1)
*   [R152x4](https://tfhub.dev/google/bit/m-r152x4/1)

ImageNet-1k classification:

*   [R50x1](https://tfhub.dev/google/bit/m-r50x1/ilsvrc2012_classification/1)
*   [R50x3](https://tfhub.dev/google/bit/m-r50x3/ilsvrc2012_classification/1)
*   [R101x1](https://tfhub.dev/google/bit/m-r101x1/ilsvrc2012_classification/1)
*   [R101x3](https://tfhub.dev/google/bit/m-r101x3/ilsvrc2012_classification/1)
*   [R152x4](https://tfhub.dev/google/bit/m-r152x4/ilsvrc2012_classification/1)

ImageNet-21k classification:

*   [R50x1](https://tfhub.dev/google/bit/m-r50x1/imagenet21k_classification/1)
*   [R50x3](https://tfhub.dev/google/bit/m-r50x3/imagenet21k_classification/1)
*   [R101x1](https://tfhub.dev/google/bit/m-r101x1/imagenet21k_classification/1)
*   [R101x3](https://tfhub.dev/google/bit/m-r101x3/imagenet21k_classification/1)
*   [R152x4](https://tfhub.dev/google/bit/m-r152x4/imagenet21k_classification/1)

### Fine-tuning

All the modules can be fine-tuned. We recommend using the models intended for
feature extraction when fine-tuning on new datasets. In the BiT paper, we
propose a recipe, which we call BiT HyperRule, that transfers very well to a
diverse set of tasks with few labeled data examples. We recommend using these
hyperparameter settings as a starting point.
