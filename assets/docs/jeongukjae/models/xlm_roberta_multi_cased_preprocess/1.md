# Module jeongukjae/xlm_roberta_multi_cased_preprocess/1

Text preprocessing model for `jeongukjae/xlm_roberta_multi_cased_L-24_H-1024_A-16/1` and `jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1`.

<!-- asset-path: https://storage.googleapis.com/jeongukjae-tf-models/RoBERTa/xlm_roberta_cased_preprocess.tar.gz -->
<!-- task: text-preprocessing -->
<!-- fine-tunable: false -->
<!-- license: mit -->
<!-- format: saved_model_2 -->
<!-- language: ar -->
<!-- language: as -->
<!-- language: bg -->
<!-- language: bn -->
<!-- language: ca -->
<!-- language: cs -->
<!-- language: da -->
<!-- language: de -->
<!-- language: el -->
<!-- language: en -->
<!-- language: es -->
<!-- language: et -->
<!-- language: fa -->
<!-- language: fi -->
<!-- language: fr -->
<!-- language: gu -->
<!-- language: he -->
<!-- language: hi -->
<!-- language: hr -->
<!-- language: hu -->
<!-- language: id -->
<!-- language: it -->
<!-- language: ja -->
<!-- language: ko -->
<!-- language: kn -->
<!-- language: lt -->
<!-- language: lv -->
<!-- language: ml -->
<!-- language: mr -->
<!-- language: ms -->
<!-- language: ne -->
<!-- language: nl -->
<!-- language: no -->
<!-- language: or -->
<!-- language: pa -->
<!-- language: pl -->
<!-- language: pt -->
<!-- language: ro -->
<!-- language: ru -->
<!-- language: sa -->
<!-- language: sd -->
<!-- language: sk -->
<!-- language: sl -->
<!-- language: sr -->
<!-- language: sv -->
<!-- language: ta -->
<!-- language: th -->
<!-- language: tr -->
<!-- language: uk -->
<!-- language: ur -->
<!-- language: vi -->
<!-- language: zh-cn -->
<!-- language: zh-tw -->

## Overview

This model is a text preprocessing model for [`jeongukjae/xlm_roberta_multi_cased_L-24_H-1024_A-16/1`](https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_L-24_H-1024_A-16/1) and [`jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1`](https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_L-12_H-768_A-12/1).

This model has no trainable parameters and can be used in an input pipeline outside the training loop.

## Prerequisites

This model uses [`SentencepieceTokenizer`](https://www.tensorflow.org/text/api_docs/python/text/SentencepieceTokenizer) in TensorFlow Text. You can register required ops as follows.

```python
# Install it with "pip install tensorflow-text"
import tensorflow_text as text
```

## Usage

This model supports preprocessing single or multi segment text inputs.

### Preprocess single text segment

```python
sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name="sentences")
preprocessor = hub.KerasLayer("https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_preprocess/1")
encoder_inputs = preprocessor(sentences)
```

### Preprocess multiple text segments

```python
preprocessor = hub.load("https://tfhub.dev/jeongukjae/xlm_roberta_multi_cased_preprocess/1")
tokenize = hub.KerasLayer(preprocessor.tokenize)
bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs)
# You can use different sequence length like below. (default is 128)
#
# bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs, arguments=dict(seq_length=64))

sentences = [
    tf.keras.layers.Input(shape=(), dtype=tf.string, name="segment_a"),
    tf.keras.layers.Input(shape=(), dtype=tf.string, name="segment_b"),
]
tokenized_sentences = [tokenize(segment) for segment in sentences]
encoder_inputs = bert_pack_inputs(tokenized_sentences)
```

### Output details

The result of preprocessing is a batch of fixed-length input sequences for the RoBERTa encoder.

An input sequence starts with one start-of-sequence token, followed by the tokenized segments, each terminated by one end-of-segment token. Remaining positions up to `seq_length`, if any, are filled up with padding tokens. If an input sequence would exceed `seq_length`, the tokenized segments in it are truncated to prefixes of approximately equal sizes to fit exactly.

The `encoder_inputs` are a dict of three int32 Tensors, all with shape `[batch_size, seq_length]`, whose elements represent the batch of input sequences as follows:

* `"input_word_ids"`: has the token ids of the input sequences.
* `"input_mask"`: has value 1 at the position of all input tokens present before padding and value 0 for the padding tokens.
* `"input_type_ids"`: was originally the index of the input segment that gave rise to the input token at the respective position. But for RoBERTa, this value always has the value 0 at all positions.
