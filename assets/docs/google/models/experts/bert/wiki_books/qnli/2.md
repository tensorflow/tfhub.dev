# Module google/experts/bert/wiki_books/qnli/2

BERT trained on Wikipedia and BooksCorpus and fine-tuned on QNLI.

<!-- asset-path: legacy -->
<!-- dataset: qnli -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- task: text-embedding -->
<!-- network-architecture: transformer -->
<!-- colab: https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/bert_experts.ipynb -->

## Overview

This model uses a BERT base architecture[1] initialized from https://tfhub.dev/google/experts/bert/wiki_books/1 and fine-tuned on QNLI[5] and was exported from code in the [TensorFlow Official Model Garden](https://github.com/tensorflow/models/tree/master/official/nlp/bert).

This is a BERT base architecture but some changes have been made to the original training and export scheme based on more recent learnings. See the Datasets & Training section for more details.

## Tokenization

This SavedModel implements the encoder API for [text embeddings with transformer
encoders](https://www.tensorflow.org/hub/common_saved_model_apis/text#transformer-encoders).
It uses an associated **preprocessing model** at
[https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3) that
transforms plain text inputs into the format that can be fed into this model.
Its vocabulary is the original BERT uncased WordPiece vocabulary generated
from English Wikipedia[3] and BooksCorpus[4] datasets.

## Datasets & Training

This model was initialized from the https://tfhub.dev/google/experts/bert/wiki_books/1 checkpoint and then fine-tuned on QNLI for 6 epochs with a batch size of 32 and
Adam optimizer using a 1e-5 learning rate. After training, the pooling layer is
replaced with an identity matrix before the model is exported which we have observed to be more stable during downstream tasks.

See https://tfhub.dev/google/experts/bert/wiki_books/1 for more details on
pre-training.

The training was done using the `run_classifier.py` script from the [TF
Model Garden](https://github.com/tensorflow/models/tree/master/official/nlp/bert).

## Usage

```
!pip3 install --quiet tensorflow-text

import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text  # Imports TF ops for preprocessing.

# Define some sentences to feed into the model
sentences = [
  "Here We Go Then, You And I is a 1999 album by Norwegian pop artist Morten Abel. It was Abel's second CD as a solo artist.",
  "The album went straight to number one on the Norwegian album chart, and sold to double platinum.",
  "Ceylon spinach is a common name for several plants and may refer to: Basella alba Talinum fruticosum",
  "A solar eclipse occurs when the Moon passes between Earth and the Sun, thereby totally or partly obscuring the image of the Sun for a viewer on Earth.",
  "A partial solar eclipse occurs in the polar regions of the Earth when the center of the Moon's shadow misses the Earth.",
]

# Load the BERT encoder and preprocessing models
preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')
bert = hub.load('https://tfhub.dev/google/experts/bert/wiki_books/qnli/2')

# Convert the sentences to bert inputs
bert_inputs = preprocess(sentences)

# Feed the inputs to the model to get the pooled and sequence outputs
bert_outputs = bert(bert_inputs, training=False)
pooled_output = bert_outputs['pooled_output']
sequence_output = bert_outputs['sequence_output']

print('\nSentences:')
print(sentences)
print('\nPooled output:')
print(pooled_output)
print('\nSequence output:')
print(sequence_output)
```

## Uses & Limitations

This model is intended to be used for a variety of English NLP tasks. This model was pre-trained on the English slice of Wikipedia[2] and BooksCorpus[3], so the
model is expected to perform well on English NLP tasks and is not expected to
perform well on text in other languages. The pre-training data contains more
formal text and the model may not generalize to more colloquial text such as
social media or messages.

This model is fine-tuned on the QNLI and is recommended for use in question-based natural language inference tasks.

The QNLI fine-tuning task where is a classification task for a question, context pair, whether the context contains the answer and where the context paragraphs are drawn from Wikipedia. This model may not generalize well to text outside of this domain.
datasets and compare with other
BERT models fine-tuned on different datasets.

## Evaluation

We provide the results of fine-tuning this model on a set of downstream tasks.
The model is trained 5 times on each downstream dataset and the metrics are
reported here with the median value highlighted along with other BERT experts
trained on different datasets.

![Metrics plot comparing BERT experts on downstream tasks](https://www.gstatic.com/aihub/tfhub/experts/bert/metrics_v0.png)

To see more details about the evaluation set-up, check out the
[BERT Experts Collection](https://tfhub.dev/google/collections/experts/bert/1).

## References

\[1]: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. [BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding](https://arxiv.org/abs/1810.04805). arXiv preprint
arXiv:1810.04805, 2018.

\[2]: [Wikipedia dataset](https://dumps.wikimedia.org)

\[3]: [BooksCorpus dataset](http://yknzhu.wixsite.com/mbweb)

[5]: [QNLI dataset](https://gluebenchmark.com/)
