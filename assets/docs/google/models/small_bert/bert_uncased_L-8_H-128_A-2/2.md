# Module google/small_bert/bert_uncased_L-8_H-128_A-2/2

Smaller BERT model

<!-- dataset: Wikipedia and BooksCorpus -->
<!-- asset-path: legacy -->
<!-- network-architecture: Transformer -->
<!-- language: en -->
<!-- fine-tunable: true -->
<!-- format: hub -->
<!-- module-type: text-embedding -->

## Overview

This is one of the smaller BERT models (English only, uncased, trained with
WordPiece masking) referenced in [Well-Read Students Learn Better: On the
Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962).

We have shown that the standard BERT recipe (including model architecture and
training objective) is effective on a wide range of model sizes, beyond
BERT-Base and BERT-Large. The smaller BERT models are intended for environments
with restricted computational resources. They can be fine-tuned in the same
manner as the original BERT models. However, they are most effective in the
context of knowledge distillation, where the fine-tuning labels are produced by
a larger and more accurate teacher.

Our goal is to enable research in institutions with fewer computational
resources and encourage the community to seek directions of innovation
alternative to increasing model capacity.

If you use this model, please cite the following paper:

```
@article{turc2019,
  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962v2 },
  year={2019}
}
```

## Example use

Please see
https://github.com/google-research/bert/blob/master/run_classifier_with_tfhub.py
for how the input preprocessing should be done to retrieve the input ids, masks,
and segment ids.

```
bert_module = hub.Module("https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-128_A-2/1",
                         trainable=True,
                         tags={"train"} if training else None)
bert_inputs = dict(
    input_ids=input_ids,
    input_mask=input_mask,
    segment_ids=segment_ids)
bert_outputs = bert_module(bert_inputs, signature="tokens", as_dict=True)
pooled_output = bert_outputs["pooled_output"]
sequence_output = bert_outputs["sequence_output"]
```

The pooled_output is a `[batch_size, hidden_size]` Tensor. The sequence_output
is a `[batch_size, sequence_length, hidden_size]` Tensor.

### Inputs

We currently only support pre-processed inputs. `input_ids`, `input_mask`, and
`segment_ids` are `int32` Tensors of shape `[batch_size, max_sequence_length]`.

### Outputs

The output dictionary contains:

*   `pooled_output`: pooled output of the entire sequence with shape
    `[batch_size, hidden_size]`.
*   `sequence_output`: representations of every token in the input sequence with
    shape `[batch_size, max_sequence_length, hidden_size]`.

## Changelog

#### Version 1

*   Initial release.

#### Version 2

*   Release consistent with [GitHub](https://github.com/google-research/bert).
