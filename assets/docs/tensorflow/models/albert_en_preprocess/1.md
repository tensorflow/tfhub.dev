# Module tensorflow/&zwnj;albert_en_preprocess/1
Text preprocessing for ALBERT (A Lite BERT)

<!-- asset-path: legacy -->
<!-- dataset: Wikipedia and BooksCorpus -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- module-type: text-preprocessing -->

## Overview

This SavedModel is a companion of ALBERT models to preprocess plain text inputs
into the input format expected by ALBERT.
**Check the encoder model documentation** to
find the correct preprocessing model for each particular encoder.

ALBERT and its preprocessing were originally described by

*   Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,
    Radu Soricut. [ALBERT: A Lite BERT for Self-supervised Learning of Language
    Representations](https://arxiv.org/abs/1909.11942). arXiv preprint
    arXiv:1909.11942, 2019.

This model uses a vocabulary for English extracted from
the Wikipedia and BooksCorpus (same as in the models by the original ALBERT authors).
Text inputs have been normalized the "uncased" way, meaning that the text has
been lower-cased before tokenization into word pieces, and any accent markers
have been stripped.

This model has no trainable parameters and can be used in an input pipeline
outside the training loop.


## Prerequisites

This model uses TensorFlow operations defined by the
[TensorFlow Text](https://github.com/tensorflow/text) library.
On [Google Colaboratory](https://colab.research.google.com/),
it can be installed with

```python
!pip install tensorflow_text
import tensorflow_text as text  # Registers the ops.
```


## Usage

This model expects a batch of text segments (plain text encoded as UTF-8)
and produces a batch of inputs to the ALBERT encoder, one for each segment.

The input text gets normalized and tokenized with the wordpiece vocabulary
of the model. As expected by ALBERT, special tokens are added at the start and
the end. Tokenized inputs are truncated if the length including special tokens
would exceed 128.

```python
text_inputs = tf.keras.layers.Input(shape=(), dtype=tf.string)
preprocessing = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/albert_en_preprocess/1")
encoder_inputs = preprocessing(text_inputs)
```

The `encoder_inputs` are a dict of tensors ready for input into the ALBERT
encoder model: the keys are `"input_word_ids"`, `"input_mask"`, and
`"input_type_ids"` (in any order), and the values are `Tensor`s of type
`tf.int32` and shape `[batch_size, 128]`.


## Changelog

### Version 1

  * Initial release.
