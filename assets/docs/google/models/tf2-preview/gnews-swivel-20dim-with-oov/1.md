# Module google/tf2-preview/gnews-swivel-20dim-with-oov/1
Token based text embedding trained on English Google News
130GB corpus.

<!-- dataset: Google News -->
<!-- asset-path: legacy -->
<!-- language: en -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Swivel -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->


[![Open Colab notebok]](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_text_classification.ipynb)

## Overview
This module is in the **SavedModel 2.0** format and was created to help preview
TF2.0 functionalities.

Text embedding based on Swivel co-occurrence matrix factorization[1] with
pre-built OOV. Maps from text to 20-dimensional embedding vectors.

#### Example use
The saved model can be loaded directly:

```
import tensorflow_hub as hub

embed = hub.load("https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1")
embeddings = embed(["cat is on the mat", "dog is in the fog"])
```

It can also be used within Keras:

```
hub_layer = hub.KerasLayer("https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1", output_shape=[20],
                           input_shape=[], dtype=tf.string)

model = keras.Sequential()
model.add(hub_layer)
model.add(keras.layers.Dense(16, activation='relu'))
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.summary()
```

## Details
Created using Swivel matrix factorization method.

#### Input
The module takes **a batch of sentences in a 1-D tensor of strings** as input.

#### Preprocessing
The module preprocesses its input by **splitting on spaces**.

#### Vocabulary
The vocabulary contains 20,000 tokens with small fraction of the least frequent
tokens and embeddings (~2.5%) **replaced by hash buckets**. Each hash bucket is
initialized using the remaining embedding vectors that hash to the same bucket.

#### Sentence embeddings
Word embeddings are combined into sentence embedding using the `sqrtn` combiner
(see [tf.nn.embedding_lookup_sparse](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse)).

#### References
[1] Noam Shazeer, Ryan Doherty, Colin Evans, Chris Waterson.
[Swivel: Improving Embeddings by Noticing What's Missing](https://arxiv.org/abs/1602.02215).
