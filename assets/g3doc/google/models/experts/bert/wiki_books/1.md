# Module google/&zwnj;experts/&zwnj;bert/&zwnj;wiki_books/1

BERT trained on Wikipedia and BooksCorpus

<!-- dataset: Wikipedia and BooksCorpus -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->

## Overview

This model uses a BERT base architecture[1] pretrained from scratch on Wikipedia and BooksCorpus and was exported from code in the [TensorFlow Official Model Garden](https://github.com/tensorflow/models/tree/master/official/nlp/bert).

This is a BERT base architecture but some changes have been made to the original
training and export scheme based on more recent learnings that improve its accuracy over
the original BERT base checkpoint. For example:

*   See more examples during training with a larger batch size and always using
    the max sequence length, and generating examples with unique masks so an
    exact example is not repeated during training[6]
*   Use contiguous ngram masking[5]
*   The pooling layer is replaced with an identity matrix after training
    which we have observed to be more stable during downstream tasks

See the Datasets & Training section for more details.

## Tokenization

This model uses a tokenizer for transforming raw text into tokens that can be
fed to the model. The vocabulary for the tokenizer is the original BERT uncased WordPiece vocabulary generated from English Wikipedia[3] and BooksCorpus[4] datasets.

## Datasets & Training

This model was trained from scratch on Wikipedia and BooksCorpus.

The model was trained using a joint self-supervised masked language model and
next sentence prediction task. The language modeling task uses whole word
masking up to 3 contiguous masked words with a maximum of 76 predictions. The
model was trained for 500K steps, with a batch size of 2048, and a max sequence
length of 512. The training is done with an Adam optimizer using a learning rate
schedule of linear warm up for 2500 steps to a 4e-4 learning rate, followed by
polynomial decay. After pretraining, the pooling layer is replaced with an
identity matrix before the model is exported which we have observed to be more
stable during downstream tasks

The training was done using the `run_pretraining.py` script from the  [TF
Model Garden](https://github.com/tensorflow/models/tree/master/official/nlp/bert).

## Usage

```
!pip3 install --quiet sentencepiece
!pip3 install --quiet tf-models-official

import numpy as np
import tensorflow.compat.v2 as tf
import tensorflow_hub as hub
import sentencepiece as spm
from official.nlp.bert import tokenization


# Define a helper function for converting sentences input ids, masks, and segment ids
def build_input(tokenizer, sentence, max_seq_length=512):
  """Generate (input_ids, input_mask, segment_ids) for a single sentence."""
  # Tokenize and
  tokens = tokenizer.tokenize(sentence)
  tokens = ["[CLS]"] + tokens + ["[SEP]"]
  ids = tokenizer.convert_tokens_to_ids(tokens)

  # Pad the ids to max sequence length
  pad_len = max_seq_length - len(ids)
  input_ids = ids + [0]*pad_len
  input_mask = [1]*len(ids) + [0]*pad_len

  # Single sentence segment_ids are all 0
  segment_ids = [0]*max_seq_length
  return (input_ids, input_mask, segment_ids)


# Define some sentences to feed into the model
sentences = [
  "Here We Go Then, You And I is a 1999 album by Norwegian pop artist Morten Abel. It was Abel's second CD as a solo artist.",
  "The album went straight to number one on the Norwegian album chart, and sold to double platinum.",
  "Ceylon spinach is a common name for several plants and may refer to: Basella alba Talinum fruticosum",
  "A solar eclipse occurs when the Moon passes between Earth and the Sun, thereby totally or partly obscuring the image of the Sun for a viewer on Earth.",
  "A partial solar eclipse occurs in the polar regions of the Earth when the center of the Moon's shadow misses the Earth.",
]

# Load the BERT model
bert = hub.load('https://tfhub.dev/google/experts/bert/wiki_books/1')

# Load the vocab file and configure the tokenizer
vocab_file = bert.vocab_file.asset_path.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case=bert.do_lower_case)

# Convert the sentences to bert inputs
inputs = [build_input(tokenizer, s) for s in sentences]

# Slice to batch each input tensor
input_ids = np.array([x[0] for x in inputs], dtype=np.int32)
input_masks = np.array([x[1] for x in inputs], dtype=np.int32)
segment_ids = np.array([x[2] for x in inputs], dtype=np.int32)

# Feed the inputs to the model to get the pooled and sequence outputs
pooled_output, sequence_output = bert([input_ids, input_masks, segment_ids])

print('\nSentences:')
print(sentences)
print('\nPooled outputs:')
print(pooled_output)
```

## Uses & Limitations

This model is intended to be used for a variety of English NLP tasks. This model was pre-trained on the English slice of Wikipedia[2] and BooksCorpus[3], so the
model is expected to perform well on English NLP tasks and is not expected to
perform well on text in other languages. The pre-training data contains more
formal text and the model may not generalize to more colloquial text such as
social media or messages.

## References

\[1]: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. [BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding](https://arxiv.org/abs/1810.04805). arXiv preprint
arXiv:1810.04805, 2018.

\[2]: [Wikipedia dataset](https://dumps.wikimedia.org)

\[3]: [BooksCorpus dataset](http://yknzhu.wixsite.com/mbweb)

\[5]: Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer,
Omer Levy.
[SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://arxiv.org/abs/1907.10529).
arxiv:1907.10529, 2019.

\[6]: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.
[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692).
arxiv:1907.11692, 2019.
