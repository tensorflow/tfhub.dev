# Collection sayakpaul/cait/1

Collection of CaiT models.

<!-- dataset: imagenet-ilsvrc-2012-cls -->
<!-- task: image-classification -->

## Overview

This collection contains different Swin Transformer [1, 2] models. For more details on the training protocols,
please follow [3, 4]. The original model weights are provided from [3, 4]. There were ported to Keras models
(`tf.keras.Model`) and then serialized as TensorFlow SavedModels. The porting steps are available in [5].
Some models in this collection were first pre-trained on ImageNet-22k and then fine-tuned on ImageNet-1k.
Rest were directly pre-trained on ImageNet-1k. The former usually leads to better performance.

Note that ImageNet-22k and ImageNet-21k refer to the same dataset.

## About the models

Models included in this collection have two variants: (1) off-the-shelf inference for image
classification, (2) fine-tuning on downstream tasks. These models are accompanied by
Colab Notebooks for demonstration purposes. 

The table below provides a performance summary (ImageNet-1k validation set):

| model_name                     |   top1_acc(%) |   top5_acc(%) |   orig_top1_acc(%) |
|:------------------------------:|:-------------:|:-------------:|:------------------:|
| swin_base_patch4_window7_224   |        85.134 |        97.48  |               85.2 |
| swin_large_patch4_window7_224  |        86.252 |        97.878 |               86.3 |
| swin_s3_base_224               |        83.958 |        96.532 |               84   |
| swin_s3_small_224              |        83.648 |        96.358 |               83.7 |
| swin_s3_tiny_224               |        82.034 |        95.864 |               82.1 |
| swin_small_patch4_window7_224  |        83.178 |        96.24  |               83.2 |
| swin_tiny_patch4_window7_224   |        81.184 |        95.512 |               81.2 |
| swin_base_patch4_window12_384  |        86.428 |        98.042 |               86.4 |
| swin_large_patch4_window12_384 |        87.272 |        98.242 |               87.3 |


The `base` and `large` models were first pre-trained on the ImageNet-22k dataset and then fine-tuned
on the ImageNet-1k dataset.

[This directory](https://github.com/sayakpaul/swin-transformers-tf/tree/main/in1k-eval) provides details
on how these numbers were generated. Original scores for all the models except for the `s3` ones were
gathered from [here](https://github.com/microsoft/Swin-Transformer/blob/main/get_started.md). Scores
for the `s3` model were gathered from [here](https://github.com/microsoft/Cream/tree/main/AutoFormerV2#model-zoo).

### Image classifiers

* [swin_base_patch4_window12_384](https://tfhub.dev/sayakpaul/swin_base_patch4_window12_384)
* [swin_base_patch4_window7_224](https://tfhub.dev/sayakpaul/swin_base_patch4_window7_224)
* [swin_large_patch4_window12_384](https://tfhub.dev/sayakpaul/swin_large_patch4_window12_384)
* [swin_large_patch4_window7_224](https://tfhub.dev/sayakpaul/swin_large_patch4_window7_224)
* [swin_small_patch4_window7_224](https://tfhub.dev/sayakpaul/swin_small_patch4_window7_224)
* [swin_tiny_patch4_window7_224](https://tfhub.dev/sayakpaul/swin_tiny_patch4_window7_224)
* [swin_base_patch4_window12_384_in22k](https://tfhub.dev/sayakpaul/swin_base_patch4_window12_384_in22k)
* [swin_base_patch4_window7_224_in22k](https://tfhub.dev/sayakpaul/swin_base_patch4_window7_224_in22k)
* [swin_large_patch4_window12_384_in22k](https://tfhub.dev/sayakpaul/swin_large_patch4_window12_384_in22k)
* [swin_large_patch4_window7_224_in22k](https://tfhub.dev/sayakpaul/swin_large_patch4_window7_224_in22k)
* [swin_s3_tiny_224](https://tfhub.dev/sayakpaul/swin_s3_tiny_224)
* [swin_s3_small_224](https://tfhub.dev/sayakpaul/swin_s3_small_224)
* [swin_s3_base_224](https://tfhub.dev/sayakpaul/swin_s3_base_224)


### Feature extractors

* [swin_base_patch4_window12_384_fe](https://tfhub.dev/sayakpaul/swin_base_patch4_window12_384_fe)
* [swin_base_patch4_window7_224_fe](https://tfhub.dev/sayakpaul/swin_base_patch4_window7_224_fe)
* [swin_large_patch4_window12_384_fe](https://tfhub.dev/sayakpaul/swin_large_patch4_window12_384_fe)
* [swin_large_patch4_window7_224_fe](https://tfhub.dev/sayakpaul/swin_large_patch4_window7_224_fe)
* [swin_small_patch4_window7_224_fe](https://tfhub.dev/sayakpaul/swin_small_patch4_window7_224_fe)
* [swin_tiny_patch4_window7_224_fe](https://tfhub.dev/sayakpaul/swin_tiny_patch4_window7_224_fe)
* [swin_base_patch4_window12_384_in22k_fe](https://tfhub.dev/sayakpaul/swin_base_patch4_window12_384_in22k_fe)
* [swin_base_patch4_window7_224_in22k_fe](https://tfhub.dev/sayakpaul/swin_base_patch4_window7_224_in22k_fe)
* [swin_large_patch4_window12_384_in22k_fe](https://tfhub.dev/sayakpaul/swin_large_patch4_window12_384_in22k_fe)
* [swin_large_patch4_window7_224_in22k_fe](https://tfhub.dev/sayakpaul/swin_large_patch4_window7_224_in22k_fe)
* [swin_s3_tiny_224_fe](https://tfhub.dev/sayakpaul/swin_s3_tiny_224_fe)
* [swin_s3_small_224_fe](https://tfhub.dev/sayakpaul/swin_s3_small_224_fe)
* [swin_s3_base_224_fe](https://tfhub.dev/sayakpaul/swin_s3_base_224_fe)'

## Notes

All the models output attention weights from each of the transformer blocks. The [classification Colab Notebook](https://colab.research.google.com/github/sayakpaul/cait-tf/blob/main/notebooks/classification.ipynb) shows how to fetch the attention weights for a given prediction image. 


## References

[1] [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows Liu et al.](https://arxiv.org/abs/2103.14030)

[2] [Searching the Search Space of Vision Transformer by Chen et al.](https://arxiv.org/abs/2111.14725)

[3] [Swin Transformers GitHub](https://github.com/microsoft/Swin-Transformer)

[4] [AutoFormerV2 GitHub](https://github.com/silent-chen/AutoFormerV2-model-zoo)

[5] [Swin-TF GitHub](https://github.com/sayakpaul/swin-transformers-tf)


## Acknowledgements

* [Willi Gierke](https://ch.linkedin.com/in/willi-gierke)
* [ML-GDE program](https://developers.google.com/programs/experts/)
