# Module google/universal-sentence-encoder-cmlm/multilingual-preprocess/2

Text preprocessing for universal-sentence-encoder-cmlm multilingual models.

<!-- asset-path: internal -->
<!-- dataset: wikipedia -->
<!-- dataset: commoncrawl -->
<!-- fine-tunable: false -->
<!-- format: saved_model_2 -->
<!-- language: multilingual -->
<!-- module-type: text-preprocessing -->
<!-- task: text-preprocessing -->

## Overview

universal-sentence-encoder-cmlm [1] uses a BERT [2] transformer backbone. So
this SavedModel is a companion to the BERT models for preprocsssing plain text
inputs into the input format expected by BERT. **Check the encoder model
documentation** to find the correct preprocessing model for each particular
encoder.

This model uses a vocabulary for multilingual models extracted from the
Wikipedia, CommonCrawl, and translation pairs from Web.

This model has no trainable parameters and can be used in an input pipeline
outside the training loop.

## Prerequisites

This SavedModel uses TensorFlow operations defined by the
[TensorFlow Text](https://github.com/tensorflow/text) library. On
[Google Colaboratory](https://colab.research.google.com/), it can be installed
with

```python
!pip install tensorflow_text
import tensorflow_text as text  # Registers the ops.
```

## Usage

This SavedModel implements the preprocessor API for
[text embeddings with Transformer encoders](https://www.tensorflow.org/hub/common_saved_model_apis/text#transformer-encoders),
which offers several ways to go from one or more batches of text segments (plain
text encoded as UTF-8) to the inputs for the Transformer encoder model.

This preprocessor API is for
[universal-sentence-encoder-cmlm multilingual models](https://tfhub.dev/s?q=universal-sentence-encoder-cmlm).
See the detailed usage in the
[model page](https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1).

### Basic usage for single segments

Inputs with a single text segment can be mapped to encoder inputs like this:

```python
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)

preprocessor = hub.KerasLayer(
    "https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2")

encoder_inputs = preprocessor(text_input)
```

The resulting encoder inputs have `seq_length=128`.

See [common API doc](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2)
For pairs of input segments, to control the `seq_length`, or to modify tokenized
sequences before packing them into encoder inputs.

### Output details

The result of preprocessing is a batch of fixed-length input sequences for the
Transformer encoder.

An input sequence starts with one start-of-sequence token, followed by the
tokenized segments, each terminated by one end-of-segment token. Remaining
positions up to `seq_length`, if any, are filled up with padding tokens. If an
input sequence would exceed `seq_length`, the tokenized segments in it are
truncated to prefixes of approximately equal sizes to fit exactly.

The `encoder_inputs` are a dict of three int32 Tensors, all with shape
`[batch_size, seq_length]`, whose elements represent the batch of input
sequences as follows:

*   `"input_word_ids"`: has the token ids of the input sequences.
*   `"input_mask"`: has value 1 at the position of all input tokens present
    before padding and value 0 for the padding tokens.
*   `"input_type_ids"`: has the index of the input segment that gave rise to the
    input token at the respective position. The first input segment (index 0)
    includes the start-of-sequence token and its end-of-segment token. The
    second segment (index 1, if present) includes its end-of-segment token.
    Padding tokens get index 0 again.

## References

[1] Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve. [Universal Sentence
Representations Learning with Conditional Masked Language
Model](https://openreview.net/forum?id=WDVD4lUCTzU). November 2020

[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova: ["BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding"](https://arxiv.org/abs/1810.04805), 2018.
