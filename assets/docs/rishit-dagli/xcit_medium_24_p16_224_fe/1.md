 # Module rishit-dagli/xcit_medium_24_p16_224_fe/1

    Fine-tunable XCiT: Cross-Covariance Image Transformers model, pre-trained on the ImageNet-22k dataset and was then fine-tuned on the ImageNet-1k dataset.

    <!-- task: image-classification -->
    <!-- network-architecture: xcit -->
    <!-- dataset: imagenet -->
    <!-- fine-tunable: true -->
    <!-- license: mit -->
    <!-- format: saved_model_2 -->
    <!-- asset-path: https://storage.googleapis.com/hub-models.appspot.com/xcit/xcit_medium_24_p16_224_dist_fe.tar.gz -->

    ### TF2 SavedModel
    This is a [SavedModel in TensorFlow 2 format](https://www.tensorflow.org/hub/tf2_saved_model). Using it requires TensorFlow 2 (or 1.15) and TensorFlow Hub 0.5.0 or newer.

    ### Overview

    This model is a XCiT: Cross-Covariance Image Transformer [1] pre-trained on the ImageNet-22k dataset and was then fine-tuned on the ImageNet-1k dataset. You can find the complete collection of XCiT models on TF-Hub on [this page](https://tfhub.dev/rishit-dagli/collections/xcit/1). You can use this model for feature extraction and fine-tuning.

    The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. The flexibility comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images this paper proposes a "transposed" version of self-attention that operates across feature channels rather than tokens. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. [1]

    ### Example use

    The saved model can be loaded directly:

    ```py
    import tensorflow_hub as hub

    model = hub.load("https://tfhub.dev/rishit-dagli/xcit_medium_24_p16_224_fe/1")
    ```

    The input images are expected to have color values in the range `[-1, 1]`, following the [common image input](https://www.tensorflow.org/hub/common_signatures/images#input) conventions. The expected size of the input images is height x width = 224 x 224 pixels by default in the defult channels last format. This outputs an array of size `[-1, 1000]` corresponding to the 1000 ImageNet-1K classes.

    It can also be used within a `KerasLayer`:

    ```py
    hub_layer = hub.KerasLayer("https://tfhub.dev/rishit-dagli/xcit_medium_24_p16_224_fe/1")
    ```

    ### A complete example

    Make sure to download the class names first:

    ```sh
    !wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt -O ilsvrc2012_wordnet_lemmas.txt
    ```

    Here is a complete example to infer on an image:

    ```py
    import tensorflow_hub as hub
    import tensorflow as tf
    import numpy as np
    import requests
    from PIL import Image
    from io import BytesIO

    model = hub.load("https://tfhub.dev/rishit-dagli/xcit_medium_24_p16_224_fe/1")


    def preprocess_image(image):
        image = np.array(image)
        image_resized = tf.image.resize(image, (224, 224))
        image_resized = tf.cast(image_resized, tf.float32)
        image_resized / 127.5 - 1
        # ImageNet stats
        image_resized = tf.keras.layers.Normalization(
            mean=(0.485, 0.456, 0.406), variance=(0.052441, 0.050176, 0.050625)
        )(image_resized)
        return tf.expand_dims(image_resized, 0).numpy()


    def load_image_from_url(url):
        response = requests.get(url)
        image = Image.open(BytesIO(response.content))
        image = preprocess_image(image)
        return image


    with open("ilsvrc2012_wordnet_lemmas.txt", "r") as f:
        lines = f.readlines()
    imagenet_int_to_str = [line.rstrip() for line in lines]


    def infer_on_image(img_url, model):
        image = load_image_from_url(img_url)
        predictions = model.signatures["serving_default"](tf.constant(image))
        logits = predictions["output"][0]
        predicted_label = imagenet_int_to_str[int(np.argmax(logits))]
        print("Predicted label: " + predicted_label)


    infer_on_image(
        img_url="https://storage.googleapis.com/rishit-dagli.appspot.com/sample-images/A5m4ZG1.jpg", model=model
    )
    ```

    ### Acknowledgements

    Supported with Cloud TPUs from [Google's TPU Research Cloud (TRC)](https://sites.research.google/trc)

    ### References

    [1] Ali, A., Touvron, H., Caron, M., Bojanowski, P., Douze, M., Joulin, A., Laptev, I., Neverova, N., Synnaeve, G., Verbeek, J., and Jegou, H. 2021. XCiT: Cross-Covariance Image Transformers. In Advances in Neural Information Processing Systems (pp. 20014â€“20027). Curran Associates, Inc..

    [2] XCiT official code: https://github.com/facebookresearch/xcit

    [3] BiT ImageNet-22k model usage: https://tfhub.dev/google/bit/m-r50x1/imagenet21k_classification/1#usage

    [4] PyTorch Image Models: https://github.com/rwightman/pytorch-image-models
    