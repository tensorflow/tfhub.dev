# Module google/spice/1

A mobile-compatible pitch extraction model to recognize the dominant pitch in
sung audio. Trained (in a self-supervised way) on the MIR-1k dataset.

<!-- module-type: audio-pitch-extraction -->
<!-- asset-path: legacy -->
<!-- fine-tunable: false -->
<!-- format: saved_model -->
<!-- interactive-model-name: spice -->

## Overview

### Module description

A mobile-compatible pitch extraction model to recognize the dominant pitch in
sung audio. Trained (in a self-supervised way) on the
[MIR-1k](https://sites.google.com/site/unvoicedsoundseparation/mir-1k) dataset.

### Input

This model takes raw audio waveform as input.

*   Input is expected to be a list of floats corresponding to audio samples of
    mono audio at 16khz sampling rate (make sure to convert to mono and
    re-sample to 16khz before calling the model). The model normalizes the range
    of the input (it computes a
    [CQT](https://en.wikipedia.org/wiki/Constant-Q_transform) and takes the
    normalized result as input).

### Output

The model outputs two lists of values.

*   **`uncertainties`**: A list of values in the interval [0, 1], each of which
    corresponds to the uncertainty of the model in getting the pitch prediction
    correct (you can use 1 - uncertainty to obtain the model's confidence in
    having identified pitch correctly.)

*   **`pitches`**: A list of values in the interval [0, 1], each of which
    corresponds to the pitch of the input audio. The output value needs to be
    calibrated (with a little labeled data) to obtain the pitch in semitones:
    `semitones = kOffset + kSlope * pitch`. For this particular model, you can
    use the calibration: `semitones = 24.37 + 62.51 * pitch`.

Each output value in the two lists is centered around a multiples of 32ms (512
samples) in the input. For example, if one passes 128ms of audio, the model
returns 5 pitch values, corresponding to times [0ms, 32ms, 64ms, 96ms, 128ms] in
the input audio.

Internally the model computes a
[Constant-Q transform](https://en.wikipedia.org/wiki/Constant-Q_transform),
which requires padding of the input. This padding affects in particular the
first output (corresponding to `t=0ms`) and the last output (corresponding to
`t=128ms`). These two values will end up being less accurate than the remainder
of the output (depending on the application, these can be ignored).

## Usage

### Use SavedModel in Python

The model can be loaded in Python with TensorFlow 2 preinstalled as follows:

```python
import tensorflow_hub as hub
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

model = hub.load("https://tfhub.dev/google/spice/1")

# A single wave, 128 samples (8ms at 16kHz) long.
wave = np.array(np.sin(np.linspace(-np.pi, np.pi, 128)), dtype=np.float32)

# 16 such waves (2048 samples).
waves = np.tile(wave, 16)
plt.plot(waves)

# Run model. One would use real singing as input, here we use the above
# waveform for testing.
input = tf.constant(waves)
output = model.signatures["serving_default"](input)
pitches = output["pitch"]
some_pitch = pitches[2]

def output2hz(pitch_output):
  # Calibration constants
  PT_OFFSET = 24.374
  PT_SLOPE = 62.511
  FMIN = 10.0;
  BINS_PER_OCTAVE = 12.0;
  cqt_bin = pitch_output * PT_SLOPE + PT_OFFSET;
  return FMIN * 2.0 ** (1.0 * cqt_bin / BINS_PER_OCTAVE)

# Should be ~ 125 hz
print(output2hz(some_pitch))
```

## Performance

For details on the evaluation metrics, see the
[SPICE_paper](https://arxiv.org/abs/1910.11664): Table II shows results for
SPICE models of various sizes trained on a donated singer dataset.  The model
provided here matches the architecture of the smallest one (with 180k
parameters) but is trained on the public
[MIR-1k](https://sites.google.com/site/unvoicedsoundseparation/mir-1k) dataset.

The evaluation metrics are

*   **Raw Pitch Accuracy**: 91.3%
*   **Voicing Recall Rate**: 86.5%

in which

*   **Raw Pitch Accuracy (RPA)** is defined as the percentage of voiced frames
    for which the pitch error is less than 0.5 semitones.
*   **Voicing Recall Rate (VRR)** refers to the proportion of voiced frames in
    the ground truth that are recognized as 'voiced' by the algorithm.

## Training

### Training dataset

The model was trained on the
[MIR-1k](https://sites.google.com/site/unvoicedsoundseparation/mir-1k) dataset,
in a self-supervised way. That is, the labels were not used during training.
Later, a small subset of the labels was used to calibrate the output of the
model.

### Model architecture

The (simplified) SPICE model architecture is shown below. Two pitch-shifted
versions of the same CQT frame are fed to two encoders with shared weights. The
loss is designed to make the difference between the outputs of the encoders
proportional to the relative pitch difference. In addition (not shown), a
reconstruction loss is added to regularize the model. The model also learns to
produce the confidence/uncertainty of the pitch estimation.

<div width:500 text-align:"center">
<img src="https://1.bp.blogspot.com/-cHsSiIoMqQ0/XcyfHjk07dI/AAAAAAAAE88/aQVUDIYeIZEJc3yYa9nzbxGr6kF5GisZACEwYBhgL/s1600/image2.png" alt="Architecture" width="500">
</div>

For details see

*   Beat Gfeller, Christian Frank, Dominik Roblek, Matt Sharifi, Marco
    Tagliasacchi, Mihajlo Velimirović:
    [“SPICE: Self-supervised Pitch Estimation”](https://arxiv.org/abs/1910.11664), 2019.
*   Or the short version in the
    [Google AI blogpost](http://ai.googleblog.com/2019/11/spice-self-supervised-pitch-estimation.html).

## Suitable Use, Limitations, and Fair Use Terms

### Suitable use cases

This model is suitable for:

*   Extracting the dominant pitch from a cappella singing (monophonic audio).

### Limitations

*   This model may not work well with polyphonic audio (multiple singers, or a
    singer with a backing track).
*   This model may not work well with background noise (it may still return a
    pitch).

### License

This model follows
*[CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)* license. If
you intend to use it beyond permissible usage, please consult with the model
owners ahead of time.
