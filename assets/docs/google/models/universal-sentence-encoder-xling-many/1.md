# Module google/universal-sentence-encoder-xling-many/1

English, French, German, Spanish, Italian, Chinese, Korean, and Japanese (8
language) text encoder.

<!-- module-type: text-embedding -->
<!-- asset-path: legacy -->
<!-- network-architecture: transformer -->
<!-- fine-tunable: true -->
<!-- format: hub -->


[![Open Colab notebook]](https://colab.research.google.com/github/tensorflow/hub/blob/3880b82596d2cf5401095b6ada51cb2d543c2050/examples/colab/cross_lingual_similarity_with_tf_hub_multilingual_universal_encoder.ipynb)

## Overview

The Universal Sentence Encoder Cross-lingual (XLING) module is an extension of
the
[Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/2)
that includes training on multiple tasks across languages. The multi-task
training setup is based on the paper "Learning Cross-lingual Sentence
Representations via a Multi-task Dual Encoder" [1].

This specific module is trained on **English, French, German, Spanish, Italian,
Chinese, Korean, and Japanese** tasks, and optimized for greater-than-word
length text, such as sentences, phrases or short paragraphs. It is trained on a
variety of data sources and tasks, with the goal of learning text
representations that are useful out-of-the-box for a number of applications. The
input to the module is variable length text in any of the eight aforementioned
languages and the output is a 512 dimensional vector. We note that one _does not
need to specify the language_ of the input, as the model was trained such that
text across languages with similar meanings will have embeddings with high dot
product scores.

To learn more about text embeddings, refer to the
[TensorFlow Embeddings](https://www.tensorflow.org/tutorials/text/word_embeddings)
documentation.

#### Prerequisites

This module relies on the
[SentencePiece library](https://github.com/google/sentencepiece) for input
preprocessing (please make sure the TensorFlow version you use is compatible
with the SentencePiece library). On
[Google Colaboratory](https://colab.research.google.com/), the SentencePiece
library is available by:

```python
!pip3 install sentencepiece
!pip3 install tf-sentencepiece
```

#### Example use

```python
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import tf_sentencepiece

# Some texts of different lengths.
english_sentences = ["dog", "Puppies are nice.", "I enjoy taking long walks along the beach with my dog."]
italian_sentences = ["cane", "I cuccioli sono carini.", "Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane."]
japanese_sentences = ["犬", "子犬はいいです", "私は犬と一緒にビーチを散歩するのが好きです"]

# Set up graph.
g = tf.Graph()
with g.as_default():
  text_input = tf.placeholder(dtype=tf.string, shape=[None])
  xling_8_embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder-xling-many/1")
  embedded_text = xling_8_embed(text_input)
  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
g.finalize()

# Initialize session.
session = tf.Session(graph=g)
session.run(init_op)

# Compute embeddings.
en_result = session.run(embedded_text, feed_dict={text_input: english_sentences})
it_result = session.run(embedded_text, feed_dict={text_input: italian_sentences})
ja_result = session.run(embedded_text, feed_dict={text_input: japanese_sentences})

# Compute similarity matrix. Higher score indicates greater similarity.
similarity_matrix_it = np.inner(en_result, it_result)
similarity_matrix_ja = np.inner(en_result, ja_result)
```

## References

[1] M. Chidambaram, Y. Yang, D. Cer, S. Yuan, Y.-H. Sung, B. Strope, and R.
Kurzweil. Learning Cross-Lingual Sentence Representations via a Multi-task
Dual-Encoder Model. ArXiv e-prints, October 2018.
