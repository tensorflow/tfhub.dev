# Module tensorflow/&zwnj;small_bert/&zwnj;bert_en_uncased_L-12_H-128_A-2/1
Smaller BERT model.

<!-- dataset: Wikipedia and BooksCorpus -->
<!-- asset-path: legacy -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->

## Overview

This is one of the smaller BERT models referenced in
[Well-Read Students Learn Better: On the
Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962),
republished for use with TensorFlow 2.

We have shown that the standard BERT recipe (including model architecture and
training objective) is effective on a wide range of model sizes, beyond
BERT-Base and BERT-Large. The smaller BERT models are intended for environments
with restricted computational resources. They can be fine-tuned in the same
manner as the original BERT models. However, they are most effective in the
context of knowledge distillation, where the fine-tuning labels are produced by
a larger and more accurate teacher.

Our goal is to enable research in institutions with fewer computational
resources and encourage the community to seek directions of innovation
alternative to increasing model capacity.

If you use this model in published work, please cite the following paper:

```
@article{turc2019,
  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962v2 },
  year={2019}
}
```

This TF Hub model uses the implementation of BERT from the
TensorFlow Models repository on GitHub at
[tensorflow/models/official/nlp/bert](https://github.com/tensorflow/models/tree/master/official/nlp/bert).
It uses L=12 hidden layers (i.e., Transformer blocks),
a hidden size of H=128,
and A=2 attention heads.

The weights of this model are unchanged from its original release for
TensorFlow 1. This model has been pre-trained for English
on the Wikipedia and BooksCorpus.
Text inputs have been normalized the "uncased" way, meaning that the text has
been lower-cased before tokenization into word pieces, and any accent markers
have been stripped. For training, random input masking has been applied independently to word pieces
(as in the original BERT paper).

All parameters in the module are trainable, and fine-tuning all parameters is
the recommended practice.


## Usage

This model expects three int32 Tensors as input: numeric token ids,
an input mask to hold out padding tokens,
and input types to mark different segments within one input (if any).
The separate **preprocessing model** at
[https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1)
transforms plain text inputs into this format.

```python
seq_length = 128  # Your choice here.
inputs = dict(
    input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32))
encoder = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1",
    trainable=True)
outputs = encoder(inputs)
pooled_output = outputs["pooled_output"]      # [batch_size, 128].
sequence_output = outputs["sequence_output"]  # [batch_size, seq_length, 128].
```

The encoder's outputs are the `pooled_output` to represents each input sequence
as a whole, and the `sequence_output` to represent each input token in context.

For advanced uses, the intermediate activations of all L=12
Transformer blocks (hidden layers) are returned as a Python list:
`outputs["encoder_outputs"][i]` is a Tensor
of shape `[batch_size, seq_length, 128]`
with the outputs of the i-th Transformer block, for `0 <= i < L`.
The last value of the list is equal to `sequence_output`.


## Changelog

### Version 1

  * Initial release.
