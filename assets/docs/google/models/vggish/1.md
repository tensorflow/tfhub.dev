# Module google/vggish/1
An audio event embedding model trained on the YouTube-8M dataset.

<!-- asset-path: internal -->
<!-- module-type: audio-embedding -->
<!-- fine-tunable: false -->
<!-- format: saved_model_2 -->
<!-- network-architecture: vgg-style -->
<!-- dataset: YouTube-8M -->

## Overview

### Model description

VGGish takes audio waveform as input and produces 128-D embedding representations of the semantic content of the waveform. VGGish uses a variant of the VGG architecture and was trained using a preliminary version of the [YouTube-8M](http://research.google.com/youtube8m/) dataset. VGGish was originally released in the [TensorFlow Model Garden](https://github.com/tensorflow/models/tree/master/research/audioset/vggish), where we have the model source code, the original model checkpoint, and more detailed documentation.

### Inputs

The model accepts a 1-D Tensor or NumPy array containing a waveform of arbitrary length, represented as mono 16 kHz `float32` samples in the range `[-1.0, +1.0]`. Internally, we frame the waveform into sliding windows of 0.96 seconds with no overlap and then run the core of the model on a batch of these frames.

### Outputs

The model returns a 2-D `float32` Tensor of shape (N, 128) where N is the number of frames produced as described in the Inputs section above. Each row of the tensor is a 128-D embedding representation of the semantic content of the corresponding frame of audio.

### Usage

Here's how you would use the model in TensorFlow 2.

```python
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np

# Load the model.
model = hub.load('https://tfhub.dev/google/vggish/1')

# Input: 3 seconds of silence as mono 16 kHz waveform samples.
waveform = np.zeros(3 * 16000, dtype=np.float32)

# Run the model, check the output.
embeddings = model(waveform)
embeddings.shape.assert_is_compatible_with([None, 128])
```

You can also run the model in TensorFlow 1.

```python
import tensorflow.compat.v1 as tf
import tensorflow_hub as hub
import numpy as np

with tf.Graph().as_default():
    model = hub.load('https://tfhub.dev/google/vggish/1')
    waveform = np.zeros(3 * 16000, dtype=np.float32)
    embeddings = model(waveform)
    with tf.train.MonitoredSession() as sess:
      embeddings_np = sess.run(embeddings)
      print(embeddings_np.shape)  # (N, 128)
```

### Suitable uses

VGGish can be used

* as a high-level feature extractor: the 128-D embedding output of VGGish can be used as the input features of another shallow model which can then be trained on a small amount of data for a particular task. This allows quickly creating specialized audio classifiers without requiring a lot of labeled data and without having to train a large model end-to-end.
* as a warm start: the VGGish model parameters can be used to initialize part of a larger model which allows faster fine-tuning and model exploration.

### Limitations

VGGish has been trained on millions of YouTube videos and although these are very diverse, there can still be a domain mismatch between the average YouTube video and the audio inputs expected for any given task. You should expect to do some amount of fine-tuning and calibration to make VGGish usable in any system that you build.
