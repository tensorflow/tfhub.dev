# Module google/LaBSE/1

Language-agnostic BERT sentence embedding model supporting 109 languages.

<!-- asset-path: internal -->
<!-- module-type: text-embedding -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: multilingual -->
<!-- network-architecture: bert -->
<!-- dataset: commoncrawl -->
<!-- dataset: wikipedia -->
<!-- dataset: translation -->

## Overview

The language-agnostic BERT sentence embedding encodes text into high dimensional
vectors. The model is trained and optimized to produce similar representations
exclusively for bilingual sentence pairs that are translations of each other. So
it can be used for mining for translations of a sentence in a larger corpus.

### Metrics

*   We apply this model to the united nation parallel corpus retrieval task,
    where for each English sentence we try to find its true translation from a
    9.5 million sentence pool from the other language

    UN          | en-es | en-fr | en-ru | en-zh
    :---------- | ----: | ----: | ----: | ----:
    Precision@1 | 0.911 | 0.883 | 0.908 | 0.877

*   For tatoeba we evaluate on several language group.

    Tatoeba         | 36 languages from XTREME | All (112) languages
    :-------------- | -----------------------: | ------------------:
    Average Accuray | 0.950                    | 0.837

More details of the eval can be found in the paper [1].

To learn more about text embeddings, refer to the
[TensorFlow Embeddings](https://www.tensorflow.org/tutorials/text/word_embeddings)
documentation. Our encoder differs from word level embedding models in that we
train on a number of natural language prediction tasks that require modeling the
meaning of word sequences rather than just individual words. Details are
available in the paper "Language-agnostic BERT Sentence Embedding" [1].

## Extented Uses and Limitations

The produced embeddings can also be used for text classification, semantic
similarity, clustering and other natural language tasks. The performance may
depends on the domain / data match of a particular task. For general purpose
sentence embeddings, we refer to
[Universal Sentence Encoder family](https://tfhub.dev/google/collections/universal-sentence-encoder/1).

It is adopted from a pre-trained BERT model so it can also be used for any tasks
BERT can be applied. The performance, again, depends on the particular task.
Users may consider the
[BERT model family](https://tfhub.dev/google/collections/bert/1).

## Example Use

This model is based on the BERT model, so it can be called like a regular BERT
model as follows on tokenized text input, an input mask to hold out padding
tokens, and segment types should always set to 0.

```python
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub

def get_model(model_url, max_seq_length):
  labse_layer = hub.KerasLayer(model_url, trainable=True)

  # Define input.
  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                         name="input_word_ids")
  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                     name="input_mask")
  segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                      name="segment_ids")

  # LaBSE layer.
  pooled_output,  _ = labse_layer([input_word_ids, input_mask, segment_ids])

  # The embedding is l2 normalized.
  pooled_output = tf.keras.layers.Lambda(
      lambda x: tf.nn.l2_normalize(x, axis=1))(pooled_output)

  # Define model.
  return tf.keras.Model(
        inputs=[input_word_ids, input_mask, segment_ids],
        outputs=pooled_output), labse_layer

max_seq_length = 64
labse_model, labse_layer = get_model(
    model_url="https://tfhub.dev/google/LaBSE/1", max_seq_length=max_seq_length)
...
```

The model generates a l2 normalized pooled_output of shape [batch_size, 768]
with representations for the entire input sequences.

The tokenization of input text can be performed in Python with the FullTokenizer
class from tensorflow/models/official/nlp/bert/tokenization.py. Its vocab_file
is stored as atf.saved_model.Asset and the do_lower_case flag is stored as a
tf.Variable object on the SavedModel. A complete example is illustrated as
follows:

```python
# !pip install bert-for-tf2

import bert

vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()
tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)

def create_input(input_strings, tokenizer, max_seq_length):

  input_ids_all, input_mask_all, segment_ids_all = [], [], []
  for input_string in input_strings:
    # Tokenize input.
    input_tokens = ["[CLS]"] + tokenizer.tokenize(input_string) + ["[SEP]"]
    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)
    sequence_length = min(len(input_ids), max_seq_length)

    # Padding or truncation.
    if len(input_ids) >= max_seq_length:
      input_ids = input_ids[:max_seq_length]
    else:
      input_ids = input_ids + [0] * (max_seq_length - len(input_ids))

    input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)

    input_ids_all.append(input_ids)
    input_mask_all.append(input_mask)
    segment_ids_all.append([0] * max_seq_length)

  return np.array(input_ids_all), np.array(input_mask_all), np.array(segment_ids_all)

def encode(input_text):
  input_ids, input_mask, segment_ids = create_input(
    input_text, tokenizer, max_seq_length)
  return labse_model([input_ids, input_mask, segment_ids])

english_sentences = ["dog", "Puppies are nice.", "I enjoy taking long walks along the beach with my dog."]
italian_sentences = ["cane", "I cuccioli sono carini.", "Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane."]
japanese_sentences = ["犬", "子犬はいいです", "私は犬と一緒にビーチを散歩するのが好きです"]

english_embeddings = encode(english_sentences)
italian_embeddings = encode(italian_sentences)
japanese_embeddings = encode(japanese_sentences)

# English-Italian similarity
print (np.matmul(english_embeddings, np.transpose(italian_embeddings)))

# English-Japanese similarity
print (np.matmul(english_embeddings, np.transpose(japanese_embeddings)))

# Italian-Japanese similarity
print (np.matmul(italian_embeddings, np.transpose(japanese_embeddings)))
```

## References

[1] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Narveen Ari, Wei Wang.
[Language-agnostic BERT Sentence Embedding](https://arxiv.org/abs/2007.01852).
July 2020
