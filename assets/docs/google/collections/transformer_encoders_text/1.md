# Collection google/transformer_encoders_text/1

Transformer Encoders for Text

<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->

## Overview

TF Hub offers a variety of models that use the
[Transformer](https://arxiv.org/abs/1706.03762) architecture,
more specifically its encoder part, to compute dense vector representations
for natural language.

## Catalog


### Transformer Encoders with Preprocessing

The following SavedModels implement the encoder API for [text embeddings with
transformer encoders](https://www.tensorflow.org/hub/common_saved_model_apis/text#transformer-encoders)
and come with a preprocessor model (referenced in the model documentation)
that transforms plain text into their respective input formats.

#### [ALBERT](https://tfhub.dev/google/collections/albert/1)

  * For English: sizes
    [base](https://tfhub.dev/tensorflow/albert_en_base),
    [large](https://tfhub.dev/tensorflow/albert_en_large),
    [xlarge](https://tfhub.dev/tensorflow/albert_en_xlarge),
    [xxlarge](https://tfhub.dev/tensorflow/albert_en_xxlarge).

#### [BERT](https://tfhub.dev/google/collections/bert/1)

  * For English (uncased): sizes
    [base](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12),
    [large](https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16),
    [large (wwm)](https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16).
  * For English (cased): sizes
    [base](https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12),
    [large](https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16),
    [large (wwm)](https://tfhub.dev/tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16).
  * For Chinese: size
    [base](https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12).
  * Multilingual (cased): size
    [base](https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12).

#### [BERT Experts](https://tfhub.dev/google/collections/experts/bert/1)

BERT-base models for English, pre-trained on different corpora,
and optionally also fine-tuned on different downstream tasks.

  * Wikipedia+BooksCorpus:
    [wiki_books](https://tfhub.dev/google/experts/bert/wiki_books),
    also fine-tuned:
    [mnli](https://tfhub.dev/google/experts/bert/wiki_books/mnli),
    [qnli](https://tfhub.dev/google/experts/bert/wiki_books/qnli),
    [qqp](https://tfhub.dev/google/experts/bert/wiki_books/qqp),
    [sst2](https://tfhub.dev/google/experts/bert/wiki_books/sst2),
    [squad2](https://tfhub.dev/google/experts/bert/wiki_books/squad2).
  * MEDLINE/PubMed:
    [pubmed](https://tfhub.dev/google/experts/bert/pubmed),
    also fine-tuned:
    [squad2](https://tfhub.dev/google/experts/bert/pubmed/squad2).

#### [ELECTRA](https://tfhub.dev/google/collections/electra/1)

  * For English: sizes
    [small](https://tfhub.dev/google/electra_small),
    [base](https://tfhub.dev/google/electra_base),
    [large](https://tfhub.dev/google/electra_large).

#### Lambert

  * For English: size
    [large](https://tfhub.dev/tensorflow/lambert_en_uncased_L-24_H-1024_A-16/1)

#### [SmallBERT](https://tfhub.dev/google/collections/bert/1)

BERT models for English, with a smaller number L of layers combined with
a smaller hidden size H (and number of attention heads).


|          |H=128|H=256|H=512|H=768|
|----------|:---:|:---:|:---:|:---:|
| **L=2**  | [2/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2)  | [2/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4)  | [2/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8)   | [2/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12)   |
| **L=4**  | [4/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2)  | [4/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4)  | [4/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8)   | [4/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12)   |
| **L=6**  | [6/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2)  | [6/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4)  | [6/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8)   | [6/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12)   |
| **L=8**  | [8/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2)  | [8/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4)  | [8/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8)   | [8/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12)   |
| **L=10** | [10/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2)| [10/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4)| [10/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8) | [10/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12) |
| **L=12** | [12/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2)| [12/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4)| [12/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8) | [12/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12) |

#### Talking Heads & Gated GELU

  * For English: sizes
    [base](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base),
    [large](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large).
