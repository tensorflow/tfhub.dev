# Module google/&zwnj;electra_small/2
ELECTRA-Small++: A BERT-like model pre-trained as a discriminator.

<!-- fine-tunable: true -->
<!-- asset-path: legacy -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->

## Overview

ELECTRA is a BERT-like model that is pre-trained as a discriminator of a
generative adversarial network (GAN). It was originally published by:

  * Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning:
  [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/forum?id=r1xMH1BtvB),
  ICLR 2020.


This version packages the original ELECTRA-Small++ checkpoint made available by
[Google Research](https://github.com/google-research/electra) as a
[TF2 SavedModel](https://www.tensorflow.org/hub/tf2_saved_model) using the
implementation of BERT from the
[TensorFlow Models repository](https://github.com/tensorflow/models/tree/master/official/nlp/bert).

In order to make the checkpoint compatible, the dense layer (pooler) on top of
the CLS token (which is non-existent in the original pretraining) was
initialized with the identity matrix.

## Usage

```
!pip3 install --quiet tensorflow-text

import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text  # Imports TF ops for preprocessing.

# Define some sentences to feed into the model
sentences = [
  "Here We Go Then, You And I is a 1999 album by Norwegian pop artist Morten Abel. It was Abel's second CD as a solo artist.",
  "The album went straight to number one on the Norwegian album chart, and sold to double platinum.",
  "Ceylon spinach is a common name for several plants and may refer to: Basella alba Talinum fruticosum",
  "A solar eclipse occurs when the Moon passes between Earth and the Sun, thereby totally or partly obscuring the image of the Sun for a viewer on Earth.",
  "A partial solar eclipse occurs in the polar regions of the Earth when the center of the Moon's shadow misses the Earth.",
]

# Load the BERT encoder and preprocessing models
preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1')
bert = hub.load('https://tfhub.dev/google/electra_small/2')

# Convert the sentences to bert inputs
bert_inputs = preprocess(sentences)

# Feed the inputs to the model to get the pooled and sequence outputs
bert_outputs = bert(bert_inputs)
pooled_output = bert_outputs['pooled_output']
sequence_output = bert_outputs['sequence_output']

print('\nSentences:')
print(sentences)
print('\nPooled output:')
print(pooled_output)
print('\nSequence output:')
print(sequence_output)
```


## Changelog

### Version 2

  * Uses dicts (not lists) for inputs and outputs.
  * Works with a companion model for preprocessing of plain text.
  * For legacy users, this version still provides the now-obsolete `.vocab_file`
    and `.do_lower_case` attributes on `hub.KerasLayer.resolved_object`.

### Version 1

  * Initial release.
