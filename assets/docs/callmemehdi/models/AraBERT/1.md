# Module callmemehdi/AraBERT/1

AraBERT is an Arabic pretrained lanaguage model based on Google's BERT base architecture.

<!-- asset-path: https://gsoctfarabert.web.app/arabert.tar.gz -->
<!-- task: text-embedding -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- license: custom -->

## Overview

The original AraBERT base v2 repository is available in this [link](https://huggingface.co/aubmindlab/bert-base-arabertv2)

These results are obtained from pre-training BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language.The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches

## Performance

Performance of AraBERT on Arabic downstreamtasks compared to mBERT
Results:

 Task | metric | mBERT | AraBERT
------|--------|------ | -------
SA (HARD)        | Acc.   | 95.7  | 96.1
SA (ASTD)        | Acc.   | 80.1  | 96.5
SA (ArsenTD-Lev) | Acc.   | 51.0  | 59.4
SA (AJGT)        | Acc.   | 83.6  | 93.8
SA (LABR)        | Acc.   | 83.0  | 86.7
NER (ANERcorp) | macro-F1   | 78.4  | 81.9
QA (ARCD) | macro-F1   | 61.3  | 62.7


## Usage

In order to use this model, you need to use the arabert repository to preprocess your input.
First, clone their repository using this command:
```shell
git clone https://github.com/aub-mind/arabert.git
```
Then, use it to preprocess your input, here's an example:

```python
from arabert.preprocess import ArabertPreprocessor

model_name="bert-base-arabertv2"
arabert_prep = ArabertPreprocessor(model_name=model_name)

text = "كان الملك رجلا والملكة مرأة"
arabert_prep.preprocess(text)
```
Note that you would need to install some external libraries:
```shell
pip install pyarabic
pip install farasapy
```
After preprocessing the input, we can start tokenizing it:

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("aubmindlab/bert-base-arabertv2")

tokenized_text = tokenizer.tokenize(preprocessed_text)

input = tokenizer.convert_tokens_to_ids(tokenized_text)
```
After that, you have your input ready, so you just have to pass it through the model

```python
import tensorflow as tf
import tensorflow_hub as hub

model = hub.load('https://tfhub.dev/callmemehdi/AraBERT/1')
outputs = model(tf.convert_to_tensor([input]))

```
The model outputs has 3 main components:

  1- Last hidden output of the model: A Component of the shape (1, sequence_length, 768), type: tensorflow.python.framework.ops.EagerTensor

  2- Pooler output: Output of a layer that transforms the output shape of the Transformer from [batch_size, sequence_length, hidden_size] to [batch_size, hidden_size], which is similar to GlobalMaxPool1D. In our case the shape is (1, 768), type: tensorflow.python.framework.ops.EagerTensor

  3- Hidden states: Tuple of tf.Tensor (one for the output of the embeddings + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).

With sequence_length the number of words in the input text



## Reference

```
@inproceedings{antoun2020arabert,
  title={AraBERT: Transformer-based Model for Arabic Language Understanding},
  author={Antoun, Wissam and Baly, Fady and Hajj, Hazem},
  booktitle={LREC 2020 Workshop Language Resources and Evaluation Conference 11--16 May 2020},
  pages={9}
}
```

## License

[SOFTWARE LICENSE AGREEMENT - AraBERT](https://github.com/aub-mind/arabert/blob/master/arabert/LICENSE)
