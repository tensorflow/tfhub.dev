# Module google/planet/vision/classifier/planet_v2/1

PlaNet predicts the rough geolocation of arbitrary photos.

<!-- module-type: image-classification -->
<!-- asset-path: legacy -->
<!-- language: en -->
<!-- fine-tunable: false -->
<!-- format: hub -->
<!-- interactive-model-name: planet -->

## Overview

### Input

This model takes input of images.

*   Inputs are expected to be 3-channel RGB color images of size 299 x 299.

### Output

This model outputs to `predictions`.

*   **`predictions`**: Confidence distribution over locations on the globe.
    Classes correspond to S2 Cells (see http://s2geometry.io/).
    [Labelmap](https://www.gstatic.com/aihub/tfhub/labelmaps/planet_v2_labelmap.csv).

## Usage

### Use SavedModel in Python

The model can be loaded in a Python script as follows:

```python
module = hub.Module("https://tfhub.dev/google/planet/vision/classifier/planet_v2/1")
height, width = hub.get_expected_image_size(module)
images = ...  # A batch of images with shape [batch_size, height, width, 3].
features = module(images)  # Features with shape [batch_size, num_outputs].
```

The input `images` are expected to have color values in the range [0,1],
following the
[common image input](https://www.tensorflow.org/hub/common_signatures/images#input)
conventions. The input image size is 299x299 pixels.

Fine-tuning is not currently supported.

## Performance

### Offline evaluation

Offline evaluation was performed on the following datasets:

On
[im2GPS Test Set](http://graphics.cs.cmu.edu/projects/im2gps/gps_query_imgs.zip),
model performance is as follows:

*   **`Recall @ Street Level (1km)`**: 0.10970000177621841
*   **`Recall @ City Level (25km)`**: 0.3165000081062317
*   **`Recall @ Region Level (200km)`**: 0.5105000138282776
*   **`Recall @ Country Level (750km)`**: 0.6498000025749207
*   **`Recall @ Continent Level (2500km)`**: 0.7764000296592712

### Metrics glossary

For more details on the metrics used above, please refer to:

*   [`Recall`](https://developers.google.com/machine-learning/glossary/#recall)

## Training

### Model architecture

This model is based on the inception_v3 architecture.

Inception V3 is a neural network architecture for image classification,
originally published by

*   Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,
    Zbigniew Wojna:
    ["Rethinking the Inception Architecture for Computer Vision"](https://arxiv.org/abs/1512.00567), 2015.

This TF-Hub module uses the TF-Slim implementation of `Inception V3` with global
pooling to allow oversized inputs.

### Additional information

Supports interactive model visualization.

## Suitable Use, Limitations, and Fair Use Terms

### Suitable usecases

This model is **suitable** for:

*   Coarse-grained image localization.

### Unsuitable usecases

This model is **unsuitable** for:

*   Use in forensics applications.
*   Tracking people.

### Limitations

*   The model may not be able to localize all kinds of images, e.g. indoor
    photos, portraits, food.

### License

This model follows [*Apache 2.0*](https://www.apache.org/licenses/LICENSE-2.0).
If you intend to use it beyond permissible usage, please consult with the model
owners ahead of time.

### Citation

When used for publication or production, please cite this model as follows:

```
PlaNet - Photo Geolocation with Convolutional Neural Networks
Tobias Weyand, Ilya Kostrikov, James Philbin
European Conference on Computer Vision (ECCV) (2016)
```
