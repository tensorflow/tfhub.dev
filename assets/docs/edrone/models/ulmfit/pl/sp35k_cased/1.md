# Module edrone/ulmfit/pl/sp35k_cased/1
ULMFiT language model (Polish, 35k tokens, cased) trained on the Wikipedia corpus.

<!-- asset-path: https://d3vhsxl1pwzf0p.cloudfront.net/ava/ulmfit/plwiki100_20epochs_toks_35k_cased/saved_model/plwiki100_20epochs_toks_35k_cased.tar.gz -->
<!-- task: text-embedding -->
<!-- fine-tunable: true -->
<!-- dataset: wikipedia -->
<!-- format: saved_model_2 -->
<!-- network-architecture: ulmfit -->
<!-- language: pl -->
<!-- license: apache-2.0 -->



## Overview and model architecture

This module provides pretrained weights for the [ULMFiT language model](https://arxiv.org/abs/1801.06146) encoder. The architecture is a 3-layer unidirectional LSTM network with several regularization techniques. It was trained using [FastAI](https://fast.ai) framework and its weights were then exported to a Tensoflow SavedModel. We verified the TF outputs to be numerically compatible at inference with outputs from FastAI.

| ![](http://d3vhsxl1pwzf0p.cloudfront.net/ava/ulmfit_paper.png) |
| :----------------------------------------------------------: |
| ULMFiT architecture (from the original paper) - this TFHub module provides the encoder (a) trained on Wikipedia |

Some of the regularization techniques used in the ULMFiT paper (including AWD) are baked into the serialized model. Others (e.g. slanted triangular learning rates) have to be applied as Keras callbacks using [code available in our repo](#more-details--code-repository).

<div style="boder: 2px solid #990066; padding: 10px; outline: #990066 solid 5px; outline-offset: 5px">
    <b><span style="color:blue">RaggedTensors affect performance</span> - this module uses RaggedTensors. While they offer a lot of convenience, unfortunately TF cannot use them together with Nvidia's CuDNN kernel for LSTMs. This means that training with weights obtained from TFHub could be rather slow even on a GPU. We recommend using the `hub.load` interface described below only for experiments on small datasets. For larger data please consider using the code in our repo to build a fixed-length encoder with padding and restore weights from a checkpoint (see "More details").</b>
</div>



## Quick start

```
import tensorflow as tf
import tensorflow_text as text
import tensorflow_hub as hub

ulmfit = hub.load('https://tfhub.dev/edrone/ulmfit/pl/sp35k_cased/1')
sents = tf.constant(['Wszyscy ludzie rodzą się wolni i równi pod względem swej godności i swych praw.',
                     'Są oni obdarzeni rozumem i sumieniem i powinni postępować wobec '\
                     'innych w duchu braterstwa.'], dtype=tf.string)
encoder = hub.KerasLayer(ulmfit.signatures['string_encoder'], trainable=True)
encoder_vectors = encoder(sents)
print(encoder_vectors)
```



## Module contents

The module consists of **three signatures** which can be wrapped around a hub.KerasLayer object and used just as any other Keras layer. They are:

* **string_encoder** - accepts strings and outputs hidden states of the last RNN layer. Because the number of tokens in each string is not constant, this signature will output a variable number of tensors for each sequence. The typical structure to handle this is a [tf.RaggedTensor](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor), but we encountered difficulties when attempting to serialize models that use this object directly. For this reason the hidden states are returned as a dictionary of output_flat and output_rows - a sort of an "unfolded" or "flattened" representation of a variable-length tensor. You can go back to a RaggedTensor by calling the `from_row_splits` method. Here is an example:

  ```
  input_layer = tf.keras.layers.Input(shape=(), dtype=tf.string)
  enc_out = encoder(input_layer) # the output is a dict of tensors with keys 'output_rows' and 'output_flat'
  enc_out_ragged = tf.RaggedTensor.from_row_splits(enc_out['output_flat'],
                                                   enc_out['output_rows'])
  ```

  This signature performs tokenization and numericalization automatically. You also don't need to worry about things like loading a dictionary, padding or sequence length.
  
* **numericalized_encoder** - if your text is already tokenized and numericalized (converted to token IDs), you can use this signature to pass integer arrays directly. Please note that you must encode your text using the same Sentencepiece vocabulary as is used in this module.

  This signature requires the input ragged tensor to be "unfolded" into flat_values and row_splits. However, as of TF 2.4.1 ***you are not able to do something like this***:

  ```
  input_layer = tf.keras.layers.Input(shape=(None,), ragged=True, dtype=tf.int32)
  encoder = hub.KerasLayer(ulmfit.signatures['numericalized_encoder'](flatvals=input_layer.flat_values, 
                                                                      rowspl=input_layer.row_splits), trainable=True)
  ```

  The reason is that KerasTensor is not compatible with Tensor. There is a workaround for this (HubRaggedWrapper) - see the guide in our repo if you need more details.

  

* **spm_processor** - this signature accepts strings, tokenizes them with Sentencepiece and converts to token IDs. Again, since RaggedTensor outputs are problematic during serialization to a SavedModel, we output a dictionary with two keys (`numericalized_rows` and `numericalized_flat`) which you can then convert to a RaggedTensor manually:

  ```
  numericalized_dict = ulmfit.signatures['spm_processor'](tf.constant(['Dzień dobry.', 'To jest przykład.']))
  input_ragged = tf.RaggedTensor.from_row_splits(numericalized_dict['numericalized_flat'],
                                                 numericalized_dict['numericalized_rows'])
  ```

  

  If you have your own Sentencepiece-related code and you would rather use the SPM model file, check the TF Hub cache directory where this module was downloaded. There should be an `assets` subdirectory with a file called `plwiki100-cased-sp35k.model`. (alternatively this magic spell should locate the .model file for you: `getattr(ulmfit.spm_encoder_model, 'layer-1').spm_asset.asset_path`).

  One important thing to remember about vocabulary in our pretrained models (especially if you are planning to convert them to fixed-length representations) is that the `<pad>` token is found under index 1. The index 0 is reserved for `<unk>`s. See the guide in our repo for more details on this.



### How to use with Keras

Here is an example of how to build a very simple document classifier that uses vectors produced by the RNN encoder. The snippet below does not replicate the approach to text classification described in the ULMFiT paper but is just meant to illustrate how to use this module in practice:

```
input_layer = tf.keras.layers.Input(shape=(), dtype=tf.string)
enc_out = encoder(input_layer) # the output is a dict of tensors with keys 'output_rows' and 'output_flat'
enc_out_ragged = tf.RaggedTensor.from_row_splits(enc_out['output_flat'],   # converting to RaggedTensor
                                                 enc_out['output_rows'])
vector_average = tf.reduce_mean(enc_out_ragged, axis=1) # rather than use the last hidden state, we take the average
                                                        # value of each dimension in a sequence
dense = tf.keras.layers.Dense(3)(vector_average)   # classifier head
model = tf.keras.models.Model(inputs=encoder.input, outputs=dense)
model.summary()

Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None,)]            0                                            
__________________________________________________________________________________________________
keras_layer (KerasLayer)        {'output_rows': (Non 34262464    input_3[0][0]                    
__________________________________________________________________________________________________
tf.RaggedTensor.from_row_splits (None, None, 400)    0           keras_layer[0][0]                
                                                                 keras_layer[0][1]                
__________________________________________________________________________________________________
tf.math.reduce_mean_1 (TFOpLamb (None, 400)          0           tf.RaggedTensor.from_row_splits_1
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 3)            1203        tf.math.reduce_mean_1[0][0]      
==================================================================================================
Total params: 34,263,667
Trainable params: 34,263,667
Non-trainable params: 0
__________________________________________________________________________________________________

```

The second layer (KerasLayer) is the encoder. If you now compile and train this model, this layer will apply several dropout types for regularization. We have followed the ULMFiT paper and [this guide to it](https://blog.mlreview.com/understanding-building-blocks-of-ulmfit-818d3775325b) and baked the following dropout types into the SavedModel:

* encoder dropout (applied to subword embeddings)
* input dropout (randomly dropping the entire dimension for all tokens in a sequence)
* hidden and output dropout (applied to vectors produced by LSTM layers)

Crucially, **weight dropout (AWD) is not applied automatically** during training. To apply AWD, you should call the `apply_awd` function on the restored `ulmfit` object before each batch. The easiest way with Keras is to use a custom training loop:

```
def train_step(*, model, ulmfit, loss_fn, optimizer, x, y):
    ulmfit.apply_awd(0.5)
    with tf.GradientTape() as tape:
        y_preds = model(x, training=True)
        loss_value = loss_fn(y_true=y, y_pred=y_preds)
    grads = tape.gradient(loss_value, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
```

Alternatively, to use AWD with `model.fit` in Keras you can define a custom callback:

```
class AWDCallback(tf.keras.callbacks.Callback):
    def __init__(self, *, ulmfit, awd_rate=0.5):
        super().__init__()
        self.ulmfit = ulmfit

    def on_train_batch_begin(self, batch, logs=None):
    	self.ulmfit.apply_awd(self.awd_rate)
...
model.fit(x, y, callbacks=[AWDCallback(ulmfit)])
```

For simple tasks you may want to skip AWD altogether - your model will be slightly less well regularized, but should still have quite decent performance.



### **Learning rates**

Slanted Triangular Learning Rates or the One-Cycle scheduler are typically used to fine-tune ULMFiT. Since these are not part of Tensorflow and are not easily serializable to a SavedModel, you need to create a scheduler object from Python code. You will find the `SLTRSchedule` and `OneCycleScheduler`in [`tf2_ulmfit.py`](https://bitbucket.org/edroneteam/tf2_ulmfit/src/master/ulmfit_tf2.py) and there is an example how to use both in [`examples/ulmfit_tf_text_classifier.py`](https://bitbucket.org/edroneteam/tf2_ulmfit/src/master/examples/ulmfit_tf_text_classifier.py). The example file also allows you to run a mock training for a specified number of steps to determine the optimal peak rate for STLR / 1cycle.

![](http://d3vhsxl1pwzf0p.cloudfront.net/ava/lrfinder.png)

Both the OneCycleScheduler and LRFinder are implementations by Andrich van Wyk (see [here](https://www.kaggle.com/avanwyk/tf2-super-convergence-with-the-1cycle-policy)).



## Differences with respect to FastAI training method

As said earlier, our models were trained using [FastAI](https://fast.ai) framework and the module you are downloading from TF Hub contains converted weights. In addition, we have decided to introduce certain modifications with respect to the original training setup.



| Our models                                                   | Off-the-shelf ULMFiT model from FastAI                       |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Trained on running text, but the Wikitext corpus was sentence-tokenized. BOS and EOS markers surround each sentence.<br /><br />**This means that the encoder works on sentences, not documents. ** The default `string_encoder` signature will add &lt;s&gt; and &lt;/s&gt; tokens automatically at the beginning and end of each sequence. If a sequence contains multiple sentences and you want to surround each of them with BOS/EOS markers, these sentences should have a `[SEP]` string between them. For example:<br /><br />`multisent_example = "Mój poduszkowiec jest pełen węgorzy.[SEP]Nie kupię tej płyty, jest porysowana."` | Trained on running text without sentence tokenization. BOS and EOS markers surround each document. |
| Minimal preprocessing, no special tokens, no pre/post tokenization rules | Special tokens and associated rules. For example, words in capital letters such as `London` are downcased, but a marker token is inserted before: `\\xxmaj london`. Repetitions are also handled via special token `\\xxrep`. |
| Subword tokenization. We used a Sentencepiece model built on the Wikitext training corpus using `spm_train`. Tokenization and numericalization was also performed outside FastAI. | No subword tokenization. Token dictionary built by the framework as part of the training process. |



## More details + code repository

The original ULMFiT paper focuses on document classification and its FastAI implementation is also tightly coupled with this particular task. However, just like any other recurrent model, the ULMFiT encoder can be used as a basis for sequence taggers, autoencoders and more.

We are publishing [a bitbucket repo](https://bitbucket.org/edroneteam/tf2_ulmfit/) which contains code used to create our models. It also contains some examples of how you can use the encoder with custom heads. Additionally, you will find preprocessing and training scripts which use FastAI together with a conversion script. The tools in the repo will also allow you to build fixed-length models (with padding and LSTM computations handled by a CuDNN kernel) rather than ones which use RaggedTensors. Please see [this guide](https://bitbucket.org/edroneteam/tf2_ulmfit/src/master/README.md) for an in-depth description.

