# Collection sayakpaul/vision_transformer/1

Collection of new Vision Transformer models fine-tuned on ImageNet-1k.

<!-- dataset: imagenet-ilsvrc-2012-cls -->
<!-- task: image-classification -->

## Overview

This collection contains eight different Vision Transformer [1] models that were fine-tuned 
on the ImageNet-1k dataset [2]. For more details on the training protocols, please follow [3].
The authors of [3] open-sourced about 50k different variants of Vision Transformer models in JAX. 
This collection contains seven of the best ImageNet-1k models from that pool. 

The models contained in this collection were converted from the original model classes and
weights [4] using the `jax2tf` tool [5]. For more details on the conversion process, please
follow [this notebook](https://colab.research.google.com/github/sayakpaul/ViT-jax2tf/blob/main/conversion.ipynb).
**Using this notebook, one should be able to take a model (that is not already a part of
this collection) from [4] and convert that to a TensorFlow SavedModel.**

The criteria that were used to select the models included in this collection can be found
in [this notebook](https://colab.research.google.com/github/sayakpaul/ViT-jax2tf/blob/main/model-selector.ipynb). 

## About the models

Models included in this collection have two variants: (1) off-the-shelf inference for image
classification, (2) fine-tuning on downstream tasks. These models are accompanied by
Colab Notebooks for demonstration purposes. 

A huge thanks to the authors of [3] for their open-sourcing efforts and for making the models
as reproducible as possible.

The table below provides a performance summary:

| **Model** | **Top-1 Accuracy** | **Checkpoint** | 
|:---:|:---:|:---:|
| B/8 | 85.948 | B_8-i21k-300ep-lr_0.001-aug_medium2-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224.npz |  
| L/16 | 85.716 | L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224.npz |  
| B/16 | 84.018 | B_16-i21k-300ep-lr_0.001-aug_medium2-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz |  
| R50-L/32 | 83.784 | R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224.npz |  |
| R26-S/32 (light aug) | 80.944 | R26_S_32-i21k-300ep-lr_0.001-aug_light0-wd_0.03-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.03-res_224.npz | 
| R26-S/32 (medium aug) | 80.462 | R26_S_32-i21k-300ep-lr_0.001-aug_medium2-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224.npz |  
| S/16 | 80.462 | S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz | 
| B/32 | 79.436 | B_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz |  

Note that the top-1 accuracy is reported on the ImageNet-1k validation set. **Checkpoint** refers to the `filename`
column of the dataframe shown in [this notebook](https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax_augreg.ipynb). 
The checkpoints are present in the following GCS  location: `gs://vit_models/augreg`. More details on these can be
found in [4].

The reported scores come from the dataframe shown in [this official Vision Transformer notebook](https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax_augreg.ipynb).

Additionally, the author of this collection independently verified two models:

| **Model** | **Top-1 Accuracy** | **tb.dev link** |
|:---:|:---:|:---:|
| [R26-S/32 (light aug)](https://tfhub.dev/sayakpaul/vit_r26_s32_lightaug_classification/1) | 80.96% | [Link](https://tensorboard.dev/experiment/8rjW26CoRJWdAR3ejtgvHQ/) |
| [S/16](https://tfhub.dev/sayakpaul/vit_s16_classification/1) | 80.44% | [Link](https://tensorboard.dev/experiment/52LkVYfnQDykgyDHmWjzBA/) |

The code to produce these numbers is available [here](https://github.com/sayakpaul/ViT-jax2tf/tree/main/i1k_eval). 

### Image classifiers

* [ViT-S16](https://tfhub.dev/sayakpaul/vit_s16_classification/1)
* [ViT-B8](https://tfhub.dev/sayakpaul/vit_b8_classification/1)
* [ViT-B16](https://tfhub.dev/sayakpaul/vit_b16_classification/1)
* [ViT-B32](https://tfhub.dev/sayakpaul/vit_b32_classification/1)
* [ViT-L16](https://tfhub.dev/sayakpaul/vit_l16_classification/1)
* [ViT-R26-S32 (light augmentation)](https://tfhub.dev/sayakpaul/vit_r26_s32_lightaug_classification/1)
* [ViT-R26-S32 (medium augmentation)](https://tfhub.dev/sayakpaul/vit_r26_s32_medaug_classification/1)
* [ViT-R50-L32](https://tfhub.dev/sayakpaul/vit_r50_l32_classification/1)

### Feature extractors

* [ViT-S16](https://tfhub.dev/sayakpaul/vit_s16_fe/1)
* [ViT-B8](https://tfhub.dev/sayakpaul/vit_b8_fe/1)
* [ViT-B16](https://tfhub.dev/sayakpaul/vit_b16_fe/1)
* [ViT-B32](https://tfhub.dev/sayakpaul/vit_b32_fe/1)
* [ViT-L16](https://tfhub.dev/sayakpaul/vit_l16_fe/1)
* [ViT-R26-S32 (light augmentation)](https://tfhub.dev/sayakpaul/vit_r26_s32_lightaug_fe/1)
* [ViT-R26-S32 (medium augmentation)](https://tfhub.dev/sayakpaul/vit_r26_s32_medaug_fe/1)
* [ViT-R50-L32](https://tfhub.dev/sayakpaul/vit_r50_l32_fe/1)

### Fine-tuning results

The table below shows fine-tuning results (top-1 accuracy) on two image classification
datasets: 

| **Dataset** | **S/16** | **R26_S32 <br>(medium aug)** |
|:---:|:---:|:---:|
| Flowers<br>([`tfds` link](https://www.tensorflow.org/datasets/catalog/tf_flowers)) | 98.64%<br> (held-out split)<br>([tb.dev run](https://tensorboard.dev/experiment/z2ixc3gtQCeEf9ONJmc2nw/)) | 98.09%<br>(held-out split)<br>([tb.dev run](https://tensorboard.dev/experiment/qjbF1nGOR62ldbfN9MCZcw/)) |
| Pets37<br>([`tfds` link](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet)) | 94.57%<br>(`test` split)<br>([tb.dev run](https://tensorboard.dev/experiment/jdfjGblCRGiV1o2rb9WuJA/)) | 90.57%<br>(`test` split)<br>([tb.dev run](https://tensorboard.dev/experiment/zb16c3NMSP6l4t6Y7b1DyA)) |

## References

[1] [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)

[2] [ImageNet-1k](https://www.image-net.org/challenges/LSVRC/2012/index.php)

[3] [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers by Steiner et al.](https://arxiv.org/abs/2106.10270)

[4] [Vision Transformer GitHub](https://github.com/google-research/vision_transformer)

[5] [jax2tf tool](https://github.com/google/jax/tree/main/jax/experimental/jax2tf/)