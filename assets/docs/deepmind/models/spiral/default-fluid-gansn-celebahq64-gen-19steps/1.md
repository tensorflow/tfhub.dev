# Module deepmind/spiral/default-fluid-gansn-celebahq64-gen-19steps/1

SPIRAL agent trained on the CelebA-HQ dataset using GAN objective in conjunction
with a spectrally normalized discriminator.

<!-- dataset: CelebA HQ -->
<!-- asset-path: legacy -->
<!-- module-type: image-rnn-agent -->
<!-- network-architecture: Other -->
<!-- fine-tunable: false -->
<!-- format: hub -->


## Overview

This module provides the policy network of the SPIRAL agent trained to sample
**19-strokes** drawings of human faces. The discriminator network is **not**
included in this release.

### Architecture

The architecture of the policy network is detailed in

*   Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami, Oriol
    Vinyals:
    ["Synthesizing programs for images using reinforced adversarial learning"](http://proceedings.mlr.press/v80/ganin18a.html),
    ICML 2018

This TF-Hub module uses the
[Sonnet implementation](https://github.com/deepmind/spiral/blob/master/spiral/agents/default.py)
of the network provided as a part of the
[`spiral`](https://github.com/deepmind/spiral) package for `python`.

### Training

The weights for this module were obtained by training on the
[CelebA-HQ](https://github.com/tkarras/progressive_growing_of_gans) dataset.
CelebA-HQ consists of `30,000` images at `1024 x 1024` resolution taken from the
[CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset. It was
originally described in

*   Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen: ["Progressive Growing
    of GANs for Improved Quality, Stability, and
    Variation"](https://research.nvidia.com/publication/2017-10_Progressive-Growing-of),
    ICLR 2018.

In order to be compatible with the architecture, images were downsampled
bilinearly to the resolution of `64 x 64`.

The distributed setup used to train the model consisted of `1` policy learner
(with *NVIDIA V100*), `1` discriminator learner (also with *NVIDIA V100*) and
`2560` actors (*CPU only*).

Each policy learner ran *Adam* optimizer with `epsilon = 1e-8`, `beta1 = 0.5`,
`beta1 = 0.999`, and a batch size of `64`. The initial learning rate was sampled
*log-uniformly* from the `[1e-5, 3e-4]` range. Similarly, the entropy cost was
sampled *log-uniformly* from `[2e-3, 1e-1]`.

The discriminator learner ran *Adam* optimizer with the same parameters except
the learning rate was set to a fixed value of `1e-4`.

The discriminator network was spectrally normalized as described in

*   Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida:
    ["Spectral Normalization for Generative Adversarial Networks"](https://openreview.net/pdf?id=B1QRgziT-),
    ICLR 2018.

## Usage

This module provides two methods: `initial_state` and `step`. The first returns
the initial state of the policy network, the second takes an observation from
the environment and produces an action. Both methods in their raw form are
difficult to deal with and therefore are not meant to be used directly. Instead,
it is suggested to load the module using the
[`spiral`](https://github.com/deepmind/spiral) (please refer to its
documentation for further details):

```python
import matplotlib.pyplot as plt
from scipy import ndimage

import spiral.agents.default as default_agent
import spiral.agents.utils as agent_utils
import spiral.environments.fluid as fluid


# The path to a TF-Hub module.
MODULE_PATH = "https://tfhub.dev/deepmind/spiral/default-fluid-gansn-celebahq64-gen-19steps/1"
# The folder containing the original Fluid Paint shaders.
SHADERS_PATH = "the/path/to/fluid_paint/shaders"

# Here, we create an environment.
env = fluid.FluidPaint(episode_length=20,
                       canvas_width=256,
                       grid_width=32,
                       brush_sizes=[2.5, 5.0, 10.0, 20.0, 40.0, 80.0],
                       shaders_basedir=SHADERS_PATH)

# Now we load the agent from a snapshot.
initial_state, step = agent_utils.get_module_wrappers(MODULE_PATH)

# Everything is ready for sampling.
state = initial_state()
noise_sample = np.random.normal(size=(10,)).astype(np.float32)

time_step = env.reset()
for t in range(19):
  time_step.observation["noise_sample"] = noise_sample
  # The environment uses 256x256 canvas but the agent requires 64x64 input.
  ratio = 64 / 256
  time_step.observation["canvas"] = ndimage.zoom(
      time_step.observation["canvas"], [ratio, ratio, 1], order=1)
  action, state = step(time_step.step_type, time_step.observation, state)
  time_step = env.step(action)

# Show the sample.
plt.close("all")
plt.imshow(time_step.observation["canvas"], interpolation="nearest")
```

The code above will produce a single sample from the model.

## Changelog

#### Version 1

*   Initial release.
