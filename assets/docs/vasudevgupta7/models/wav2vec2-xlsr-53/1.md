# Module vasudevgupta7/wav2vec2-xlsr-53/1

Pre-trained speech model (without any head) from Facebook for Automatic Speech Recognition

<!-- asset-path: https://storage.googleapis.com/gsoc-weights/wav2vec2_xlsr_53.tar.gz -->
<!-- task: audio-stt -->
<!-- network-architecture: wav2vec2 -->
<!-- format: saved_model_2 -->
<!-- fine-tunable: true -->
<!-- license: apache-2.0 -->
<!-- language: en -->
<!-- colab: https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/wav2vec2_saved_model_finetuning.ipynb -->


## Overview

This model is TensorFlow equivalent of PyTorch [`facebook/wav2vec2-large-xlsr-53`](https://huggingface.co/facebook/wav2vec2-large-xlsr-53). It was published in [1].

**How to use this model?**

Add randomly initalized LM head over the top of pre-trained model & fine-tune the whole model.

```python
import tensorflow as tf
import tensorflow_hub as hub

# For using this pre-trained model for training, pass `trainable=True` in `hub.KerasLayer`
pretrained_layer = hub.KerasLayer("https://tfhub.dev/vasudevgupta7/wav2vec2-xlsr-53/1", trainable=True)

VOCAB_SIZE = 32
# model signature expects sequences of constant length of 246000
WAV_DATA_POINTS = 246000

# Let's wrap all the layers into `tf.keras.Model` using TensorFlow's Functional API
speech = tf.keras.Input(shape=(WAV_DATA_POINTS,))
attention_mask = tf.keras.Input(shape=(WAV_DATA_POINTS,))
hidden_states = pretrained_layer((speech, attention_mask))
outputs = tf.keras.layers.Dense(VOCAB_SIZE)(hidden_states)
model = tf.keras.Model(inputs=(speech, attention_mask), outputs=outputs)

# For using this model, it's important to set `jit_compile=True` on GPUs/CPUs
# as some operations in this model (i.e. group-convolutions) are unsupported without it
@tf.function(jit_compile=True)
def forward(speech, attention_mask):
    return model((speech, attention_mask), training=True)

# Now, this model can trained like any other TensorFlow model
```

Note: This model shouldn't be directly used for inference. Language Model (LM) head should be added on the top of this model & it should be fine-tuned for downstream tasks like speech -> text. Complete fine-tuning workflow is shown in the accompanying notebook.

References
--------------
[1] [Unsupervised Cross-lingual Representation Learning for Speech Recognition](https://arxiv.org/abs/2006.13979).
