# Collection google/experts/bert/1

Collection of BERT experts fine-tuned on different datasets.

<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- dataset: Wikipedia and BooksCorpus -->
<!-- dataset: MNLI -->
<!-- dataset: QNLI -->
<!-- dataset: QQP -->
<!-- dataset: SST-2 -->
<!-- dataset: MEDLINE/PubMed -->
<!-- dataset: SQuAD 2.0 -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->
<!-- language: en -->

## Overview

Starting from a pre-trained BERT model and fine-tuning on the downstream task
gives impressive results on many NLP tasks. One can further increase the
performance by starting from a BERT model that better aligns or transfers to the
task at hand, particularly when having a low number of downstream examples. For
example, one can use a BERT model that was trained on text from a similar domain
or by use a BERT model that was trained for a similar task.

This is a collection of such BERT "expert" models that were trained on a
diversity of datasets and tasks to improve performance on downstream tasks like
question answering, tasks that require natural language inference skills, NLP
tasks in the medical text domain, and more.

## Evaluation

To help navigate this collection of models we provide results of fine-tuning
each model on a set of downstream tasks. The models are trained 5 times on each
downstream dataset and the metrics are reported here with the median value
highlighted. A small hyperparameter search was conducted over learning rate
(3e-5, 1e-4), number of epochs (6, 12) and batch size (32). The hyperparemer
setting with the highest median was selected to be reported.

![Metrics plot comparing BERT experts on downstream tasks](https://www.gstatic.com/aihub/tfhub/experts/bert/metrics_v0.png)

The tasks that the models are evaluated on are included in the table below.

Task                    | Description
----------------------- | -----------
CORD-19 Reference Title | The CORD-19 Reference Title task is a task derived from the CORD-19 dataset[4] which contains scholarly articles about COVID-19. This task is a sentence pair classification task where the first sentence is a sentence from the article containing a reference (e.g., "Roses are red [1]."). The second sentence is a title from the article references (e.g., "A Survey of Roses"). The label is 1 if the sentence and title match or 0 if the title was randomly selected (from another reference in that article).
CoLA                    | The Corpus of Linguistic Acceptability (CoLA)[5] consists of English acceptability judgments drawn from books and journal articles on linguistic theory. Each example is a sequence of words annotated with whether it is a grammatical English sentence.
MRPC                    | The Microsoft Research Paraphrase Corpus (MRPC)[6] is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.

## Wikipedia and BooksCorpus Experts

These models were pre-trained on the English Wikpedia[1] & BooksCourpus[2]
datasets. The models are intended to be used for a **variety of English NLP
tasks**.

Model                                                                        | Description
---------------------------------------------------------------------------- | -----------
[wiki_books](https://tfhub.dev/google/experts/bert/wiki_books)               | The model was trained in a self-supervised manner on the **Wikipedia + BooksCorpus** datasets.
[wiki_books/mnli](https://tfhub.dev/google/experts/bert/wiki_books/mnli)     | This model was initialized from the base Wikipedia + BooksCorpus BERT model and was fine-tuned on MNLI, a dataset for **multi-genre natural language inference**.
[wiki_books/qnli](https://tfhub.dev/google/experts/bert/wiki_books/qnli)     | This model was initialized from the base Wikipedia + BooksCorpus BERT model and was fine-tuned on QNLI, a dataset for **question-based natural language inference**.
[wiki_books/qqp](https://tfhub.dev/google/experts/bert/wiki_books/qqp)       | This model was initialized from the base Wikipedia + BooksCorpus BERT model and was fine-tuned on the Quora Question Pairs dataset (QQP), a dataset for the **semantic similarity of question pairs**.
[wiki_books/sst2](https://tfhub.dev/google/experts/bert/wiki_books/sst2)     | This model was initialized from the base Wikipedia + BooksCorpus BERT model and was fine-tuned on the Stanford Sentiment Treebank (SST-2), a dataset for **sentiment analysis**.
[wiki_books/squad2](https://tfhub.dev/google/experts/bert/wiki_books/squad2) | This model was initialized from the base Wikipedia + BooksCorpus BERT model and was fine-tuned on the SQuAD 2.0, a dataset for **question-answering**.

## PubMed Experts

These models were pre-trained on the MEDLINE/PubMed[3] corpus of biomedical and
life sciences literature abstracts. The models are intended to be used on
**medical or scientific text NLP tasks**.

Model                                                                | Description
-------------------------------------------------------------------- | -----------
[pubmed](https://tfhub.dev/google/experts/bert/pubmed)               | The model was trained in a self-supervised manner on the **MEDLINE/Pubmed** corpus.
[pubmed/squad2](https://tfhub.dev/google/experts/bert/pubmed/squad2) | This model was initialized from the base PubMed model and was fine-tuned on SQuAD 2.0, a dataset for **question-answering**.

\[1]: [Wikipedia dataset](https://dumps.wikimedia.org)

\[2]: [BooksCorpus dataset](http://yknzhu.wixsite.com/mbweb)

\[3]:
[MEDLINE/PubMed dataset](https://www.nlm.nih.gov/databases/download/pubmed_medline.html)

\[4]: [CORD-19 dataset](https://www.semanticscholar.org/cord19)

\[5]: [CoLA dataset](https://nyu-mll.github.io/CoLA/)

\[6]:
[MRPC dataset](https://www.microsoft.com/en-us/download/details.aspx?id=52398)
