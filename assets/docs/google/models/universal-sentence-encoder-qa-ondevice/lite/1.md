# Lite google/universal-sentence-encoder-qa-ondevice/1

Greater-than-word length text encoder for web question answer retrieval.

<!-- asset-path: internal -->
<!-- parent-model: google/universal-sentence-encoder-qa-ondevice -->

### Model Summary

The Universal Sentence Encoder for question answering (USE-QA) is a model that
encodes question and answer texts into 100-dimensional embeddings. The product
of these embeddings gives the score of the correlation between the
question-answer pair. It can also be used in other applications, including any
type of text classification, clustering, etc.

This module is a lightweight TensorFlow Lite [flatbuffer](https://www.tensorflow.org/lite/api_docs/cc/class/tflite/flat-buffer-model).
The model is based on the Transformer ([Vaswani et al, 2017](https://arxiv.org/abs/1706.03762))
architecture, and uses an 8k [SentencePiece](https://github.com/google/sentencepiece)
vocabulary. It is trained on a variety of data sources, with the goal of
learning text representations that are useful out-of-the-box to
[retrieve](https://arxiv.org/abs/1907.04780) an answer given a question.

### Model Details

*   Developed by researchers at Google, 2019. See QALite in [1].
*   Transformer.
*   Strong performance on question answer retrieval for English.
*   We recommend question and response inputs that are approximately one
    sentence in length. All input text is tokenized to fixed length of 192 and
    truncated from the right. Thus, the model time and space complexity is
    fixed.
*   The model has been integrated into the Tensorflow Lite [Task Library] (https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview).
    Please refer to the [header file](https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/cc/task/text/universal_sentence_encoder_qa.h)
    and the __Usage__ section below.

To learn more about text embeddings, refer to the
[TensorFlow Embeddings](https://www.tensorflow.org/tutorials/text/word_embeddings)
documentation.

### Intended Use

*   It is trained on a variety of data sources, with the goal of
    learning text representations that are useful out-of-the-box to retrieve an
    answer given a question.
*   It can also be used in other applications, including any type of text
    classification, clustering, etc.
*   The intended use is not-for-profit.

### A Word About Biases in Language Understanding Modelsâ€¦

Language understanding models use millions of examples to learn about language
and how words and sentences relate to each other. What they learn can be used to
power applications relevant to communication in society, and can also reflect
human cognitive biases.

When using any NLU model, including this one, careful product design decisions
are critical. For example, a toxicity classifier or sensitive topics classifier
could determine when an input or the output may be objectionable or contain an
unwanted association. We recommend taking bias-impact mitigation steps if
applicable to your application.

You can find resources on ML fairness [here](https://developers.google.com/machine-learning/fairness-overview/).

### Metrics

*   We apply this model on the
    [SQuAD retrieval task](https://github.com/google/retrieval-qa-eval). See
    performance below:

    <table style="table-layout:auto;">
      <tr style="text-align:center;">
        <th>SQuAD Retrieval</th>
        <th>dev</th>
        <th>train</th>
      </tr>
      <tr style="text-align:center;">
        <td rowspan="10">Precision@1</td>
        <td rowspan="5">39.7</td>
        <td rowspan="5">29.4</td>
      </tr>
    </table>

## Usage

To create an API client:

```c++
RetrievalOptions options;
options.mutable_base_options()->mutable_model_file()->set_file_name(filename);
auto status = UniversalSentenceEncoderQA::CreateFromOption(options);
CHECK_OK(status);
std::unique_ptr<UniversalSentenceEncoderQA> client = std::move(status.value());
```
where `filename` is the path to the tflite model.

To retrieve top responses:

```c++
// Create RetrievalInput with a query and responses.
RetrievalInput input;
// Set a sentence of text as the query.
input.set_query_text("How are you feeling today?");
// Add 3 candidate responses, and each one contains a sentence of text. (May
// set context too).
input.add_responses()->mutable_raw_text()->set_text("I'm not feeling very well.");
input.add_responses()->mutable_raw_text()->set_text("Paris is the capital of France.");
input.add_responses()->mutable_raw_text()->set_text("He looks good.");

// Run inference with the Retrieve function.
const StatusOr<RetrievalOutput>& output_status = client->Retrieve(input);
CHECK_OK(output_status);  // Check ok
const RetrievalOutput& output = output_status.value();

// Get top results (may set optional parameter k=? to limit top-K results).
const std::vector<size_t>& top = client->Top(output);

// Consume the results according to the ranking. Here we just print them out.
std::cout << input.query_text() << std::endl;
for (size_t k : top) {
   std::cout << input.responses(k).raw_text().text() << ", "
             << input.responses(k).raw_text().context() << ", "
             << output.response_results(k).score() << std::endl;
}
```

The program would print out:

```text
How are you feeling today?
I'm not feeling very well., , 14.9595
He looks good., , 8.80944
Paris is the capital of France., , 5.63753
```

Alternatively, you can encode query or response independently:

```c++
const StatusOr<EncodedVector> encoded_query = client->EncodeQuery("sample query");
const StatusOr<EncodedVector> encoded_response = client->EncodeQuery("sample response", "sample context");
```


[1] Amin Ahmad, Noah Constant, Yinfei Yang, Daniel Cer.
[ReQA: An Evaluation for End-to-End Answer Retrieval Models](https://arxiv.org/abs/1907.04780).
July 2019
