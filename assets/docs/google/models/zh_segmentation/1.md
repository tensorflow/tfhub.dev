# Module google/zh_segmentation/1

Chinese segmentation model trained on the Chinese Treebank 6.0.

<!-- asset-path: internal -->
<!-- module-type: text-segmentation -->
<!-- fine-tunable: false -->
<!-- format: hub -->
<!-- language: zh -->
<!-- network-architecture: Convolutional Neural Network (CNN) -->
<!-- dataset: Chinese Treebank 6.0 -->

## Overview

This model segments Chinese text into
[tokens](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation).  It
allows users to perform Chinese segmentation as part of their TensorFlow (TF)
graph, instead of using a separate preprocessing pipeline that can get out of
sync with the TF graph.

The most flexible way to use this model is via the wrapper `HubModuleTokenizer`
from [tensorflow_text](https://www.tensorflow.org/tutorials/tensorflow_text/intro).


## Requirements and compatibility

This model requires tensorflow_text version 2.4.0b0 (or later) and works with
both native TF2 and TF1 compatibility mode (see examples below).  On many
systems, this requirement can be satisfied by running

```shell
pip install "tensorflow_text>=2.4.0b0"
```

This command also installs the required TensorFlow 2 version (tensorflow_text
depends on it).

Note: tensorflow_text is required even if you decide to bypass the wrapper
`HubModuleTokenizer`: this model uses a TF op defined in tensorflow_text.


## Model API

We strongly recommend you use this model via the wrapper `HubModuleTokenizer`,
as explained in this section.

First, instantiate a `HubModuleTokenizer`:

```python
import tensorflow_text as text

# Handle for this model:
MODEL_HANDLE = "https://tfhub.dev/google/zh_segmentation/1"
segmenter = text.HubModuleTokenizer(MODEL_HANDLE)
```

Next, perform tokenization using calls like

```python
tokens, starts, ends = segmenter.tokenize_with_offsets(input_text)
```

`input_text` should be a Tensor or
[RaggedTensor](https://www.tensorflow.org/guide/ragged_tensor) of
[UTF-8](https://en.wikipedia.org/wiki/UTF-8) strings.  This input tensor can
have an arbitrary rank (not necessarily 1); in the rest of this section, assume
`input_text` has rank `N`.

The output is a tuple of three elements:

*   `tokens` is a `RaggedTensor` of UTF-8 strings where `tokens[i1...iN, j]` is
    the string content of the `j-th` token in `input_text[i1...iN]`.
    Intuitively, this tensor (and the other two below) have the same shape as
    `input_text`, but with an extra `N+1`-th dimension for the tokens.  This
    dimension is ragged, because different pieces of text can have different
    numbers of tokens.

*   `starts` is a `RaggedTensor` of int64s where `start_offsets[i1...iN, j]` is
    the byte offset for the start of the `j-th` token in `input_text[i1...iN]`.

*   `ends` is a `RaggedTensor` of int64s where `end_offsets[i1...iN, j]` is the
    byte offset immediately after the end of the `j-th` token in
    `input_text[i...iN]`.

Note: the token start/end offsets are expressed in bytes (not Unicode
characters) and are relative to the beginning of the relevant element of
`input_text` (first byte has offset 0).

In case you don't need the offsets, you can use the simpler

```python
tokens = segmenter.tokenize(input_text)
```

## Examples

The examples below assume Python 3 and use the following setting:

```python
MODEL_HANDLE = "https://tfhub.dev/google/zh_segmentation/1"
```

### Native TF2, Eager Mode

```python
import tensorflow_text as text

# Instantiating a HubModuleTokenizer takes time so it's good to create a
# single instance and (re)use it for multiple queries.
segmenter = text.HubModuleTokenizer(MODEL_HANDLE)

# Segment a batch of two strings.
input_text = ["新华社北京", "北京"]
tokens, starts, ends = segmenter.tokenize_with_offsets(input_text)

# Expected results:
assert tokens.to_list() == [
    ["新华社".encode("utf-8"), "北京".encode("utf-8")],
    ["北京".encode("utf-8")]
]
assert starts.to_list() == [[0, 9], [0]]
assert ends.to_list() == [[9, 15], [6]]

# Possibly use `segmenter` to process more strings.
...
```

The first text ("新华社北京") is split into two tokens, "新华社" and "北京".
The first token starts at byte offset 0 and ends right before byte offset 9
(i.e., at byte offset 8) relative to the beginning of the input string.  The
second token starts at byte offset 9 and ends right before byte offset 15.  Note
that these offsets are expressed in bytes, not in Unicode characters.  The
second text ("北京") consists of a single token.


### TF1 Compatibility Mode

```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

import tensorflow_text as text

# Build TensorFlow graph.
segmentation_graph = tf.Graph()
with segmentation_graph.as_default():
  input_text = tf.placeholder(dtype=tf.string, shape=(None,))
  segmenter = text.HubModuleTokenizer(MODEL_HANDLE)
  tokens, starts, ends = segmenter.tokenize_with_offsets(input_text)

with tf.Session(graph=segmentation_graph) as sess:
  # Run initializers
  sess.run(tf.tables_initializer())
  sess.run(tf.global_variables_initializer())
  sess.run(tf.local_variables_initializer())

  # Use TF graph to segment two strings.
  results = sess.run(
      [tokens, starts, ends],
      feed_dict={input_text: ["新华社北京", "北京"]})

# Expected results:
assert results[0].to_list() == [
    ["新华社".encode("utf-8"), "北京".encode("utf-8")],
    ["北京".encode("utf-8")]
]
assert results[1].to_list() == [[0, 9], [0]]
assert results[2].to_list() == [[9, 15], [6]]

# Possibly use segmentation_graph to process more strings.
```

## Training Data

This model was trained on the [Chinese Treebank
6.0](https://catalog.ldc.upenn.edu/LDC2007T36), a dataset of newswire articles
in Mandarin Chinese, from the Xinhua News Agency, the Sinorama Magazine, the
website of the Hong Kong Special Administrative Region, and various broadcast
news programs.


## Performance

On the recommended test part of the [Chinese TreeBank
6.0](https://catalog.ldc.upenn.edu/LDC2007T36), this model achieves the
following token-level metrics:

*   **Precision**: 94.76%
*   **Recall**: 94.91%
*   **F1 Score**: 94.83%

This means that on average, a predicted token has 94.76% chances of being
correct (i.e., identical to a token from the test data) and a token from the
test data has 94.91% chances of being reported by our model.  The F1 score is
the harmonic mean of the precision and recall metrics.


## How to Reference

The segmenter is based on the following paper:

*   Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu, David
    Weiss, Ryan McDonald, Slav Petrov: [Natural Language Processing with Small
    Feed-Forward Networks](https://arxiv.org/pdf/1708.00214.pdf)

which employs a neural network to predict split/merge label for each Unicode
character from the input, indicating whether we start or not a new token at that
point.
A special
[tensorflow_text](https://www.tensorflow.org/tutorials/tensorflow_text/intro) op
uses these labels to generate the actual tokens and offsets.
Instead of Feed-Forward Neural Network, we use a one layer Convolution Neural
Network (CNN) due to its superior inference speed.
Other differences include smaller embedding sizes to save model space and
computation costs, and training using more modern TensorFlow tools.


## Suitable Use and Limitations

This model is suitable for segmenting formal, newswire-style Chinese text using
the Chinese Simplified script.

This model is not recommended for informal text, e.g., text messages, nor for
text that uses Chinese Traditional characters.


## License

This model follows [*Apache 2.0*](https://www.apache.org/licenses/LICENSE-2.0).
If you intend to use it beyond permissible usage, please consult with the model
owners ahead of time.
