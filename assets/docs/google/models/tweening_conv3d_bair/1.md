# Module google/tweening_conv3d_bair/1
Conv3d-based video inbetweening model [1] trained on the BAIR robot pushing
dataset (with 64x64 RGB frames).

<!-- dataset: BAIR -->
<!-- asset-path: legacy -->
<!-- module-type: video-generation -->
<!-- network-architecture: Other -->
<!-- fine-tunable: false -->
<!-- format: hub -->

[![Open Colab notebok]](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tweening_conv3d.ipynb)

## Overview

This module stochastically generates 14 in-between video frames given the start
and end frames.

It takes as input `<Tensor(tf.float32, shape=[16, 2, 64, 64, 3])>`, representing
a (fixed-size) batch of 16 time-wise concatenated start/end frames of 64x64 with
3 channels (RGB), and outputs `<Tensor(tf.float32, shape=[16, 14, 64, 64, 3])>`,
which are the 14 in-between frames. The pixel value range is \[0.0, 255.0\].

Note that the model is trained for this particular dataset. Therefore it will
only work on video frames from the dataset or someting very similar in
appearance.

#### Example use

See the Colab notebook below.

#### References

[1] Yunpeng Li, Dominik Roblek, and Marco Tagliasacchi.
[From Here to There: Video Inbetweening Using Direct 3D Convolutions](https://arxiv.org/abs/1905.10240).
arXiv preprint arXiv:1905.10240, 2019.
