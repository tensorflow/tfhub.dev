# Module emilutz/vgg19-block3-conv2-unpooling-decoder/1

Image decoder based on vgg19 that uses unpooling layers for upsampling.

<!-- asset-path: https://storage.googleapis.com/vgg19-with-unpooling/vgg19-block3-conv2/decoder.tar.gz -->
<!-- task: image-feature-vector -->
<!-- fine-tunable: true -->
<!-- dataset: coco-2017 -->
<!-- format: saved_model_2 -->
<!-- network-architecture: vgg-style -->
<!-- license: mit -->

## Overview

### Module description

This model accepts as input a feature vector (extracted by passing an image through its pair encoder from block3_conv2 level of VGG19) and outputs the reconstructed image. It is part of a larger collection of such pairs designed to offer the ability to manipulate images by changing the latent space. One example of such use case can be found in style transfer applications like https://arxiv.org/abs/1802.06474. The addition of unpooling layers (as oposed to using regular upsampling methods) ensures fine image details are reconstructed with higher precision and without introducing artifacts.

### Model architecture

This model contains only the component enclosed in the dashed line below.

<img src="https://storage.googleapis.com/vgg19-with-unpooling/vgg19-block3-conv2/decoder.png" height="384">

### Input

*   Inputs are expected to be vgg19-block3-conv2 encoded image features + maxpooling argmax indices
*   Shape: `[(B, H/4, W/4, 256), (B * H * W * 16, 4), (B * H * W * 8, 4)]`

### Output

*   Outputs are expected to be 3-channel RGB images with values in range [0, 255] and type float32
*   Shape: `(B, H, W, 3)`

## Usage

### Use SavedModel in Python

```python
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub

# Load encoder and decoder
encoder = hub.KerasLayer('https://tfhub.dev/emilutz/vgg19-block3-conv2-unpooling-encoder/1') # external model
decoder = hub.KerasLayer('https://tfhub.dev/emilutz/vgg19-block3-conv2-unpooling-decoder/1') # this model

# Read image from disk and add batch size
image_file = tf.io.read_file(image_path)
image_uint8 = tf.image.decode_image(image_file)
image_float32 = tf.cast(image_uint8, tf.float32)
image_batch = tf.expand_dims(image_float32, axis=0)

# Encode and decode the image
[image_features, b1_indices, b2_indices] = encoder(image_batch)
image_decoded = decoder([image_features, b1_indices, b2_indices])

reconstruction_loss = np.abs((image_decoded - image_batch).numpy()).mean()
```

## Training

This decoder was trained on the COCO 2017 dataset for 30 epochs while keeping the weights of the encoder frozen. An Adam optimizer was used with default parameters and a linear learning rate decay between 0.001 and 0.0002. In order to ensure the quality of the reconstructions is high from multiple points of view like pixel values proximity compared to the originals and maintenance of overall colour attributes like contrast and brightness, a combination of 4 losses was minimzed during training:
- L1 loss
- L2 loss
- MS-SSIM loss (computed as `1.0 - tf.image.ssim_multiscale(inputs, outputs, 255)`; more information about multi-scale structural similarity can be found here: https://www.cns.nyu.edu/pub/eero/wang03b.pdf)
- Perceptual loss (computed as `tf.keras.losses.mean_squared_error(encoder(inputs), encoder(decoder(encoder(inputs))))` so every encoder also acts as the perceptual loss feature extractor for his decoder; more information about perceptual loss can be found here: https://arxiv.org/pdf/1603.08155.pdf)

The weights used for the loss components were chosen in accordance to their magnitudes and are the following:
- `lambda_l1 = 10.0`
- `lambda_l2 = 1.0`
- `lambda_ms_ssim = 1000.0`
- `lambda_perceptual = 0.1`

### Training dataset

COCO 2017 Unlabelled images (123K) https://cocodataset.org/#download.
- 90% used for training (111063 images)
- 9% used for validation (11106 images)
- 1% left for testing (1234 images)

### Evaluation

The mean absolute difference between the pixel values of the input images and their reconstructions `abs(input - decoder(encoder(input)))` for this pair as evaluated on the test set is **5.124**. 
