# Module vasudevgupta7/wav2vec2-base/1

Pre-trained speech model (without any head) from Facebook for Automatic Speech Recognition

<!-- asset-path:  https://huggingface.co/vasudevgupta/tf-wav2vec2-base/resolve/main/wav2vec2-base.tar.gz -->
<!-- task: audio-stt -->
<!-- network-architecture: wav2vec2-base -->
<!-- format: saved_model_2 -->
<!-- fine-tunable: true -->
<!-- license: apache-2.0 -->
<!-- language: en -->
<!-- colab: https://colab.research.google.com/github/vasudevgupta7/gsoc-wav2vec2/blob/main/notebooks/wav2vec2_base_saved_model.ipynb -->

## Overview

This model is TensorFlow equivalent of PyTorch [`facebook/wav2vec2-base`](https://huggingface.co/facebook/wav2vec2-base). This model was introduced in the paper: [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477).

**How to use this model?**

Add randomly initalized LM head over the top of pre-trained model & fine-tune the whole model.

```python
import tensorflow as tf
import tensorflow_hub as hub

loaded = hub.load("https://tfhub.dev/vasudevgupta7/wav2vec2-base/1")

# For using this pre-trained model for training, just pass `trainable=True` in `hub.KerasLayer`
pretrained_layer = hub.KerasLayer(loaded.signatures["wav2vec2"], trainable=True)

# add some LM head as per your downstream task
VOCAB_SIZE = 32
lm_head = tf.keras.layers.Dense(VOCAB_SIZE)

# Let's wrap all the layers into `tf.keras.Model`
inputs = tf.keras.Input(shape=(246000,))
hidden_states = pretrained_layer(inputs)["output_0"]
outputs = tf.keras.layers.Dense(VOCAB_SIZE)(hidden_states)
model = tf.keras.Model(inputs=inputs, outputs=outputs)

# For using this model, it's important to set `jit_compile=True` on GPUs/CPUs
# as some operations in this model (i.e. group-convolutions) are unsupported without it
@tf.function(jit_compile=True)
def forward(speech):
    return model(speech)

# Now, this model can trained like any other TensorFlow model
```

**Note:** This model shouldn't be directly used for inference. LM head should be added on the top of this model & it should be fine-tuned for downstream tasks like `speech -> text`. Complete fine-tuning workflow is shown in this [notebook](https://colab.research.google.com/github/vasudevgupta7/gsoc-wav2vec2/blob/main/notebooks/wav2vec2_base_saved_model.ipynb).
