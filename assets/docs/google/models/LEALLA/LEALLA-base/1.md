# Module google/LEALLA/LEALLA-base/1
LEALLA: Lightweight language-agnostic sentence embedding model supporting 109 languages.

<!-- asset-path: internal -->
<!-- task: text-embedding -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: multilingual -->
<!-- network-architecture: bert -->
<!-- dataset: commoncrawl -->
<!-- dataset: wikipedia -->
<!-- dataset: translation -->

## Overview

LEALLA \[1\] encodes text into low-dimensional vectors.
It is faster for inference because LEALLA contains fewer model parameters compared with [LaBSE](https://tfhub.dev/google/LaBSE) \[2\].
LEALLA also accelerates applications to downstream tasks because it generates low-dimensional sentence embeddings.

Same as LaBSE, LEALLA is trained and optimized to produce similar representations exclusively for bilingual sentence pairs that are translations of each other.
So it can be used for mining translations of sentences in a larger corpus.
LEALLA is further enhanced by knowledge distillation from LaBSE.

We trained three LEALLA models: LEALLA-small, LEALLA-base, and LEALLA-large with different model sizes as shown below:

|Model|Number of languages|Dimension of sentence embedding|Number of parameters|Exported SavedModel size|
|:-------------|:---:|:---:|:----:|:----:|
| LaBSE        | 109 | 768 | 471M | 1.8G |
| LEALLA-small | 109 | 128 | 69M  | 263M |
| LEALLA-base  | 109 | 192 | 107M | 408M |
| LEALLA-large | 109 | 256 | 147M | 562M |

## Metrics

* We evaluate LEALLA models by the United Nations parallel corpus retrieval task, where for each English sentence we try to find its true translation from a 9.5 million sentence pool in the other language.

  | Model        | en-es| en-fr| en-ru| en-zh|
  |:-------------|:----:|:----:|:----:|:----:|
  | LaBSE        | 90.8 | 89.0 | 90.4 | 88.3 |
  | LEALLA-small | 89.4 | 86.0 | 88.7 | 84.9 |
  | LEALLA-base  | 90.3 | 87.4 | 89.8 | 87.2 |
  | LEALLA-large | 90.8 | 88.5 | 89.9 | 87.9 |
  (Results of LaBSE are slightly different from those on https://tfhub.dev/google/LaBSE. We report the results based on our implementation.)

* For Tatoeba, we report the average results for all the 112 languages.

  | Model | All (112) languages |
  |:-------------|:----:|
  | LaBSE        | 83.7 |
  | LEALLA-small | 80.7 |
  | LEALLA-base  | 82.4 |
  | LEALLA-large | 83.5 |

More details of evaluation can be found in the paper \[1\].

## Extended Uses and Limitations

The produced embeddings can also be used for text classification, semantic similarity, clustering and other natural language tasks.
The performance may depend on the domain / data match of a particular task.
For general purpose sentence embeddings, we refer to the [Universal Sentence Encoder family](https://tfhub.dev/google/collections/universal-sentence-encoder).

It is adopted from a pre-trained BERT model so it can also be used for any tasks BERT can be applied to.
The performance, again, depends on the particular task.
Users may consider the [BERT model family](https://tfhub.dev/google/collections/bert).

## Example Use

Our model is based on the BERT transformer architecture, which generates a pooled_output of shape \[batch_size, 192\] for LEALLA-base (\[batch_size, 128\] for LEALLA-small and \[batch_size, 256\] for LEALLA-large) with representations for the entire input sequences.
The representation generated from this model is recommended to be used as is without fine-tuning.
The model is also fine-tunable like other BERT models.
The TF Hub model can be called as follows:

```python
import tensorflow_hub as hub
import tensorflow as tf
import numpy as np

# encoder = hub.KerasLayer("https://tfhub.dev/google/LEALLA/LEALLA-small/1")
encoder = hub.KerasLayer("https://tfhub.dev/google/LEALLA/LEALLA-base/1")
# encoder = hub.KerasLayer("https://tfhub.dev/google/LEALLA/LEALLA-large/1")

english_sentences = tf.constant(["dog", "Puppies are nice.", "I enjoy taking long walks along the beach with my dog."])
italian_sentences = tf.constant(["cane", "I cuccioli sono carini.", "Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane."])
japanese_sentences = tf.constant(["犬", "子犬はいいです", "私は犬と一緒にビーチを散歩するのが好きです"])

english_embeds = encoder(english_sentences)
japanese_embeds = encoder(japanese_sentences)
italian_embeds = encoder(italian_sentences)

# English-Italian similarity
print(np.matmul(english_embeds, np.transpose(italian_embeds)))

# English-Japanese similarity
print(np.matmul(english_embeds, np.transpose(japanese_embeds)))

# Italian-Japanese similarity
print(np.matmul(italian_embeds, np.transpose(japanese_embeds)))
```

## References

\[1\] Zhuoyuan Mao, Tetsuji Nakagawa. LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with Knowledge Distillation. EACL 2023.

\[2\] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Narveen Ari, Wei Wang. [Language-agnostic BERT Sentence Embedding](https://arxiv.org/abs/2007.01852). July 2020
